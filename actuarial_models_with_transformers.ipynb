{"cells":[{"cell_type":"markdown","metadata":{"id":"_I2LuIoojymG"},"source":["***\n","<center>Research Project</center>\n","<h1><center>Research Project</center></h1>\n","<h2><center>Enhancing actuarial non-life pricing models via transformers</center></h2>\n","<center>by Alexej Brauer </center>\n","<center>M.Sc. (TUM) / Aktuar DAV / CADS </center>\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"22zt2a5cjymK"},"source":["# This notebook will provide the code to reproduce the data cleaning/preparation and results of the paper: Enhancing actuarial non-life pricing models via transformers."]},{"cell_type":"markdown","metadata":{"id":"oAHaEQNcMKNY"},"source":["I used here the following work as a foundation:     \n","* Ronald Richman, Mario V. Wüthrich \"LocalGLMnet: interpretable deep learning for tabular data\" 2023\n","* Mario V. Wüthrich, M. Merz, \"Statistical Foundations of Actuarial Learning and its Applications\" 2023\n","* Gorishniy, Rubachev, Khrulkov, Babenko \"Revisiting Deep Learning Models for Tabular Data\" 2021\n","* Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, \"Attention Is All You Need\", NeurIPS 2017"]},{"cell_type":"markdown","metadata":{"id":"lTJJtjEYjymL"},"source":["# 1. Basic Setting:"]},{"cell_type":"markdown","metadata":{"id":"ArsRIwe_lsGa"},"source":["## 1.1 Load packages:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b20Q_krDjymL"},"outputs":[],"source":["# sys and os imports\n","import os\n","import sys\n","import platform\n","import subprocess\n","import re\n","import warnings\n","\n","# display and plotting\n","from IPython.display import Image\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","# data\n","import numpy as np\n","import pandas as pd\n","from dataclasses import dataclass, field\n","\n","# modelling\n","import random\n","# modelling scikit-learn\n","import sklearn as sk\n","from sklearn.linear_model import PoissonRegressor\n","from sklearn.preprocessing import StandardScaler\n","# modelling tensorflow\n","import tensorflow as tf\n","import keras\n","from keras.activations import (tanh, exponential, gelu)\n","\n","# saveing & time\n","import pickle\n","import time\n","import datetime\n","\n","# for multiprocessing\n","from multiprocessing import Process\n","import logging\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JwwzFre4l9L3"},"source":["## 1.2 Storage settings:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23448,"status":"ok","timestamp":1699568777598,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"1UVA7TnLl-yY","outputId":"8158037b-cdf7-4942-857f-88f9dfa95fa7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# set the path to the storage folder:\n","storage_path = \".\"\n","\n","# import my classes for the ft-transformer models:\n","# ----------------------\n","sys.path.insert(1, f'{storage_path}/helper')\n","import main_model_classes as EnhActuar\n","import helper as helper"]},{"cell_type":"markdown","metadata":{"id":"1PV82qH-mP74"},"source":["## 1.3 Display settings:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NkS37qPMmS74"},"outputs":[],"source":["pd.set_option('display.max_columns', None)"]},{"cell_type":"markdown","metadata":{"id":"ZIUBtBS0lgB0"},"source":["## 1.4 Get information about the system:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":745,"status":"ok","timestamp":1699568778341,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"NRfwcUjHlEfK","outputId":"94eba63d-b5e6-43df-e4c6-9a73acb6c2c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Version of Python: \n","--------------------\n","Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n","\n","Version of main Packages (full list below): \n","--------------------\n","Tensor Flow Version: 2.14.0\n","Keras Version: 2.14.0\n","Pandas Version: 1.5.3\n","Scikit-Learn: 1.2.2\n","\n","Information about CPU: \n","--------------------\n"," Intel(R) Xeon(R) CPU @ 2.00GHz\n","\n","Information about GPU: \n","--------------------\n","GPU is available\n","[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","Thu Nov  9 22:26:17 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   50C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# if one has a kernel installed that supports gpu's uncomment this:\n","# -----------\n","# Printing the versions of the main packages:\n","print(f\"Version of Python: \")\n","print(\"--------------------\")\n","print(f\"Python {sys.version}\")\n","print()\n","print(f\"Version of main Packages (full list below): \")\n","print(\"--------------------\")\n","print(f\"Tensor Flow Version: {tf.__version__}\")\n","print(f\"Keras Version: {keras.__version__}\")\n","print(f\"Pandas Version: {pd.__version__}\")\n","print(f\"Scikit-Learn: {sk.__version__}\")\n","\n","print()\n","print(\"Information about CPU: \")\n","print(\"--------------------\")\n","# code from: https://stackoverflow.com/a/13078519\n","def get_processor_name():\n","    if platform.system() == \"Windows\":\n","        return platform.processor()\n","    elif platform.system() == \"Darwin\":\n","        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin'\n","        command =\"sysctl -n machdep.cpu.brand_string\"\n","        return subprocess.check_output(command).strip()\n","    elif platform.system() == \"Linux\":\n","        command = \"cat /proc/cpuinfo\"\n","        all_info = subprocess.check_output(command, shell=True).decode().strip()\n","        for line in all_info.split(\"\\n\"):\n","            if \"model name\" in line:\n","                return re.sub( \".*model name.*:\", \"\", line,1)\n","    return \"\"\n","print(get_processor_name())\n","\n","print()\n","print(\"Information about GPU: \")\n","print(\"--------------------\")\n","print(\"GPU is\", \"available\" if len(tf.config.list_physical_devices('GPU'))>0 else \"NOT AVAILABLE\")\n","gpus = tf.config.list_physical_devices('GPU')\n","if len(gpus) != 0:\n","  tf.config.set_visible_devices(gpus[0], 'GPU')\n","  # Set GPU Device:\n","  tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n","  tf.config.experimental.set_memory_growth(gpus[0], True)\n","  print(gpus)\n","  !nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"_cfBAUgzkBVI"},"source":["For further details regarding the environment, see last chapter of this notebook."]},{"cell_type":"markdown","metadata":{"id":"avrhgXfKjymV"},"source":["## 1.5 Set random seeds:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TU0YUePQjymW"},"outputs":[],"source":["def set_random_seeds(seed_nr):\n","    tf.random.set_seed(seed_nr)\n","    np.random.seed(seed_nr)\n","    random.seed(seed_nr)\n","    os.environ['PYTHONHASHSEED']=str(seed_nr)\n","\n","set_random_seeds(42)\n","\n","# create 15 random seeds for the 15 models\n","random_seeds = np.random.randint(0, 1000000000, 15)\n"]},{"cell_type":"markdown","metadata":{"id":"V4RYzAlFjymW"},"source":["# 2. Load & prepare the Data:"]},{"cell_type":"markdown","metadata":{"id":"F9qNpCkwjymX"},"source":["## 2.1 Load-File"]},{"cell_type":"markdown","metadata":{"id":"fV8mgLsIjymX"},"source":["We are using the same data and data preperation as described in this Paper/Book:\n","* 2023 Book by  M. V. Wüthrich, M. Merz, \"Statistical Foundations of Actuarial Learning and its Applications\"\n","* 2023 Richmann & Wüthrich: \"LocalGLMnet: interpretable deep learning for tabular data\""]},{"cell_type":"markdown","metadata":{"id":"-Sr4_oEpjymX"},"source":["We refer here to section 3.4 in then LocalGLMnet paper.\n","Note that we are not just downloading the French Motor Third Party Liability Data files from CASdatasets.\n","They are downloading another version and they describe why in the Footnote 2 of page 553 of the 2023 Book by  M. V. Wüthrich, M. Merz.  \n","\n","So we download the data in the same way (the R code looks like this):\n","```R\n","--------------------------\n","library(OpenML)\n","library(farff)\n","library(feather)\n","freMTPL2freq <- getOMLDataSet(data.id = 41214)$data\n","freMTPL2sev<-getOMLDataSet(data.id = 41215)$data\n","\n","str(freMTPL2freq)\n","str(freMTPL2sev)\n","\n","# Save the Datasets as feather files\n","write_feather(freMTPL2freq, \"./Data/freMTPL2freq.feather\")\n","write_feather(freMTPL2sev, \"./Data/freMTPL2sev.feather\")\n","```\n","\n","In Python we are now loading in the feather files:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KY2xrO9YjymX"},"outputs":[],"source":["df_freq = pd.read_feather(f'{storage_path}/Data/freMTPL2freq.feather')\n","df_sev = pd.read_feather(f'{storage_path}/Data/freMTPL2sev.feather')"]},{"cell_type":"markdown","metadata":{"id":"1-mb7OYrjymY"},"source":["## 2.2 Data Cleaning"]},{"cell_type":"markdown","metadata":{"id":"SyAOxlaLjymY"},"source":["Now it gets a bit complicated:\n","\n","If one wants to replicate the Results of these Papers (1):\n","* 2018 Noll Case Study: \"French Motor Third-Party Liability Claims\"\n","* 2019 Schelldorfer Paper: \"Nesting Classical Actuarial Models into Neural Networks\"\n","* 2020 Wüthrich Paper: \"From Generalized Linear Models to Neural Networks, and Back\"\n","\n","Then:\n","1. One does not delete raws/lines of this data set\n","2. One uses the ClaimNb as it was in the original dataset\n","\n","Whereas if one wants to replicate the Results of these Papers (2):\n","* 2023 Book by  M. V. Wüthrich, M. Merz, \"Statistical Foundations of Actuarial Learning and its Applications\"\n","* 2023 Richmann & Wüthrich: \"LocalGLMnet: interpretable deep learning for tabular data\"\n","\n","Then:\n","1. One uses ClaimNb as aggregation of claim from the freMTPL2sev dataset\n","2. One does delete raws/lines of this data set that have more than 4 claim\n","\n","Since this notebook focuses on the second 2 papers the data-prep part is done in the same why as described there."]},{"cell_type":"markdown","metadata":{"id":"3PE_uyMYjymY"},"source":["They are doing here some basic data cleaning that we will also do before we go into the actually data preperation chapter.\n","For the original R-Code for the data cleaning we refer here to Listing 13.1 of 2023 Book by  M. V. Wüthrich, M. Merz. Regarding the summary of the data please see there Listing 13.2."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1699787192761,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"tj4PL1DHjymY"},"outputs":[],"source":["# drop the column ClaimNb from df_freq:\n","df_freq = df_freq.drop(columns=[\"ClaimNb\"])\n","# convert the column \"VehGas\" into categorical:\n","df_freq[\"VehGas\"] = df_freq[\"VehGas\"].astype(\"category\")\n","# create a temporary dataframe with the column IDpol and the number of claims per policy:\n","temp_df_ClaimNb_from_sev_df = pd.DataFrame(df_sev[\"IDpol\"].value_counts()).reset_index()\n","temp_df_ClaimNb_from_sev_df.columns = [\"IDpol\", \"ClaimNb_from_sev_df\"]\n","# we merge the two dataframes so that we have the column \"ClaimNb_from_sev_df\" in df_freq:\n","df_freq = pd.merge(df_freq, temp_df_ClaimNb_from_sev_df, on='IDpol', how='left')\n","df_freq[\"ClaimNb_from_sev_df\"] = df_freq[\"ClaimNb_from_sev_df\"].fillna(0)\n","# rename the column \"ClaimNb_from_sev_df\" to \"ClaimNb\":\n","df_freq = df_freq.rename(columns={\"ClaimNb_from_sev_df\":\"ClaimNb\"})\n","# replace all nan values of numerical columns in the dataframe with 0:\n","for col in df_freq.select_dtypes(include=['number']).columns:\n","    df_freq[col] = df_freq[col].fillna(0)\n","# restrict the dataframe to those raws that have a ClaimNb smaller or equal to 5:\n","df_freq = df_freq[df_freq[\"ClaimNb\"]<=5]\n","# if exposure is bigger then 1 set it to 1:\n","df_freq.loc[df_freq[\"Exposure\"]>1,\"Exposure\"] = 1\n","# reordering the categories of the column VehBrand to \"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B10\",\"B11\",\"B12\",\"B13\",\"B14\":\n","df_freq[\"VehBrand\"] = df_freq[\"VehBrand\"].cat.reorder_categories([\"B1\",\"B2\",\"B3\",\"B4\",\"B5\",\"B6\",\"B10\",\"B11\",\"B12\",\"B13\",\"B14\"])\n"]},{"cell_type":"markdown","metadata":{"id":"iRWWeLDTjymY"},"source":["They described in the paper that they have after data cleaning the claim counts, time exposures and feature information, with  \n","> six continuous feature components (called ‘Area Code’, ‘Bonus-Malus Level’, ‘Density’, ‘Driver’s Age’, ‘Vehicle Age’, ‘Vehicle Power’), 1 binary component (called ‘Vehicle Gas’) and two categorical components with more than two levels (called ‘Vehicle Brand’ and ‘Region’).\n","\n","Note that in the listing of the book they change area code at another stage, but will transform already here since the LocalGLMnet paper says so:  "]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":268,"status":"ok","timestamp":1699787210909,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"h6V_wFXxjymY"},"outputs":[],"source":["df_freq[\"Area\"] = df_freq[\"Area\"].map({\"A\":1,\"B\":2,\"C\":3,\"D\":4,\"E\":5,\"F\":6}).astype(int)"]},{"cell_type":"markdown","metadata":{"id":"UDWZZG8ZjymZ"},"source":["Sort the Dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDlB6moZjymZ"},"outputs":[],"source":["# sort the dataframe by IDpol:\n","df_freq = df_freq.sort_values(by=[\"IDpol\"])"]},{"cell_type":"markdown","metadata":{"id":"K_7tZR-TjymZ"},"source":["## 2.3 Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"EA7Ev0T7jymZ"},"source":["After saving and loading the dataframe as a feather file in python and doing the same small datacleaning part as described in the book the summary looks like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1699568782136,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"EAucpfYkjymZ","outputId":"b9cae9e1-86c4-4dd2-b2fe-4bab9d8e314c"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-148b0306-b9a5-43e9-ba72-284f9ce19d68\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>IDpol</th>\n","      <th>Exposure</th>\n","      <th>Area</th>\n","      <th>VehPower</th>\n","      <th>VehAge</th>\n","      <th>DrivAge</th>\n","      <th>BonusMalus</th>\n","      <th>VehBrand</th>\n","      <th>VehGas</th>\n","      <th>Density</th>\n","      <th>Region</th>\n","      <th>ClaimNb</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>0.10</td>\n","      <td>4</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>55.0</td>\n","      <td>50.0</td>\n","      <td>B12</td>\n","      <td>Regular</td>\n","      <td>1217.0</td>\n","      <td>R82</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3.0</td>\n","      <td>0.77</td>\n","      <td>4</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>55.0</td>\n","      <td>50.0</td>\n","      <td>B12</td>\n","      <td>Regular</td>\n","      <td>1217.0</td>\n","      <td>R82</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5.0</td>\n","      <td>0.75</td>\n","      <td>2</td>\n","      <td>6.0</td>\n","      <td>2.0</td>\n","      <td>52.0</td>\n","      <td>50.0</td>\n","      <td>B12</td>\n","      <td>Diesel</td>\n","      <td>54.0</td>\n","      <td>R22</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10.0</td>\n","      <td>0.09</td>\n","      <td>2</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>46.0</td>\n","      <td>50.0</td>\n","      <td>B12</td>\n","      <td>Diesel</td>\n","      <td>76.0</td>\n","      <td>R72</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11.0</td>\n","      <td>0.84</td>\n","      <td>2</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>46.0</td>\n","      <td>50.0</td>\n","      <td>B12</td>\n","      <td>Diesel</td>\n","      <td>76.0</td>\n","      <td>R72</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-148b0306-b9a5-43e9-ba72-284f9ce19d68')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-148b0306-b9a5-43e9-ba72-284f9ce19d68 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-148b0306-b9a5-43e9-ba72-284f9ce19d68');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-68267848-95a5-43fc-b561-95d7cdf5bcb0\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-68267848-95a5-43fc-b561-95d7cdf5bcb0')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-68267848-95a5-43fc-b561-95d7cdf5bcb0 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["   IDpol  Exposure  Area  VehPower  VehAge  DrivAge  BonusMalus VehBrand  \\\n","0    1.0      0.10     4       5.0     0.0     55.0        50.0      B12   \n","1    3.0      0.77     4       5.0     0.0     55.0        50.0      B12   \n","2    5.0      0.75     2       6.0     2.0     52.0        50.0      B12   \n","3   10.0      0.09     2       7.0     0.0     46.0        50.0      B12   \n","4   11.0      0.84     2       7.0     0.0     46.0        50.0      B12   \n","\n","    VehGas  Density Region  ClaimNb  \n","0  Regular   1217.0    R82      0.0  \n","1  Regular   1217.0    R82      0.0  \n","2   Diesel     54.0    R22      0.0  \n","3   Diesel     76.0    R72      0.0  \n","4   Diesel     76.0    R72      0.0  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df_freq.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699568782137,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"fu9TmgVBjymZ","outputId":"6949a360-28c1-472e-c115-e3ff34ae4e90"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 678007 entries, 0 to 678012\n","Data columns (total 12 columns):\n"," #   Column      Non-Null Count   Dtype   \n","---  ------      --------------   -----   \n"," 0   IDpol       678007 non-null  float64 \n"," 1   Exposure    678007 non-null  float64 \n"," 2   Area        678007 non-null  int64   \n"," 3   VehPower    678007 non-null  float64 \n"," 4   VehAge      678007 non-null  float64 \n"," 5   DrivAge     678007 non-null  float64 \n"," 6   BonusMalus  678007 non-null  float64 \n"," 7   VehBrand    678007 non-null  category\n"," 8   VehGas      678007 non-null  category\n"," 9   Density     678007 non-null  float64 \n"," 10  Region      678007 non-null  category\n"," 11  ClaimNb     678007 non-null  float64 \n","dtypes: category(3), float64(8), int64(1)\n","memory usage: 53.7 MB\n"]}],"source":["df_freq.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"elapsed":571,"status":"ok","timestamp":1699568782701,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"7B9nqoIBjymZ","outputId":"87db5939-e0fa-4f93-b493-92ccaa214a64"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-a20ca035-a7b4-4e16-b0df-318b6f1676aa\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>IDpol</th>\n","      <th>Exposure</th>\n","      <th>Area</th>\n","      <th>VehPower</th>\n","      <th>VehAge</th>\n","      <th>DrivAge</th>\n","      <th>BonusMalus</th>\n","      <th>VehBrand</th>\n","      <th>VehGas</th>\n","      <th>Density</th>\n","      <th>Region</th>\n","      <th>ClaimNb</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>6.780070e+05</td>\n","      <td>678007.000000</td>\n","      <td>678007.000000</td>\n","      <td>678007.000000</td>\n","      <td>678007.000000</td>\n","      <td>678007.000000</td>\n","      <td>678007.000000</td>\n","      <td>678007</td>\n","      <td>678007</td>\n","      <td>678007.000000</td>\n","      <td>678007</td>\n","      <td>678007.000000</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>11</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>22</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>B12</td>\n","      <td>Regular</td>\n","      <td>NaN</td>\n","      <td>R24</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>166024</td>\n","      <td>345871</td>\n","      <td>NaN</td>\n","      <td>160601</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>2.621857e+06</td>\n","      <td>0.528547</td>\n","      <td>3.289692</td>\n","      <td>6.454653</td>\n","      <td>7.044218</td>\n","      <td>45.499061</td>\n","      <td>59.761588</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1792.430975</td>\n","      <td>NaN</td>\n","      <td>0.038913</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1.641789e+06</td>\n","      <td>0.364081</td>\n","      <td>1.382689</td>\n","      <td>2.050902</td>\n","      <td>5.666235</td>\n","      <td>14.137492</td>\n","      <td>15.636700</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>3958.663031</td>\n","      <td>NaN</td>\n","      <td>0.204752</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000e+00</td>\n","      <td>0.002732</td>\n","      <td>1.000000</td>\n","      <td>4.000000</td>\n","      <td>0.000000</td>\n","      <td>18.000000</td>\n","      <td>50.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1.000000</td>\n","      <td>NaN</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.157948e+06</td>\n","      <td>0.180000</td>\n","      <td>2.000000</td>\n","      <td>5.000000</td>\n","      <td>2.000000</td>\n","      <td>34.000000</td>\n","      <td>50.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>92.000000</td>\n","      <td>NaN</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>2.272153e+06</td>\n","      <td>0.490000</td>\n","      <td>3.000000</td>\n","      <td>6.000000</td>\n","      <td>6.000000</td>\n","      <td>44.000000</td>\n","      <td>50.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>393.000000</td>\n","      <td>NaN</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>4.046278e+06</td>\n","      <td>0.990000</td>\n","      <td>4.000000</td>\n","      <td>7.000000</td>\n","      <td>11.000000</td>\n","      <td>55.000000</td>\n","      <td>64.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>1658.000000</td>\n","      <td>NaN</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>6.114330e+06</td>\n","      <td>1.000000</td>\n","      <td>6.000000</td>\n","      <td>15.000000</td>\n","      <td>100.000000</td>\n","      <td>100.000000</td>\n","      <td>230.000000</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>27000.000000</td>\n","      <td>NaN</td>\n","      <td>5.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a20ca035-a7b4-4e16-b0df-318b6f1676aa')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a20ca035-a7b4-4e16-b0df-318b6f1676aa button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a20ca035-a7b4-4e16-b0df-318b6f1676aa');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d80e4438-4db3-46e9-9641-d5bc3a630dc7\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d80e4438-4db3-46e9-9641-d5bc3a630dc7')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d80e4438-4db3-46e9-9641-d5bc3a630dc7 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["               IDpol       Exposure           Area       VehPower  \\\n","count   6.780070e+05  678007.000000  678007.000000  678007.000000   \n","unique           NaN            NaN            NaN            NaN   \n","top              NaN            NaN            NaN            NaN   \n","freq             NaN            NaN            NaN            NaN   \n","mean    2.621857e+06       0.528547       3.289692       6.454653   \n","std     1.641789e+06       0.364081       1.382689       2.050902   \n","min     1.000000e+00       0.002732       1.000000       4.000000   \n","25%     1.157948e+06       0.180000       2.000000       5.000000   \n","50%     2.272153e+06       0.490000       3.000000       6.000000   \n","75%     4.046278e+06       0.990000       4.000000       7.000000   \n","max     6.114330e+06       1.000000       6.000000      15.000000   \n","\n","               VehAge        DrivAge     BonusMalus VehBrand   VehGas  \\\n","count   678007.000000  678007.000000  678007.000000   678007   678007   \n","unique            NaN            NaN            NaN       11        2   \n","top               NaN            NaN            NaN      B12  Regular   \n","freq              NaN            NaN            NaN   166024   345871   \n","mean         7.044218      45.499061      59.761588      NaN      NaN   \n","std          5.666235      14.137492      15.636700      NaN      NaN   \n","min          0.000000      18.000000      50.000000      NaN      NaN   \n","25%          2.000000      34.000000      50.000000      NaN      NaN   \n","50%          6.000000      44.000000      50.000000      NaN      NaN   \n","75%         11.000000      55.000000      64.000000      NaN      NaN   \n","max        100.000000     100.000000     230.000000      NaN      NaN   \n","\n","              Density  Region        ClaimNb  \n","count   678007.000000  678007  678007.000000  \n","unique            NaN      22            NaN  \n","top               NaN     R24            NaN  \n","freq              NaN  160601            NaN  \n","mean      1792.430975     NaN       0.038913  \n","std       3958.663031     NaN       0.204752  \n","min          1.000000     NaN       0.000000  \n","25%         92.000000     NaN       0.000000  \n","50%        393.000000     NaN       0.000000  \n","75%       1658.000000     NaN       0.000000  \n","max      27000.000000     NaN       5.000000  "]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["df_freq.describe(include=\"all\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1WvhTlWvzMihienIqcV7ewOPa13nQtVvz"},"executionInfo":{"elapsed":12759,"status":"ok","timestamp":1699568795456,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"g3qKuuUyjymZ","outputId":"e7c82c12-7cbb-4fae-8398-dc474d531fc1"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["for column in df_freq.columns.drop([\"IDpol\",\"Exposure\",\"Density\"]):\n","    value_counts = df_freq[column].value_counts(dropna=False).sort_index()\n","    fig = px.bar(x=value_counts.index, y=value_counts.values)\n","    fig.update_layout(\n","        title=f\"Value Counts Bar Chart of: {column}\",\n","        xaxis_title=f\"{column}\",\n","        yaxis_title=\"Count\",\n","        showlegend=False)\n","    fig.show()\n","column=\"Density\"\n","fig = px.histogram(df_freq[column])\n","fig.update_layout(\n","    title=f\"Histogram {column}\",\n","    xaxis_title=\"Values\",\n","    yaxis_title=\"Frequency\",\n","    showlegend=False\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"GGUTrEsHjyma"},"source":["Quick test if the column IDPol is unique: yes they are unique :)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1699568795457,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"XHEKvHU0jyma","outputId":"6c1a266d-0bb2-4f61-bff3-ebf3b5b0129d"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df_freq[\"IDpol\"].value_counts().max()"]},{"cell_type":"markdown","metadata":{"id":"vCf6Ex8qjyma"},"source":["## 2.4 Split: Learn/Val/Test definition:"]},{"cell_type":"markdown","metadata":{"id":"2rzMeYnmjyma"},"source":["In the LocalGLMnet paper they are mentioning that they do it exactly like it is done in the Book Wüthrich & Merz (2021):  \n","> To do a proper out-of-sample generalization analysis we partition the data randomly into a learning data set $L$ and a test data set $T$ . The learning data L contains\n","$n = 610,206$ instances and the test data set $T$ contains $67,801$ instances; we use exactly the same split as in Table 5.2 of Wüthrich & Merz (2021). The learning data L will be used to learn the network parameters and the test data $T$ is used to perform an out-of-sample generalization analysis.\n","\n","So to get the same results I also run the code splitting code in R instead of python and exported the splitting feature to later then import it to python.\n","The R Code looked like this to reproduce the results (Note here the RNGversion!):\n","\n","```R\n","\n","RNGversion(\"3.5.0\")\n","set.seed(100)\n","ll_replicate_papers_2 <- sample (c(1: nrow(freMTPL2freq)) , round(0.9* nrow(freMTPL2freq)), replace = FALSE)\n","learn <- freMTPL2freq[ll_replicate_papers_1 ,]\n","test <- freMTPL2freq[-ll_replicate_papers_1 ,]\n","\n","# Save the list to a text file\n","write.table(learn$IDpol, \"./Data/learn_split_IDpols_2.txt\", row.names = FALSE, col.names = FALSE)\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmS4X8Ywjyma"},"outputs":[],"source":["ids_in_learn = list(np.genfromtxt(f\"{storage_path}/Data/learn_split_IDpols_2.txt\").astype(int))\n","ids_in_test = list(df_freq[~df_freq[\"IDpol\"].isin(ids_in_learn)][\"IDpol\"].astype(int))\n","\n","bool_in_learn = df_freq['IDpol'].isin(ids_in_learn) # be careful if the dataset is not sorted by IDpol\n","bool_in_test = df_freq['IDpol'].isin(ids_in_test) # be careful if the dataset is not sorted by IDpol"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":93},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1699568798607,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"-g5VcSkbjyma","outputId":"b2baf8f8-5d1c-4b84-9053-9b29f04d959f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The learning data L contains so many instances: 610206'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'The test data T contains so many instances: 67801'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Test the resulting portfolio freq (w.r.t Exposure) in learn df:  7.36%'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Test the resulting portfolio freq (w.r.t Exposure) in test df:  7.35%'"]},"metadata":{},"output_type":"display_data"}],"source":["display(f\"The learning data L contains so many instances: {len(ids_in_learn)}\")\n","display(f\"The test data T contains so many instances: {len(ids_in_test)}\")\n","\n","freq_learn = df_freq[bool_in_learn]['ClaimNb'].sum()/df_freq[bool_in_learn]['Exposure'].sum()\n","freq_test = df_freq[bool_in_test]['ClaimNb'].sum()/df_freq[bool_in_test]['Exposure'].sum()\n","display(f\"Test the resulting portfolio freq (w.r.t Exposure) in learn df: {freq_learn: .2%}\")\n","display(f\"Test the resulting portfolio freq (w.r.t Exposure) in test df: {freq_test: .2%}\")"]},{"cell_type":"markdown","metadata":{"id":"5Mw1ByCBjyma"},"source":["We add also a split of the the learn dataset into train and validation: (90% / 10%). In the LocalGLMnet Paper I didn't found a specific split here so a create a new one. We create here 15 train/val splits because we want to fit 15 differant models."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":93},"executionInfo":{"elapsed":2501,"status":"ok","timestamp":1699568801104,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"-XE2xr4Djymb","outputId":"4240a156-7d13-4cd9-bd27-96f1d8fbf17c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Example train/validation split freq: \n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Test the resulting portfolio freq (w.r.t Exposure) in learn df:  7.36%'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Test the resulting portfolio freq (w.r.t Exposure) in learn-train df:  7.37%'"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Test the resulting portfolio freq (w.r.t Exposure) in learn-val df:  7.28%'"]},"metadata":{},"output_type":"display_data"}],"source":["# create 15 new train and validation split with sklearn:\n","train_val_split = {}\n","for run_index in range(15):\n","  temp_learn_train, temp_learn_val = sk.model_selection.train_test_split(df_freq[bool_in_learn][['IDpol']],\n","                                                                        test_size=0.1,\n","                                                                        random_state=random_seeds[run_index])\n","  train_val_split[f\"learn_train_{run_index}\"] = df_freq['IDpol'].isin(temp_learn_train['IDpol']) # be careful if the dataset is not sorted by IDpol\n","  train_val_split[f\"learn_val_{run_index}\"]  = df_freq['IDpol'].isin(temp_learn_val['IDpol']) # be careful if the dataset is not sorted by IDpol\n","\n","print(\"Example train/validation split freq: \")\n","freq_learn_train = df_freq[train_val_split[f\"learn_train_{run_index}\"]]['ClaimNb'\n","                          ].sum()/df_freq[train_val_split[f\"learn_train_{run_index}\"]]['Exposure'].sum()\n","freq_learn_val = df_freq[train_val_split[f\"learn_val_{run_index}\"]]['ClaimNb'\n","                        ].sum()/df_freq[train_val_split[f\"learn_val_{run_index}\"]]['Exposure'].sum()\n","\n","display(f\"Test the resulting portfolio freq (w.r.t Exposure) in learn df: {freq_learn: .2%}\")\n","display(f\"Test the resulting portfolio freq (w.r.t Exposure) in learn-train df: {freq_learn_train: .2%}\")\n","display(f\"Test the resulting portfolio freq (w.r.t Exposure) in learn-val df: {freq_learn_val: .2%}\")\n","del temp_learn_train, temp_learn_val"]},{"cell_type":"markdown","metadata":{"id":"FpIgmUwSjymb"},"source":["Also create a really small learn set (for dummy training of transformers - basically just to check if the code is running)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1699568801104,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"SVBuVEBnjymb","outputId":"8aaecf61-44a6-4711-c37e-2a975bf93639"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Test the resulting portfolio freq (w.r.t Exposure) in learn train dummy df:  7.66%'"]},"metadata":{},"output_type":"display_data"}],"source":["# create a new train and test split with sklearn:\n","temp_1, temp_lean_train_dummy = sk.model_selection.train_test_split(df_freq[train_val_split[f\"learn_train_{run_index}\"]][['IDpol']],\n","                                                                    test_size=0.01,\n","                                                                    random_state=random_seeds[0]+1)\n","bool_in_learn_train_dummy = df_freq['IDpol'].isin(temp_lean_train_dummy['IDpol']) # be careful if the dataset is not sorted by IDpol\n","\n","freq_learn_train_dummy = df_freq[bool_in_learn_train_dummy]['ClaimNb'].sum()/df_freq[bool_in_learn_train_dummy]['Exposure'].sum()\n","\n","display(f\"Test the resulting portfolio freq (w.r.t Exposure) in learn train dummy df: {freq_learn_train_dummy: .2%}\")\n","del temp_1, temp_lean_train_dummy"]},{"cell_type":"markdown","metadata":{"id":"B7Bov2OWjymb"},"source":["## 2.5 Data-Preperation for GLMs"]},{"cell_type":"markdown","metadata":{"id":"Ap67eQXJjymb"},"source":["We use the same data preperation as described in the Book by Wüthrich & Merz (2023)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUBpkSTDjymb"},"outputs":[],"source":["# Copy the dataframe df_freq:\n","df_freq_glm = df_freq.copy()\n","# Area:\n","# is already numerical (due to the mapping above)\n","# VehPower:\n","temp_dict_change_VehPower={}\n","for i,v in enumerate(sorted(df_freq[\"VehPower\"].unique())):\n","    if v <9:\n","        temp_dict_change_VehPower[v]=i+1\n","    else:\n","        temp_dict_change_VehPower[v]=6\n","df_freq_glm[\"VehPower\"] = df_freq[\"VehPower\"].map(temp_dict_change_VehPower).astype('category')\n","# VehAge:\n","# note: this part is different from the one in these papers:\n","# * 2018 Noll Case Study: \"French Motor Third-Party Liability Claims\"\n","# * 2019 Schelldorfer Paper: \"Nesting Classical Actuarial Models into Neural Networks\"\n","# * 2020 Wüthrich Paper: \"From Generalized Linear Models to Neural Networks, and Back\"\n","bins = [0, 6, 13, float('inf')]\n","labels = ['[0, 6)', '[6, 13)', '[13, ∞)']\n","df_freq_glm[\"VehAge\"] = pd.cut(df_freq[\"VehAge\"],bins=bins, labels=labels, right=False).astype('category')\n","# DrivAge:\n","bins = [18, 21, 26, 31, 41, 51, 71, float('inf')]\n","labels = ['[18, 21)', '[21, 26)', '[26, 31)', '[31, 41)', '[41, 51)', '[51, 71)', '[71, ∞)']\n","df_freq_glm[\"DrivAge\"] = pd.cut(df_freq[\"DrivAge\"],bins=bins, labels=labels, right=False).astype('category')\n","df_freq_glm[\"DrivAge_Nr\"] = df_freq[\"DrivAge\"]\n","# BonusMalus:\n","df_freq_glm.loc[df_freq_glm[\"BonusMalus\"] >= 150, \"BonusMalus\"] = 150\n","# VehBrand:\n","# is already categorical (due to the reordering above)\n","# VehGas:\n","# is already categorical (due to the cast above)\n","# Density:\n","df_freq_glm[\"Density\"] = np.log(df_freq_glm[\"Density\"])\n","# Region:\n","# is already categorical\n","\n","# check if we have the same number of features that we need for the glms as in the paper:\n","'''\n","print(\"Check if we have the same number of features that we need for the glms as in the paper\")\n","print(\"------------\")\n","test_dim_feature_space = 0\n","for col in df_freq_glm.select_dtypes(include=[int,float]).columns.drop([\"IDpol\",\"ClaimNb\",\"Exposure\",\"DrivAge_Nr\"]):\n","    display(f\"Dimensions for feature space of {col}: 1\")\n","    test_dim_feature_space+=1\n","for col in df_freq_glm.select_dtypes(include=['category']).columns:\n","    display(f\"Dimensions for feature space of {col}: {len(df_freq_glm[col].cat.categories)-1}\")\n","    test_dim_feature_space=test_dim_feature_space+len(df_freq_glm[col].cat.categories)-1\n","display(f\"Total dimensions for feature space: {test_dim_feature_space}\")\n","'''\n","\n","# Dummy encode all categorical variable for GLM1:\n","X_glm1 = pd.get_dummies(df_freq_glm, columns=df_freq_glm.select_dtypes(include=['category']).columns,drop_first=True).drop(columns=[\"IDpol\",\"ClaimNb\",\"Exposure\",\"DrivAge_Nr\"])\n","X_glm1_learn = X_glm1[bool_in_learn]\n","X_glm1_test = X_glm1[bool_in_test]\n","\n","# Create the new DrivAge (power and log) columns for GLM2:\n","columns_to_drop = [col for col in X_glm1.columns if col.startswith('DrivAge_')]\n","X_glm2 = X_glm1.drop(columns=columns_to_drop)\n","X_glm2[\"DrivAge_1\"] = df_freq_glm[\"DrivAge_Nr\"]\n","X_glm2[\"DrivAge_2\"] = df_freq_glm[\"DrivAge_Nr\"]**2\n","X_glm2[\"DrivAge_3\"] = df_freq_glm[\"DrivAge_Nr\"]**3\n","X_glm2[\"DrivAge_4\"] = df_freq_glm[\"DrivAge_Nr\"]**4\n","X_glm2[\"DrivAge_log\"] = np.log(df_freq_glm[\"DrivAge_Nr\"])\n","X_glm2_learn = X_glm2[bool_in_learn].reset_index(drop=True)\n","means_DrivAge_learn = X_glm2_learn[[col for col in X_glm2_learn.columns if col.startswith('DrivAge_')]].mean()\n","for col in X_glm2_learn.columns:\n","    if col.startswith('DrivAge_'):\n","        X_glm2[col] = np.array(X_glm2[col]/means_DrivAge_learn[col])\n","X_glm2_learn = X_glm2[bool_in_learn].reset_index(drop=True)\n","X_glm2_test = X_glm2[bool_in_test].reset_index(drop=True)\n","\n","# Adding interaction columns to the data frame for GLM3:\n","# one has to be careful here since the dataframes before are reindexed:\n","X_glm3 = X_glm2.copy()\n","X_glm3[\"DrivAge_1_x_BonusMalus\"] = list(df_freq_glm[\"BonusMalus\"]*df_freq_glm[\"DrivAge_Nr\"])\n","X_glm3[\"DrivAge_2_x_BonusMalus\"] = list(df_freq_glm[\"BonusMalus\"]*df_freq_glm[\"DrivAge_Nr\"]**2)\n","X_glm3_learn = X_glm2_learn.copy()\n","X_glm3_learn[\"DrivAge_1_x_BonusMalus\"] = list(df_freq_glm[bool_in_learn][\"BonusMalus\"]*df_freq_glm[bool_in_learn][\"DrivAge_Nr\"])\n","X_glm3_learn[\"DrivAge_2_x_BonusMalus\"] = list(df_freq_glm[bool_in_learn][\"BonusMalus\"]*df_freq_glm[bool_in_learn][\"DrivAge_Nr\"]**2)\n","means_DrivAge_x_BonusMalus_learn = X_glm3_learn[[\"DrivAge_1_x_BonusMalus\", \"DrivAge_2_x_BonusMalus\"]].mean()\n","for col in list(means_DrivAge_x_BonusMalus_learn.index):\n","    X_glm3[col] = np.array(X_glm3[col]/means_DrivAge_x_BonusMalus_learn[col])\n","X_glm3_learn = X_glm3[bool_in_learn].reset_index(drop=True)\n","X_glm3_test = X_glm3[bool_in_test].reset_index(drop=True)\n"]},{"cell_type":"markdown","metadata":{"id":"Qd3GcgdGjymc"},"source":["## 2.6 Data-Preperation as described in the LocalGLMnet Paper:"]},{"cell_type":"markdown","metadata":{"id":"hH-T_L5ojyml"},"source":["In the LocalGLMnet paper they write regarding the data pre-processing:\n","\n","> We pre-process these components as follows: we center and normalize\n","to unit variance the six continuous and the binary components. We apply one-hot encoding to the\n","two categorical variables, we emphasize that we do not use dummy coding as it is usually done in\n","GLMs. Below, in Section 3.6, we are going to motivate this one-hot encoding choice (which does not\n","lead to full rank design matrices); for one-hot encoding vs. dummy coding we refer to formulas (5.21)\n","and (7.29) in Wüthrich & Merz (2021).\n","\n","\n","> As a control variable, we add two random feature components that are $i.i.d.$, centered and with unit\n","variance, the first one having a uniform distribution and the second one having a standard normal\n","distribution, we call these two additional feature components ‘RandU’ and ‘RandN’. We consider\n","two additional independent components to understand whether the distributional choice influences\n","the results of hypothesis testing using the empirical interval $I_\\alpha$, see (16).\n","Altogether (and using one-hot encoding) we receive q = 42 dimensional tabular feature variables $x_i ∈ R^q$; this includes the two additional components RandU and RandN.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GZcmAE9Ijyml"},"source":["So we try now to replicate it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0KAskvFjyml"},"outputs":[],"source":["df_freq_prep_nn = df_freq.copy()\n","# change VehGas to binary:\n","df_freq_prep_nn[\"VehGas\"] = df_freq_prep_nn[\"VehGas\"].map({\"Diesel\":1,\"Regular\":0}).astype(int)\n","\n","nr_col = [\"Area\", \"VehPower\", \"VehAge\", \"DrivAge\", \"BonusMalus\", \"VehGas\", \"Density\"]\n","cat_col = [\"VehBrand\", \"Region\"]\n","\n","# Note: StandardScaler : = (x-mean)/standard_deviation\n","# Since it is good practice we are training the standardscaler (mean and standard_deviation) only the training data and apply it on the hole dataset (including the test data)\n","prep_standardscaler = StandardScaler()\n","prep_standardscaler.fit(df_freq_prep_nn[bool_in_learn][nr_col])\n","\n","df_freq_prep_nn[nr_col] = prep_standardscaler.transform(df_freq_prep_nn[nr_col])\n","# add the dummy columns to the df_freq_prep_nn dataframe:\n","df_freq_prep_nn = pd.concat([df_freq_prep_nn.drop(columns=cat_col),\n","                             pd.get_dummies(df_freq_prep_nn[cat_col], columns=cat_col, drop_first=False).astype(int)\n","                             ], axis=1)\n","# add back the for the categorical columns that have been dropped above:\n","df_freq_prep_nn[list(map(lambda item: \"Cat_\" + item, cat_col))] = df_freq[cat_col]\n","cat_col = list(map(lambda item: \"Cat_\" + item, cat_col))"]},{"cell_type":"markdown","metadata":{"id":"94Llo5h7jyml"},"source":["Add to random features as columns. One that is normal distributed with mean = 0 and variance = 1 and one that is uniform distributed with mean = 0 and variance = 1.\n","\n","Note that the variance of the uniform distribution is $\\displaystyle{\\frac{1}{12}}(b-a)^{2}$. So we choose $b=\\frac{\\sqrt{12}}{2}$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1LaXqZdWsRTb1uDqfnMA6oy5WWaQ-voD8"},"executionInfo":{"elapsed":91107,"status":"ok","timestamp":1699568895622,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"WNUKmEEBjymm","outputId":"4c18996a-d862-4d0c-82c9-1db85f6ba346"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# create a random column that is centered around 0 and has a standard deviation of 1 and has uniform distribution:\n","df_freq_prep_nn[\"RandU\"] = np.random.uniform(-np.sqrt(12)/2,np.sqrt(12)/2,len(df_freq_prep_nn))\n","# create a random column that is centered around 0 and has a standard deviation of 1 and has normal distribution:\n","df_freq_prep_nn[\"RandN\"] = np.random.normal(0,1,len(df_freq_prep_nn))\n","\n","column=\"RandU\"\n","fig = px.histogram(df_freq_prep_nn[column])\n","fig.update_layout(\n","    title=f\"Histogram {column}\",\n","    xaxis_title=\"Values\",\n","    yaxis_title=\"Frequency\",\n","    showlegend=False\n",")\n","fig.show()\n","\n","column=\"RandN\"\n","fig = px.histogram(df_freq_prep_nn[column])\n","fig.update_layout(\n","    title=f\"Histogram {column}\",\n","    xaxis_title=\"Values\",\n","    yaxis_title=\"Frequency\",\n","    showlegend=False\n",")\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"naLkCOhNjymm"},"source":["Check if all numerical features now have the right mean and variance:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":87,"status":"ok","timestamp":1699568895624,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"F9ECHuA9jymm","outputId":"b02ebb1b-afc1-492f-d05d-691f622b108e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Area         -0.000056\n","VehPower     -0.000085\n","VehAge       -0.000090\n","DrivAge       0.000082\n","BonusMalus   -0.000078\n","VehGas       -0.000212\n","Density      -0.000675\n","RandU         0.000565\n","RandN         0.000442\n","dtype: float64\n","Area          0.999080\n","VehPower      0.999729\n","VehAge        0.997462\n","DrivAge       1.000098\n","BonusMalus    1.000379\n","VehGas        0.999993\n","Density       0.996537\n","RandU         1.000032\n","RandN         1.001442\n","dtype: float64\n"]}],"source":["print(df_freq_prep_nn[nr_col + [\"RandU\",\"RandN\"]].mean())\n","print(df_freq_prep_nn[nr_col + [\"RandU\",\"RandN\"]].var())\n"]},{"cell_type":"markdown","metadata":{"id":"OjI_2zDsjymm"},"source":["Adding some encodings for the categorical features, in case we want to use later some embeddings:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeAJxwsGjymm"},"outputs":[],"source":["cat_encoder_all = {}\n","for col in [\"VehBrand\", \"Region\"]:\n","    cat_encoder = {}\n","    unique_cat = df_freq.dtypes[col].categories.to_list()\n","    for i in range(len(unique_cat)):\n","        cat_encoder[unique_cat[i]] = i\n","    cat_encoder_all[col]=cat_encoder # we save the encoder dict incase we will need it later to back transform the results.\n","    df_freq_prep_nn[f\"NN_EMB_{col}\"] = df_freq[col].map(cat_encoder_all[col]).astype(int)"]},{"cell_type":"markdown","metadata":{"id":"sngkMSsYjymm"},"source":["Creating the learning and test datasets for the neural network models:"]},{"cell_type":"markdown","metadata":{"id":"7l3P1gdBJ_Js"},"source":["Note we are not creating here every train and validation split dataset but instead create those when fitting the model.\n","So that we are not polluting the RAM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnk7Xjt8GNI7"},"outputs":[],"source":["# Note we are not creating here every train and validation split dataset but instead create those when fitting the model.\n","# So that we are not polluting the RAM (notebooks have no garbage collector).\n","\n","# Create Datasets for OHE FNN:\n","# ------------------------\n","col_x_fnn_ohe = nr_col + [col for col in df_freq_prep_nn.columns if col.startswith('VehBrand_') or col.startswith('Region_')]\n","def create_ffn_ohe_data(bool_list, exposure_name=\"Exposure\", response_name=\"ClaimNb\"):\n","    X_nn_ohe = np.array(df_freq_prep_nn[bool_list][col_x_fnn_ohe].values)\n","    exposure = np.array(df_freq[bool_list][exposure_name])\n","    y_true= np.array(df_freq[bool_list][response_name])\n","    return [X_nn_ohe, exposure], y_true\n","\n","\n","# Create Datasets for cat embedding FNN:\n","# ------------------------\n","def create_ffn_cat_emb_data(bool_list, exposure_name=\"Exposure\", response_name=\"ClaimNb\"):\n","    X_nn_just_nr = np.array(df_freq_prep_nn[bool_list][nr_col].values),\n","    Input_EMB_VehBrand = np.array(df_freq_prep_nn[bool_list][\"NN_EMB_VehBrand\"].values)\n","    Input_EMB_Region = np.array(df_freq_prep_nn[bool_list][\"NN_EMB_Region\"].values)\n","    exposure = np.array(df_freq_prep_nn[bool_list][exposure_name])\n","    y_true = np.array(df_freq_prep_nn[bool_list][response_name])\n","    return [X_nn_just_nr, Input_EMB_VehBrand, Input_EMB_Region, exposure], y_true\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hbrlygD3cenQ"},"source":["## 2.7 Data-Preperation for Transformer models:"]},{"cell_type":"markdown","metadata":{"id":"mnd_iViNdGub"},"source":["Since the FT transformer need the every feature as a separate tensor we create a new tensor dataset:"]},{"cell_type":"markdown","metadata":{"id":"BfeHJqoGdKey"},"source":["Note we are not creating here every train and validation split dataset but instead create those when fitting the model.\n","So that we are not polluting the RAM. Note that notebooks have usually no garbage collector, so we try to be carefull."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjY0BD6AcfCK"},"outputs":[],"source":["def df_to_tensor(df: pd.DataFrame, feature_cols: list, exposure: str=None, target: str=None, batch_size: int = 512, dummy_data_for_build=False):\n","    \"\"\"\n","    transforms the pandas dataframe to a tensorflow dataset as input for the model\n","\n","    Args:\n","        df (pd dataframe): the pandas dataframe that includes the features\n","        feature_cols (list): the list of feature columns that should be included in the model\n","        exposure (str): if the exposure is included it will be used a a separate input (if None it will be ignored)\n","        target (str): if the target is included it will be used in as a separate input (if None it will be ignored)\n","        batch_size (int): the batch size for the tensorflow dataset\n","        dummy_data_for_build (bool): build a dummy dataset for the model (only for building the model) that is not prefetched (default: False)\n","\n","    Returns:\n","        tensorflow Dataset (Prefetched and Batched)\n","    \"\"\"\n","    if exposure:\n","        feature_cols = feature_cols+[exposure]\n","    temp_dict = {k.lower(): np.array(v).reshape(-1, 1).astype(np.float32, copy=False)\n","                            if v.dtype in [\"float64\",\"float32\",\"int64\",\"int32\"] else\n","                            np.array(v).reshape(-1, 1) for k, v in df[feature_cols].items()}\n","    if target:\n","        temp_input = (temp_dict, np.array(df[target]))\n","    else:\n","        temp_input = (temp_dict)\n","    tf_dataset = tf.data.Dataset.from_tensor_slices(temp_input) # create the tf dataset\n","    tf_dataset = tf_dataset.batch(batch_size) # for parallelizing the calc\n","    if dummy_data_for_build == False:\n","        tf_dataset = tf_dataset.prefetch(batch_size) # Prefetch the data for better performance (helps to overlaps the data preprocessing and model execution)\n","    return tf_dataset\n","\n","cat_vocabulary = {}\n","for c in cat_col:\n","    cat_vocabulary[c] = df_freq_prep_nn.dtypes[c].categories.tolist()\n"]},{"cell_type":"markdown","metadata":{"id":"-2ro9ldJjymm"},"source":["## 2.8 Loss function definition:  "]},{"cell_type":"markdown","metadata":{"id":"R8gdbPNUjymn"},"source":["In the paper there are mentioning that they are using the poisson loss function:\n","\n","> As loss function for parameter fitting and generalization analysis we choose the Poisson deviance loss, which is a distribution adapted and strictly consistent loss function for the mean within the Poisson model, for details we refer to Section 4.1.3 in Wüthrich & Merz (2021).\n","\n","So we create quickly the loss function in this step.\n","* Note: on could also just use sklearn.metrics import mean_poisson_deviance\n","* Note the mean of $d(y, \\mu) = 2*\\left(y \\log \\frac{y}{\\mu} - y + \\mu\\right)$: is the same as the formula (5.28) in the Book (2023)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLZJKvvejymn"},"outputs":[],"source":["# Loss-function (for numpy arrays)\n","# ----------------------\n","def poisson_deviance_loss(y_true, y_pred):\n","    with np.errstate(divide='ignore'):\n","        with warnings.catch_warnings():\n","            warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","            xlogy = np.where(y_true != 0, y_true * np.log(y_true / y_pred), 0)\n","            dev = 2 * (xlogy - y_true + y_pred)\n","    return dev.mean()\n","\n","# Loss-Function\n","# ----------------------\n","# we use our own loss function here (because it is not included in tensorflow in the same way):\n","# normally here i would use the tf loss class (using the LossFunctionWrapper but this does not work on colab\n","# since there is no @keras_export() in the source code...):\n","# @keras.saving.register_keras_serializable(package=\"my_package\", name=\"poisson_loss_for_tf\")\n","@tf.function()\n","def poisson_loss_for_tf(y_true, y_pred, mean=True):\n","    \"\"\"Computes the Poisson loss between y_true and y_pred.\n","\n","    The Poisson loss is the mean of the elements of the `Tensor`\n","    `2 * (y_true * log(y_true / y_pred) - y_true + y_pred)`.\n","\n","    Args:\n","        y_true: A tensor of true values with shape (batch_size,).\n","        y_pred: A tensor of predicted values with shape (batch_size,).\n","\n","    Returns:\n","        The Poisson loss between y_true and y_pred.\n","   \"\"\"\n","   # NOTE: this squeeze is not very professional :) but it does its job right now...\n","    ''' TODO: check if this commented squeeze is needed or not?\n","    if y_pred.shape != y_true.shape:\n","        if y_pred.ndim > y_true.ndim:\n","            y_pred = tf.squeeze(y_pred, [-1])\n","        elif y_pred.ndim < y_true.ndim:\n","                y_true = tf.squeeze(y_true, [-1])\n","    '''\n","    if y_pred.shape != y_true.shape:\n","        y_pred = tf.squeeze(y_pred, [-1])\n","    y_pred = tf.convert_to_tensor(y_pred)\n","    y_true = tf.cast(y_true, y_pred.dtype)\n","    loss = 2 * (y_true * tf.math.log((y_true + keras.backend.epsilon()) / (y_pred + keras.backend.epsilon())) - y_true + y_pred)\n","    if mean:\n","        return keras.backend.mean(loss, axis=-1)\n","    else:\n","        return loss\n","\n","# Loss Function Wrapper\n","# ----------------------\n","class Poisson_loss_for_tf_Wrapped:\n","    def __init__(self, y_true=None, y_pred=None, name=\"poisson_loss_for_tf\"):\n","        self.name = name\n","        self.y_true = y_true\n","        self.y_pred = y_pred\n","    def __call__(self, y_true, y_pred):\n","        return poisson_loss_for_tf(y_true, y_pred)\n","\n","\n","# Loss Metrics.\n","# ----------------------\n","# See here: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric\n","class Poisson_Metric_for_tf(tf.keras.metrics.Metric):\n","    def __init__(self, name='mae', **kwargs):\n","        super(Poisson_Metric_for_tf, self).__init__(name=name, **kwargs)\n","        self.total = self.add_weight(name='total', initializer='zeros')\n","        self.count = self.add_weight(name='count', initializer='zeros')\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        batch_poisson_loss = poisson_loss_for_tf(y_true, y_pred,mean=False)\n","        sum_batch_poisson_loss = tf.reduce_sum(batch_poisson_loss)\n","        num_samples = tf.cast(tf.size(y_true), tf.float32)\n","        if sample_weight is not None:\n","            raise ValueError('Code for sample_weight is not jet implemented')\n","        self.total.assign_add(sum_batch_poisson_loss)\n","        self.count.assign_add(num_samples)\n","\n","    def result(self):\n","        return self.total / self.count\n","\n","    def reset_states(self):\n","        self.total.assign(0)\n","        self.count.assign(0)\n","\n","\n","# note that the function tf.keras.losses.Poisson() is not the same as the poisson_deviance_loss function above.\n","# the function tf.keras.losses.Poisson() is the same as mean(y_pred - y_true * tf.math.log(y_pred + 1e-10))."]},{"cell_type":"markdown","metadata":{"id":"8JCW6DkAjymn"},"source":["## 2.9 Initialize container for results:\n","(dataframe/hash-table/functions that help to store results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqoVhBFdjymn"},"outputs":[],"source":["# init hash tables for results\n","y_pred = {}\n","y_pred[\"train\"]={}\n","y_pred[\"test\"]={}\n","\n","y_true = {}\n","y_true[\"train\"] = np.array(df_freq[bool_in_learn][\"ClaimNb\"])\n","y_true[\"test\"] = np.array(df_freq[bool_in_test][\"ClaimNb\"])\n","\n","exposure = {}\n","exposure[\"train\"] = np.array(df_freq[bool_in_learn][\"Exposure\"])\n","exposure[\"test\"] = np.array(df_freq[bool_in_test][\"Exposure\"])\n","\n","log_exposure = {}\n","log_exposure[\"train\"] = np.array(np.log(df_freq[bool_in_learn][\"Exposure\"]))\n","log_exposure[\"test\"] = np.array(np.log(df_freq[bool_in_test][\"Exposure\"]))\n","\n","epochs_and_time = {}\n","\n","df_results = pd.DataFrame(columns=[\"model\",\n","                                   \"epochs\",\n","                                   \"run_time\",\n","                                   \"# parameters\",\n","                                   \"poisson deviance loss: train\",\n","                                   \"poisson deviance loss: test\",\n","                                   f\"pred-avg-freq: train (obs = {freq_learn: .2%})\",\n","                                   f\"pred-avg-freq: test (obs = {freq_test: .2%})\"])\n","\n","# create a python data class to store the results:\n","@dataclass\n","class Results:\n","    model: str\n","    epochs: int = field(default=None)\n","    run_time: float = field(default=None)\n","    nr_parameters: int = field(default=None)\n","    poisson_deviance_loss_train: float = field(default=None)\n","    poisson_deviance_loss_test: float = field(default=None)\n","    pred_avg_freq_train: float = field(default=None)\n","    pred_avg_freq_test: float = field(default=None)\n","\n","# create a function that stores the results in a dataframe not using append since dataframe object has no attribute append:\n","def store_results_in_df(results):\n","    global df_results\n","    global freq_learn\n","    global freq_test\n","    if len(df_results[df_results[\"model\"]!=results.model])==0:\n","        df_results = pd.DataFrame({\"model\":results.model,\n","                                            \"epochs\":results.epochs,\n","                                            \"run_time\":results.run_time,\n","                                            \"nr_parameters\":results.nr_parameters,\n","                                            \"loss_train\":results.poisson_deviance_loss_train,\n","                                            \"loss_test\":results.poisson_deviance_loss_test,\n","                                            f\"pred_avg_freq_train\":results.pred_avg_freq_train,\n","                                            f\"pred_avg_freq_test\":results.pred_avg_freq_test},\n","                                  index=[0])\n","    else:\n","        df_results = pd.concat([df_results[df_results[\"model\"]!=results.model],\n","                                pd.DataFrame({\"model\":results.model,\n","                                                \"epochs\":results.epochs,\n","                                                \"run_time\":results.run_time,\n","                                                \"nr_parameters\":results.nr_parameters,\n","                                                \"loss_train\":results.poisson_deviance_loss_train,\n","                                                \"loss_test\":results.poisson_deviance_loss_test,\n","                                                f\"pred_avg_freq_train\":results.pred_avg_freq_train,\n","                                                f\"pred_avg_freq_test\":results.pred_avg_freq_test},\n","                                             index=[0])\n","                                ], ignore_index=True).reset_index(drop=True)\n","\n","\n","def calc_avg_df(list_models):\n","    for i, model in enumerate(list_models):\n","        filtered_results = df_results[df_results['model'].str.startswith(model)]\n","        averages = pd.DataFrame(filtered_results.select_dtypes(include=['number']).mean()).T\n","        averages.insert(0, 'model', model)\n","        if i == 0:\n","            df_avg = averages\n","        else:\n","            df_avg = pd.concat([df_avg, averages], ignore_index=True)\n","    return df_avg\n","\n","\n","def calc_std_df(list_models):\n","    for i, model in enumerate(list_models):\n","        filtered_results = df_results[df_results['model'].str.startswith(model)]\n","        averages = pd.DataFrame(filtered_results.select_dtypes(include=['number']).std()).T\n","        averages.insert(0, 'model', model)\n","        if i == 0:\n","            df_std = averages\n","        else:\n","            df_std = pd.concat([df_std, averages], ignore_index=True)\n","    return df_std\n","\n"]},{"cell_type":"markdown","metadata":{"id":"amX41e9cjymn"},"source":["# 3. Benchmark-Models:"]},{"cell_type":"markdown","metadata":{"id":"c7NJtImPjymn"},"source":["Note for a lot of the following models we use the same model architecture as described in the Book by Wüthrich & Merz (2023)"]},{"cell_type":"markdown","metadata":{"id":"QZBEDZnRjymo"},"source":["## 3.1 Mean-Model:"]},{"cell_type":"markdown","metadata":{"id":"Ya_s-PBxjymo"},"source":["Note we run the code 15 times to get the results for an average of the runtimes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxQUjLAljymo"},"outputs":[],"source":["for run_index in range(15):\n","    start_time = time.time()\n","    constant_model=df_freq[bool_in_learn]['ClaimNb'].sum()/df_freq[bool_in_learn]['Exposure'].sum()\n","    end_time = time.time()\n","    execution_time_mean_model = end_time - start_time\n","\n","    y_pred[\"train\"][\"homogeneous model\"] = constant_model*exposure[\"train\"]\n","    y_pred[\"test\"][\"homogeneous model\"] = constant_model*exposure[\"test\"]\n","\n","    mean_model_results = Results(model=f\"homogeneous model (run: {run_index})\",\n","                                    epochs=0,\n","                                    run_time=execution_time_mean_model,\n","                                    nr_parameters=1,\n","                                    poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][\"homogeneous model\"]),\n","                                    poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][\"homogeneous model\"]),\n","                                    pred_avg_freq_train=constant_model,\n","                                    pred_avg_freq_test=constant_model)\n","    # store the results in the dataframe:\n","    store_results_in_df(mean_model_results)\n","\n","# display(df_results)"]},{"cell_type":"markdown","metadata":{"id":"rzj1JR3Vjymo"},"source":["## 3.2 GLM results:"]},{"cell_type":"markdown","metadata":{"id":"Dmsq5nsajymo"},"source":["Due to the fact:\n","> \"GLM, which is currently the industry standard for non-life claim frequency prediction\"\n","\n","We replicate here the results for the GLM (GLM3) that are shown in the LocalGLMnet Paper and the Book by Wüthrich & Merz (2023):  "]},{"cell_type":"markdown","metadata":{"id":"U_JdKWqojymo"},"source":["Note we run the code 15 times to get the results for an average of the runtimes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fnl1Gwp2jymo"},"outputs":[],"source":["# Recreating results GLM1:\n","# -------------------------\n","for run_index in range(15):\n","    start_time = time.time()\n","    poisson_glm1 = PoissonRegressor(alpha = 0,max_iter=1000, solver='newton-cholesky') # scikit-learn.org: alpha = 0 is equivalent to unpenalized GLMs\n","    poisson_glm1.fit(X_glm1_learn,y_true[\"train\"]/exposure[\"train\"],sample_weight=exposure[\"train\"])\n","    end_time = time.time()\n","    execution_time_glm1 = end_time - start_time\n","    # Make predictions using the fitted model\n","    y_pred[\"train\"][\"GLM1\"] = poisson_glm1.predict(X_glm1_learn)*exposure[\"train\"]\n","    y_pred[\"test\"][\"GLM1\"] = poisson_glm1.predict(X_glm1_test)*exposure[\"test\"]\n","    # store the results in the results class:\n","    glm1_results = Results(model=f\"GLM1 (run: {run_index})\",\n","                            epochs=0,\n","                            run_time=execution_time_glm1,\n","                            nr_parameters=len(poisson_glm1.coef_)+len([poisson_glm1.intercept_]),\n","                            poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][\"GLM1\"]),\n","                            poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][\"GLM1\"]),\n","                            pred_avg_freq_train=y_pred[\"train\"][\"GLM1\"].sum()/exposure[\"train\"].sum(),\n","                            pred_avg_freq_test=y_pred[\"test\"][\"GLM1\"].sum()/exposure[\"test\"].sum())\n","    # store the results in the dataframe:\n","    store_results_in_df(glm1_results)\n","\n","\n","# Recreating results GLM2:\n","# -------------------------\n","for run_index in range(15):\n","    start_time = time.time()\n","    poisson_glm2 = PoissonRegressor(alpha = 0,max_iter=1000, solver='newton-cholesky') # scikit-learn.org: alpha = 0 is equivalent to unpenalized GLMs\n","    poisson_glm2.fit(X_glm2_learn,y_true[\"train\"]/exposure[\"train\"],sample_weight=exposure[\"train\"])\n","    end_time = time.time()\n","    execution_time_glm2 = end_time - start_time\n","    # Make predictions using the fitted model\n","    y_pred[\"train\"][\"GLM2\"] = poisson_glm2.predict(X_glm2_learn)*exposure[\"train\"]\n","    y_pred[\"test\"][\"GLM2\"] = poisson_glm2.predict(X_glm2_test)*exposure[\"test\"]\n","    # store the results in the results class:\n","    glm2_results = Results(model=f\"GLM2 (run: {run_index})\",\n","                            epochs=0,\n","                            run_time=execution_time_glm2,\n","                            nr_parameters=len(poisson_glm2.coef_)+len([poisson_glm2.intercept_]),\n","                            poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][\"GLM2\"]),\n","                            poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][\"GLM2\"]),\n","                            pred_avg_freq_train=y_pred[\"train\"][\"GLM2\"].sum()/exposure[\"train\"].sum(),\n","                            pred_avg_freq_test=y_pred[\"test\"][\"GLM2\"].sum()/exposure[\"test\"].sum())\n","    # store the results in the dataframe:\n","    store_results_in_df(glm2_results)\n","\n","\n","# Recreating results GLM3:\n","# -------------------------\n","for run_index in range(15):\n","    start_time = time.time()\n","    poisson_glm3 = PoissonRegressor(alpha = 0,max_iter=1000, solver='newton-cholesky') # scikit-learn.org: alpha = 0 is equivalent to unpenalized GLMs\n","    poisson_glm3.fit(X_glm3_learn,y_true[\"train\"]/exposure[\"train\"],sample_weight=exposure[\"train\"])\n","    end_time = time.time()\n","    execution_time_glm3 = end_time - start_time\n","\n","    # Make predictions using the fitted model\n","    y_pred[\"train\"][\"GLM3\"] = poisson_glm3.predict(X_glm3_learn)*exposure[\"train\"]\n","    y_pred[\"test\"][\"GLM3\"] = poisson_glm3.predict(X_glm3_test)*exposure[\"test\"]\n","\n","    # store the results in the results class:\n","    glm3_results = Results(model=f\"GLM3 (run: {run_index})\",\n","                            epochs=0,\n","                            run_time=execution_time_glm3,\n","                            nr_parameters=len(poisson_glm3.coef_)+len([poisson_glm3.intercept_]),\n","                            poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][\"GLM3\"]),\n","                            poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][\"GLM3\"]),\n","                            pred_avg_freq_train=y_pred[\"train\"][\"GLM3\"].sum()/exposure[\"train\"].sum(),\n","                            pred_avg_freq_test=y_pred[\"test\"][\"GLM3\"].sum()/exposure[\"test\"].sum())\n","    # store the results in the dataframe:\n","    store_results_in_df(glm3_results)\n","# display(df_results)"]},{"cell_type":"markdown","metadata":{"id":"0adpvxc8jymo"},"source":["## 3.3 Feedforward Neural Network OHE:"]},{"cell_type":"markdown","metadata":{"id":"fQdvi-8ujymp"},"source":["Note: we run the code 15 times on different seeds (to calc the avg and std of runtime and results)."]},{"cell_type":"markdown","metadata":{"id":"5E-8_pU6jymp"},"source":["Create and Build the model:"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":231,"status":"ok","timestamp":1699787243888,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"bkmyxbENjymp"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_ohe_learn, y_true_learn = create_ffn_ohe_data(bool_in_learn)\n","data_nn_ohe_test, y_true_test = create_ffn_ohe_data(bool_in_test)\n","\n","for run_index in range(15):\n","    # Create the dataframes needed for training:\n","    data_nn_ohe_learn_train, y_true_learn_train = create_ffn_ohe_data(train_val_split[f\"learn_train_{run_index}\"])\n","    data_nn_ohe_learn_val, y_true_learn_val = create_ffn_ohe_data(train_val_split[f\"learn_val_{run_index}\"])\n","\n","    print(f\"Model: {run_index}\")\n","    # Define FNN Model:\n","    # ----------------------\n","    # note we use here the function api instead of the model subclassing\n","    # to make the code more readable and easier to understand:\n","    # (for the transformer based models we will use model subclasses)\n","    def Create_Poisson_FFN_OHE(input_dim=42,mean_model_results=1):\n","        # set random seeds\n","        set_random_seeds(int(random_seeds[run_index]))\n","        # Build the network\n","        Input_Matrix_OHE = tf.keras.layers.Input(shape=(input_dim,), dtype='float32', name='Input_Matrix')\n","        Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","        hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(Input_Matrix_OHE)\n","        hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","        hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","        Result_FFN1 = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_FFN1',\n","                        weights=[np.zeros((10, 1)), np.array([np.log(mean_model_results)])],\n","                        trainable=True)(hidden3)\n","        Response = tf.keras.layers.Multiply(name='Result')([Result_FFN1, Input_Exposure])\n","        # Define and Return the model\n","        return tf.keras.models.Model(inputs=[Input_Matrix_OHE, Input_Exposure], outputs=[Response], name='Poisson_FFN_OHE')\n","\n","    # create the model:\n","    # ----------------------\n","    FFN_OHE = Create_Poisson_FFN_OHE(input_dim=40,mean_model_results=constant_model)\n","\n","    # Compile the models\n","    # ----------------------\n","    FFN_OHE.compile(optimizer='nadam', loss=poisson_loss_for_tf, metrics=[poisson_loss_for_tf])\n","\n","    # model callbacks:\n","    # ----------------------\n","    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=15, monitor='val_poisson_loss_for_tf', restore_best_weights=True)\n","\n","\n","    # model fitting:\n","    # ----------------------\n","    # model without RandU and RandN:\n","    start_time = time.time()\n","    epochs_OHE=500\n","\n","    FFN_OHE_history = FFN_OHE.fit( x=data_nn_ohe_learn_train,\n","                                   y=y_true_learn_train,\n","                                  validation_data=[data_nn_ohe_learn_val, y_true_learn_val],\n","                                    epochs=epochs_OHE,\n","                                    batch_size=5000,\n","                                    verbose=0,\n","                                    callbacks=[early_stopping_callback]\n","                                    )\n","    end_time = time.time()\n","    execution_time_nn_ohe = end_time - start_time\n","    best_epoch_FFN_ohe = np.argmin(FFN_OHE_history.history['val_poisson_loss_for_tf'])+1\n","\n","    # save models:\n","    # ----------------------\n","    FFN_OHE.save_weights(f'{storage_path}/saved_models/Poisson_FFN_OHE_{run_index}.weights.h5')\n","\n","\n","    # load the saved model weights:\n","    # ----------------------\n","    FFN_OHE.load_weights(f'{storage_path}/saved_models/Poisson_FFN_OHE_{run_index}.weights.h5')\n","\n","\n","    # predict with the models:\n","    # ----------------------\n","    y_pred[\"train\"][\"FFN_OHE\"] = np.array([x for [x] in FFN_OHE.predict(data_nn_ohe_learn,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","    y_pred[\"test\"][\"FFN_OHE\"] = np.array([x for [x] in FFN_OHE.predict(data_nn_ohe_test,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","\n","    # evaluate the models:\n","    # ----------------------\n","    # store the results in the results class:\n","    FFN_OHE_results = Results(model=f\"FFN_OHE (run: {run_index})\",\n","                                epochs=best_epoch_FFN_ohe,\n","                                run_time=execution_time_nn_ohe,\n","                                nr_parameters=[np.sum([np.prod(v.get_shape().as_list()) for v in FFN_OHE.trainable_weights])],\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][\"FFN_OHE\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][\"FFN_OHE\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][\"FFN_OHE\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][\"FFN_OHE\"].sum()/exposure[\"test\"].sum())\n","\n","    # store the results in the result-dataframe:\n","    store_results_in_df(FFN_OHE_results)\n","# display(df_results)\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_ohe_learn, data_nn_ohe_test, data_nn_ohe_learn_train, data_nn_ohe_learn_val\n"]},{"cell_type":"markdown","metadata":{"id":"10ICfGxV_oOq"},"source":["NOTE: in the case of FNN_OHE:\n","if i use the tf-dataframes instead of the array inputs: the fit is 4-5 times slower and i get way worse results...\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ur8BMzqajymp"},"source":["## 3.4 Feedforward Neural Network with Categorical Embeddings:\n"]},{"cell_type":"markdown","metadata":{"id":"65TVT5jEjymp"},"source":["Note: we run the code 15 times on different seeds (to calc the avg and std of runtime and results)."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699787254652,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"gabNPqKxjymp"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_emb_learn, y_true_learn = create_ffn_cat_emb_data(bool_in_learn)\n","data_nn_emb_test, y_true_test = create_ffn_cat_emb_data(bool_in_test)\n","\n","for run_index in range(15):\n","    # Create the dataframes needed for training:\n","    data_nn_emb_learn_train, y_true_learn_train = create_ffn_cat_emb_data(train_val_split[f\"learn_train_{run_index}\"])\n","    data_nn_emb_learn_val, y_true_learn_val = create_ffn_cat_emb_data(train_val_split[f\"learn_val_{run_index}\"])\n","\n","    # Define FNN with Cat. Embedding Model:\n","    # ----------------------\n","    # note we use here the function api instead of the model subclassing\n","    # to make the code more readable and easier to understand:\n","    # (for the transformer based models we will use model subclasses)\n","    print(f\"Model: {run_index}\")\n","    def Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=1,mean_model_results=1):\n","        # set random seeds\n","        set_random_seeds(int(random_seeds[run_index]))\n","\n","        Input_Matrix_Num = tf.keras.layers.Input(shape=(input_nr_dim,), dtype='float32', name='Input_Matrix_Num')\n","        Input_VehBrand = tf.keras.layers.Input(shape=(1,), name='Input_VehBrand')\n","        Input_Region = tf.keras.layers.Input(shape=(1,), name='Input_Region')\n","        Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","\n","        All_Inputs = [Input_Matrix_Num,Input_VehBrand,Input_Region,Input_Exposure]\n","\n","        Emb_VehBrand = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"VehBrand\"].keys()),output_dim=emb_dim,\n","                                embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05 ),\n","                                name=\"Embedding_VehBrand\")(Input_VehBrand)\n","        Emb_Region = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"Region\"].keys()),output_dim=emb_dim,\n","                                embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05),\n","                            name=\"Embedding_Region\")(Input_Region)\n","\n","        Reshaped_Emb_VehBrand = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_VehBrand\")(Emb_VehBrand)\n","        Reshaped_Emb_Region = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_Region\")(Emb_Region)\n","\n","        concatenation_layer = tf.keras.layers.Concatenate(name=\"concatenation_layer\")([Input_Matrix_Num,Reshaped_Emb_VehBrand,Reshaped_Emb_Region])\n","\n","        # Build the network\n","        hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(concatenation_layer)\n","        hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","        hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","        Result_FFN1 = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_FFN1',\n","                        weights=[np.zeros((10, 1)), np.array([np.log(mean_model_results)])],\n","                        trainable=True)(hidden3)\n","\n","        Response = tf.keras.layers.Multiply(name='Result')([Result_FFN1, Input_Exposure])\n","\n","        # Define the model\n","        return tf.keras.models.Model(inputs=All_Inputs, outputs=[Response], name='Poisson_CAT_EMB')\n","\n","    # create the model:\n","    # ----------------------\n","    emb_dim=2\n","    FNN_CAT_EMB = Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=emb_dim,mean_model_results=constant_model)\n","\n","    # Compile the model\n","    # ----------------------\n","    FNN_CAT_EMB.compile(optimizer='nadam', loss=poisson_loss_for_tf, metrics=[poisson_loss_for_tf])\n","\n","    # model callbacks:\n","    # ----------------------\n","    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=15, monitor='val_poisson_loss_for_tf', restore_best_weights=True)\n","\n","    # model fitting:\n","    # ----------------------\n","    start_time = time.time()\n","    epochs_CAT_EMB=500\n","    FNN_CAT_EMB_history = FNN_CAT_EMB.fit(x=data_nn_emb_learn_train,\n","                                    y=y_true_learn_train,\n","                                    validation_data=[data_nn_emb_learn_val, y_true_learn_val],\n","                                    epochs=epochs_CAT_EMB,\n","                                    batch_size=7000,\n","                                    verbose=0,\n","                                    callbacks=[early_stopping_callback]\n","                                    )\n","\n","    end_time = time.time()\n","    execution_time_FNN_CAT_EMB = end_time - start_time\n","    best_epoch_FNN_CAT_EMB = np.argmin(FNN_CAT_EMB_history.history['val_poisson_loss_for_tf'])+1\n","\n","    # save models:\n","    # ----------------------\n","    FNN_CAT_EMB.save_weights(f'{storage_path}/saved_models/Poisson_FNN_CAT_EMB_{run_index}.weights.h5')\n","\n","    # load the saved model weights:\n","    # ----------------------\n","    FNN_CAT_EMB.load_weights(f'{storage_path}/saved_models/Poisson_FNN_CAT_EMB_{run_index}.weights.h5')\n","\n","    # predict with the model:\n","    # ----------------------\n","    y_pred[\"train\"][\"FNN_CAT_EMB\"] = np.array([x for [x] in FNN_CAT_EMB.predict(data_nn_emb_learn,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","    y_pred[\"test\"][\"FNN_CAT_EMB\"] = np.array([x for [x] in FNN_CAT_EMB.predict(data_nn_emb_test,\n","                                                                    batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","    # evaluate the model:\n","    # ----------------------\n","    FNN_CAT_EMB_results = Results(model=f\"FNN_CAT_EMB (run: {run_index})\",\n","                                epochs=best_epoch_FNN_CAT_EMB,\n","                                run_time=execution_time_FNN_CAT_EMB,\n","                                nr_parameters=[np.sum([np.prod(v.get_shape().as_list()) for v in FNN_CAT_EMB.trainable_weights])],\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][\"FNN_CAT_EMB\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][\"FNN_CAT_EMB\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][\"FNN_CAT_EMB\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][\"FNN_CAT_EMB\"].sum()/exposure[\"test\"].sum())\n","    store_results_in_df(FNN_CAT_EMB_results)\n","# display(df_results)\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_emb_learn, data_nn_emb_test, data_nn_emb_learn_train, data_nn_emb_learn_val"]},{"cell_type":"markdown","metadata":{"id":"qsvACKJID4ti"},"source":["## 3.5 CANN (GLM3 and FNN_CAT_EMB):"]},{"cell_type":"markdown","metadata":{"id":"EtnDoeUJIXHG"},"source":["Note: we run the code 15 times on different seeds (to calc the avg and std of runtime and results)."]},{"cell_type":"markdown","metadata":{"id":"AR_ytor4QRBD"},"source":["Note: the Code for the CANN below is basically the same code as for the FNN with categorical embeddings. The only changes are\n","* we use a other exposure column (Exposure_x_GLM3_pred instead of Exposure)\n","* we set the initial weights and bias of the last layer to zero"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQldY6Y0Tt2h"},"outputs":[],"source":["# create the new exposure times GLM3_pred column for CANN models.\n","df_freq_prep_nn[\"Exposure_x_GLM3_pred\"] = list(poisson_glm3.predict(X_glm3)*df_freq_prep_nn[\"Exposure\"])"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":356,"status":"ok","timestamp":1699787265148,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"RKIfZOQuD4eO"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_emb_learn, y_true_learn = create_ffn_cat_emb_data(bool_in_learn, exposure_name = \"Exposure_x_GLM3_pred\")\n","data_nn_emb_test, y_true_test = create_ffn_cat_emb_data(bool_in_test, exposure_name = \"Exposure_x_GLM3_pred\")\n","\n","for run_index in range(15):\n","    # Create the dataframes needed for training:\n","    data_nn_emb_learn_train, y_true_learn_train = create_ffn_cat_emb_data(train_val_split[f\"learn_train_{run_index}\"],\n","                                                                          exposure_name = \"Exposure_x_GLM3_pred\")\n","    data_nn_emb_learn_val, y_true_learn_val = create_ffn_cat_emb_data(train_val_split[f\"learn_val_{run_index}\"],\n","                                                                      exposure_name = \"Exposure_x_GLM3_pred\")\n","\n","    # Define FNN with Cat. Embedding Model:\n","    # ----------------------\n","    # note we use here the function api instead of the model subclassing\n","    # to make the code more readable and easier to understand:\n","    # (for the transformer based models we will use model subclasses)\n","    print(f\"Model: {run_index}\")\n","    def Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=1,mean_model_results=1):\n","        # set random seeds\n","        set_random_seeds(int(random_seeds[run_index]))\n","\n","        Input_Matrix_Num = tf.keras.layers.Input(shape=(input_nr_dim,), dtype='float32', name='Input_Matrix_Num')\n","        Input_VehBrand = tf.keras.layers.Input(shape=(1,), name='Input_VehBrand')\n","        Input_Region = tf.keras.layers.Input(shape=(1,), name='Input_Region')\n","        Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","\n","        All_Inputs = [Input_Matrix_Num,Input_VehBrand,Input_Region,Input_Exposure]\n","\n","        Emb_VehBrand = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"VehBrand\"].keys()),output_dim=emb_dim,\n","                                embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05 ),\n","                                name=\"Embedding_VehBrand\")(Input_VehBrand)\n","        Emb_Region = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"Region\"].keys()),output_dim=emb_dim,\n","                                embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05),\n","                            name=\"Embedding_Region\")(Input_Region)\n","\n","        Reshaped_Emb_VehBrand = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_VehBrand\")(Emb_VehBrand)\n","        Reshaped_Emb_Region = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_Region\")(Emb_Region)\n","\n","        concatenation_layer = tf.keras.layers.Concatenate(name=\"concatenation_layer\")([Input_Matrix_Num,Reshaped_Emb_VehBrand,Reshaped_Emb_Region])\n","\n","        # Build the network\n","        hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(concatenation_layer)\n","        hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","        hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","        Result_FFN1 = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_FFN1',\n","                        weights=[np.zeros((10, 1)), np.array([0])],\n","                        trainable=True)(hidden3)\n","\n","        Response = tf.keras.layers.Multiply(name='Result')([Result_FFN1, Input_Exposure])\n","\n","        # Define the model\n","        return tf.keras.models.Model(inputs=All_Inputs, outputs=[Response], name='Poisson_CAT_EMB')\n","\n","    # create the model:\n","    # ----------------------\n","    emb_dim=2\n","    CANN = Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=emb_dim,mean_model_results=constant_model)\n","\n","    # Compile the model\n","    # ----------------------\n","    CANN.compile(optimizer='nadam', loss=poisson_loss_for_tf, metrics=[poisson_loss_for_tf])\n","\n","    # model callbacks:\n","    # ----------------------\n","    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=15, monitor='val_poisson_loss_for_tf', restore_best_weights=True)\n","\n","    # model fitting:\n","    # ----------------------\n","    start_time = time.time()\n","    epochs_CAT_EMB=500\n","\n","    CANN_history = CANN.fit(x=data_nn_emb_learn_train,\n","                                    y=y_true_learn_train,\n","                                    validation_data=[data_nn_emb_learn_val, y_true_learn_val],\n","                                    epochs=epochs_CAT_EMB,\n","                                    batch_size=7000,\n","                                    verbose=0,\n","                                    callbacks=[early_stopping_callback]\n","                                    )\n","\n","    end_time = time.time()\n","\n","    execution_time_CANN = end_time - start_time\n","    best_epoch_CANN = np.argmin(CANN_history.history['val_poisson_loss_for_tf'])+1\n","\n","    # save models:\n","    # ----------------------\n","    CANN.save_weights(f'{storage_path}/saved_models/Poisson_CANN_{run_index}.weights.h5')\n","\n","    # load the saved model weights:\n","    # ----------------------\n","    CANN.load_weights(f'{storage_path}/saved_models/Poisson_CANN_{run_index}.weights.h5')\n","\n","\n","    # predict with the model:\n","    # ----------------------\n","    y_pred[\"train\"][\"CANN\"] = np.array([x for [x] in CANN.predict(data_nn_emb_learn,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","    y_pred[\"test\"][\"CANN\"] = np.array([x for [x] in CANN.predict(data_nn_emb_test,\n","                                                                    batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","    # evaluate the model:\n","    # ----------------------\n","    CANN_results = Results(model=f\"CANN (run: {run_index})\",\n","                                epochs=best_epoch_CANN,\n","                                run_time=execution_time_CANN,\n","                                nr_parameters=[np.sum([np.prod(v.get_shape().as_list()) for v in CANN.trainable_weights])],\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][\"CANN\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][\"CANN\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][\"CANN\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][\"CANN\"].sum()/exposure[\"test\"].sum())\n","    store_results_in_df(CANN_results)\n","# display(df_results)\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_emb_learn, data_nn_emb_test, data_nn_emb_learn_train, data_nn_emb_learn_val\n"]},{"cell_type":"markdown","metadata":{"id":"Ms5YwgH7jymq"},"source":["## 3.6 LocalGLMnets OHE\n","(excl. Random Features):"]},{"cell_type":"markdown","metadata":{"id":"la5nsKcjjymq"},"source":["Note: we run the code 15 times on different seeds (to calc the avg and std of runtime and results)."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":233,"status":"ok","timestamp":1699787273177,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"fn7AAk8Wjymq"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_ohe_learn, y_true_learn = create_ffn_ohe_data(bool_in_learn)\n","data_nn_ohe_test, y_true_test = create_ffn_ohe_data(bool_in_test)\n","\n","for run_index in range(15):\n","    # Create the dataframes needed for training:\n","    data_nn_ohe_learn_train, y_true_learn_train = create_ffn_ohe_data(train_val_split[f\"learn_train_{run_index}\"])\n","    data_nn_ohe_learn_val, y_true_learn_val = create_ffn_ohe_data(train_val_split[f\"learn_val_{run_index}\"])\n","\n","    print(f\"Model: {run_index}\")\n","    # Define FNN with Cat. Embedding Model:\n","    # ----------------------\n","    # note we use here the function api instead of the model subclassing\n","    # to make the code more readable and easier to understand:\n","    # (for the transformer based models we will use model subclasses)\n","\n","    # create dummy glm for initial weights\n","    # ----------------------\n","    poisson_glm_dummy = PoissonRegressor(alpha = 0,max_iter=1000) # scikit-learn.org: alpha = 0 is equivalent to unpenalized GLMs\n","    poisson_glm_dummy.fit(data_nn_ohe_learn[0],y_true_learn/data_nn_ohe_learn[1],sample_weight=data_nn_ohe_learn[1]) # note: data_nn_ohe_learn = [X_ohe,exposure]\n","\n","    # Define LocalGLMnet:\n","    # ----------------------\n","    def Create_Poisson_LocalGLMnet(input_dim=40,initial_glm_bias=1, initial_glm_betas=None):\n","        # set random seeds\n","        set_random_seeds(int(random_seeds[run_index]))\n","        Input_Matrix_OHE = tf.keras.layers.Input(shape=(input_dim,), dtype='float32', name='Input_Matrix')\n","        Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","        # Build the network\n","        hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(Input_Matrix_OHE)\n","        hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","        hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","        Attention = tf.keras.layers.Dense(units=input_dim, activation='linear', name='attention',\n","                        weights=[np.zeros((10, input_dim)), initial_glm_betas])(hidden3)\n","        # note that the weights are set to 0 and the bias is set to the initial glm betas\n","        # create a layer that calculates the dot product between the attention weights (Attention) and the input matrix Input_Matrix_OHE:\n","        # (Attention has the same dimension as the input matrix Input_Matrix_OHE):\n","        weighted_input = tf.keras.layers.Multiply(name='feature_contributions')([Attention, Input_Matrix_OHE])\n","        scalar_product = tf.keras.layers.Dense(units=1, activation='linear', name='scalar_product',\n","                            weights=[np.ones((input_dim, 1)), np.array([0])],\n","                            trainable=False)(weighted_input)\n","        # Note that we actually don't want to make the following weights trainable,\n","        # but to get the bias to be trainable we need to do so. see comment in Book Wüthrich & Merz (2023) page 500\n","        Result_LocalGLMnet_without_Exposure = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_LocalGLMnet_without_Exposure',\n","                        weights=[np.ones((1, 1)), np.array([initial_glm_bias])],\n","                        trainable=True)(scalar_product)\n","        Response = tf.keras.layers.Multiply(name='Result')([Result_LocalGLMnet_without_Exposure, Input_Exposure])\n","        return tf.keras.models.Model(inputs=[Input_Matrix_OHE, Input_Exposure], outputs=[Response], name='Poisson_LocalGLMnet')\n","\n","    # create the model:\n","    # ----------------------\n","    LocalGLMnet = Create_Poisson_LocalGLMnet(input_dim=40,initial_glm_bias=poisson_glm_dummy.intercept_,initial_glm_betas=poisson_glm_dummy.coef_)\n","\n","    # Compile the model\n","    # ----------------------\n","    LocalGLMnet.compile(optimizer='nadam', loss=poisson_loss_for_tf, metrics=[poisson_loss_for_tf])\n","\n","    # model callbacks:\n","    # ----------------------\n","    early_stopping_callback = tf.keras.callbacks.EarlyStopping(patience=15, monitor='val_poisson_loss_for_tf', restore_best_weights=True)\n","\n","    # model fitting:\n","    # ----------------------\n","    start_time = time.time()\n","    epochs_OHE=500\n","    LocalGLMnet_history = LocalGLMnet.fit(x=data_nn_ohe_learn_train,\n","                                          y=y_true_learn_train,\n","                                          validation_data=[data_nn_ohe_learn_val, y_true_learn_val],\n","                                          epochs=epochs_OHE,\n","                                          batch_size=5000,\n","                                          verbose=0,\n","                                          callbacks=[early_stopping_callback]\n","                                          )\n","    end_time = time.time()\n","    execution_time_LocalGLMnet = end_time - start_time\n","    best_epoch_LocalGLMnet = np.argmin(LocalGLMnet_history.history['val_poisson_loss_for_tf'])+1\n","\n","    # save models:\n","    # ----------------------\n","    LocalGLMnet.save_weights(f'{storage_path}/saved_models/Poisson_LocalGLMnet_{run_index}.weights.h5')\n","\n","    # load the saved model weights:\n","    # ----------------------\n","    LocalGLMnet.load_weights(f'{storage_path}/saved_models/Poisson_LocalGLMnet_{run_index}.weights.h5')\n","\n","    # predict with the model:\n","    # ----------------------\n","    y_pred[\"train\"][\"LocalGLMnet\"] = np.array([x for [x] in LocalGLMnet.predict(data_nn_ohe_learn,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","    y_pred[\"test\"][\"LocalGLMnet\"] = np.array([x for [x] in LocalGLMnet.predict(data_nn_ohe_test,\n","                                                                    batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","    # evaluate the model:\n","    # ----------------------\n","    LocalGLMnet_results = Results(model=f\"LocalGLMnet (run: {run_index})\",\n","                                epochs=best_epoch_LocalGLMnet,\n","                                run_time=execution_time_LocalGLMnet,\n","                                nr_parameters=[np.sum([np.prod(v.get_shape().as_list()) for v in LocalGLMnet.trainable_weights])],\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][\"LocalGLMnet\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][\"LocalGLMnet\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][\"LocalGLMnet\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][\"LocalGLMnet\"].sum()/exposure[\"test\"].sum())\n","    # store the results in the dataframe:\n","    store_results_in_df(LocalGLMnet_results)\n","# display(df_results)\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_ohe_learn, data_nn_ohe_test, data_nn_ohe_learn_train, data_nn_ohe_learn_val"]},{"cell_type":"markdown","metadata":{"id":"g9-LDq0Ejymq"},"source":["## 3.7 Compare Benchmark Results:\n","to those in the LocalGLM Paper and Wüthrich & Merz Book (2023):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzbmYL4gjSaB"},"outputs":[],"source":["# # save the results:\n","# with open(f'{storage_path}/Data/df_results.pickle', 'wb') as handle:\n","#     pickle.dump(df_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# load the results:\n","with open(f'{storage_path}/Data/df_results.pickle', 'rb') as handle:\n","    df_results = pickle.load(handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDaNGET_jymq"},"outputs":[],"source":["# display(df_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":620},"executionInfo":{"elapsed":419,"status":"ok","timestamp":1699573192872,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"C1mnI-L3DIRz","outputId":"e940b4a4-7dbf-4ec3-e665-b0d2a92c7c3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results Average:\n"]},{"data":{"text/html":["\n","  <div id=\"df-60c368f6-68ec-4a3f-b51a-13cf9676cc9d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>homogeneous model</td>\n","      <td>0.000000</td>\n","      <td>0.054847</td>\n","      <td>1.0</td>\n","      <td>0.252132</td>\n","      <td>0.254454</td>\n","      <td>0.073631</td>\n","      <td>0.073631</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GLM1</td>\n","      <td>0.000000</td>\n","      <td>2.220158</td>\n","      <td>49.0</td>\n","      <td>0.241015</td>\n","      <td>0.241463</td>\n","      <td>0.073631</td>\n","      <td>0.073900</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GLM2</td>\n","      <td>0.000000</td>\n","      <td>2.752693</td>\n","      <td>48.0</td>\n","      <td>0.240911</td>\n","      <td>0.241125</td>\n","      <td>0.073631</td>\n","      <td>0.073981</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GLM3</td>\n","      <td>0.000000</td>\n","      <td>1.900497</td>\n","      <td>50.0</td>\n","      <td>0.240844</td>\n","      <td>0.241022</td>\n","      <td>0.073631</td>\n","      <td>0.074048</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>FFN_OHE</td>\n","      <td>42.200000</td>\n","      <td>37.805560</td>\n","      <td>1306.0</td>\n","      <td>0.237535</td>\n","      <td>0.238652</td>\n","      <td>0.073906</td>\n","      <td>0.074310</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>FNN_CAT_EMB</td>\n","      <td>72.933333</td>\n","      <td>58.728892</td>\n","      <td>792.0</td>\n","      <td>0.237682</td>\n","      <td>0.238267</td>\n","      <td>0.073774</td>\n","      <td>0.074238</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>CANN</td>\n","      <td>90.333333</td>\n","      <td>68.559120</td>\n","      <td>792.0</td>\n","      <td>0.237420</td>\n","      <td>0.238102</td>\n","      <td>0.074019</td>\n","      <td>0.074438</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LocalGLMnet</td>\n","      <td>25.333333</td>\n","      <td>29.720892</td>\n","      <td>1737.0</td>\n","      <td>0.237095</td>\n","      <td>0.239211</td>\n","      <td>0.073825</td>\n","      <td>0.074267</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60c368f6-68ec-4a3f-b51a-13cf9676cc9d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-60c368f6-68ec-4a3f-b51a-13cf9676cc9d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-60c368f6-68ec-4a3f-b51a-13cf9676cc9d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-b4d71c6a-08b5-4ee4-85b9-55002f9e647e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b4d71c6a-08b5-4ee4-85b9-55002f9e647e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-b4d71c6a-08b5-4ee4-85b9-55002f9e647e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["               model     epochs   run_time  nr_parameters  loss_train  \\\n","0  homogeneous model   0.000000   0.054847            1.0    0.252132   \n","1               GLM1   0.000000   2.220158           49.0    0.241015   \n","2               GLM2   0.000000   2.752693           48.0    0.240911   \n","3               GLM3   0.000000   1.900497           50.0    0.240844   \n","4            FFN_OHE  42.200000  37.805560         1306.0    0.237535   \n","5        FNN_CAT_EMB  72.933333  58.728892          792.0    0.237682   \n","6               CANN  90.333333  68.559120          792.0    0.237420   \n","7        LocalGLMnet  25.333333  29.720892         1737.0    0.237095   \n","\n","   loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0   0.254454             0.073631            0.073631  \n","1   0.241463             0.073631            0.073900  \n","2   0.241125             0.073631            0.073981  \n","3   0.241022             0.073631            0.074048  \n","4   0.238652             0.073906            0.074310  \n","5   0.238267             0.073774            0.074238  \n","6   0.238102             0.074019            0.074438  \n","7   0.239211             0.073825            0.074267  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Results Standard-Deviation:\n"]},{"data":{"text/html":["\n","  <div id=\"df-e0f3dc27-48a4-48d9-af5f-09c90f05030c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>homogeneous model</td>\n","      <td>0.000000</td>\n","      <td>0.002512</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>5.745950e-17</td>\n","      <td>2.872975e-17</td>\n","      <td>2.872975e-17</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GLM1</td>\n","      <td>0.000000</td>\n","      <td>0.473819</td>\n","      <td>0.0</td>\n","      <td>5.745950e-17</td>\n","      <td>5.745950e-17</td>\n","      <td>1.436488e-17</td>\n","      <td>0.000000e+00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GLM2</td>\n","      <td>0.000000</td>\n","      <td>0.970156</td>\n","      <td>0.0</td>\n","      <td>5.745950e-17</td>\n","      <td>2.872975e-17</td>\n","      <td>0.000000e+00</td>\n","      <td>1.436488e-17</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GLM3</td>\n","      <td>0.000000</td>\n","      <td>0.408252</td>\n","      <td>0.0</td>\n","      <td>2.872975e-17</td>\n","      <td>2.872975e-17</td>\n","      <td>0.000000e+00</td>\n","      <td>1.436488e-17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>FFN_OHE</td>\n","      <td>14.663853</td>\n","      <td>8.624492</td>\n","      <td>0.0</td>\n","      <td>3.255191e-04</td>\n","      <td>1.570462e-04</td>\n","      <td>1.223993e-03</td>\n","      <td>1.209107e-03</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>FNN_CAT_EMB</td>\n","      <td>21.661245</td>\n","      <td>13.907265</td>\n","      <td>0.0</td>\n","      <td>1.590947e-04</td>\n","      <td>1.514444e-04</td>\n","      <td>1.071399e-03</td>\n","      <td>1.088943e-03</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>CANN</td>\n","      <td>53.898935</td>\n","      <td>33.162059</td>\n","      <td>0.0</td>\n","      <td>6.076588e-04</td>\n","      <td>3.253586e-04</td>\n","      <td>1.111365e-03</td>\n","      <td>1.103431e-03</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LocalGLMnet</td>\n","      <td>7.622023</td>\n","      <td>5.097405</td>\n","      <td>0.0</td>\n","      <td>3.340630e-04</td>\n","      <td>2.176521e-04</td>\n","      <td>8.787938e-04</td>\n","      <td>9.078453e-04</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0f3dc27-48a4-48d9-af5f-09c90f05030c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e0f3dc27-48a4-48d9-af5f-09c90f05030c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e0f3dc27-48a4-48d9-af5f-09c90f05030c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e50fe240-f7e6-4339-b99d-6180b4e8c4b1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e50fe240-f7e6-4339-b99d-6180b4e8c4b1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e50fe240-f7e6-4339-b99d-6180b4e8c4b1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["               model     epochs   run_time  nr_parameters    loss_train  \\\n","0  homogeneous model   0.000000   0.002512            0.0  0.000000e+00   \n","1               GLM1   0.000000   0.473819            0.0  5.745950e-17   \n","2               GLM2   0.000000   0.970156            0.0  5.745950e-17   \n","3               GLM3   0.000000   0.408252            0.0  2.872975e-17   \n","4            FFN_OHE  14.663853   8.624492            0.0  3.255191e-04   \n","5        FNN_CAT_EMB  21.661245  13.907265            0.0  1.590947e-04   \n","6               CANN  53.898935  33.162059            0.0  6.076588e-04   \n","7        LocalGLMnet   7.622023   5.097405            0.0  3.340630e-04   \n","\n","      loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0  5.745950e-17         2.872975e-17        2.872975e-17  \n","1  5.745950e-17         1.436488e-17        0.000000e+00  \n","2  2.872975e-17         0.000000e+00        1.436488e-17  \n","3  2.872975e-17         0.000000e+00        1.436488e-17  \n","4  1.570462e-04         1.223993e-03        1.209107e-03  \n","5  1.514444e-04         1.071399e-03        1.088943e-03  \n","6  3.253586e-04         1.111365e-03        1.103431e-03  \n","7  2.176521e-04         8.787938e-04        9.078453e-04  "]},"metadata":{},"output_type":"display_data"}],"source":["print(\"Results Average:\")\n","display(calc_avg_df([\"homogeneous model\",\"GLM1\",\"GLM2\",\"GLM3\",\"FFN_OHE\",\"FNN_CAT_EMB\",\"CANN\",\"LocalGLMnet\"]))\n","print(\"Results Standard-Deviation:\")\n","display(calc_std_df([\"homogeneous model\",\"GLM1\",\"GLM2\",\"GLM3\",\"FFN_OHE\",\"FNN_CAT_EMB\",\"CANN\",\"LocalGLMnet\"]))"]},{"cell_type":"markdown","metadata":{"id":"LjrC4DzIjymq"},"source":["Note that we have the same results as described in the book 2023 by Wüthrich and Merz for the Mean and GLM Models:\n","\n","| Model | In-sample | Out-of-sample |\n","|-------|-----------|---------------|\n","| Poisson null | 25.213 | 25.445 |\n","| Poisson GLM3 | 24.101 | 24.146 |\n","| Poisson GLM3 | 24.091 | 24.113 |\n","| Poisson GLM3 | 24.084 | 24.102 |\n","\n","And for the other models we have very similar results compared to those in the paper LocalGLMnet (2023):\n","(Our Results to those in the following table from from the paper (results from the paper): (a),(b),(d),(e),(f). We changed here the validation set for LocalGLMnet from 20% (in Paper) to 10% but the results is very similar the one in the 2023 LocalGLMnet paper (d) (see our std analysis).\n","\n","| Model | In-sample | Out-of-sample |\n","|-------|-----------|---------------|\n","| (a) null model | 25.213 | 25.445 |\n","| (b) FFN network | 23.764 | 23.873 |\n","| (c) LocalGLMnet | 23.728 | 23.945 |\n","| (d) reduced LocalGLMnet | 23.714 | 23.912 |\n","| (e) Poisson GLM3 | 24.084 | 24.102 |\n","| (f) Categorical Embedding network | 23.690 | 23.824 |\n","| (g) Nagging network | 23.691 | 23.783 |"]},{"cell_type":"markdown","metadata":{"id":"IEViNXYcjymr"},"source":["# 4. Transformer Models"]},{"cell_type":"markdown","metadata":{"id":"8X_3qCXojyms"},"source":["## 4.1 FT-Transformer-Model:"]},{"cell_type":"markdown","metadata":{"id":"WBWfWoyXjyms"},"source":["The FT-Transformer Model was introduced by Gorishniy et al 2021.\n","\n","So the code ideas are based on the code from [gorishniy2021revisiting]\n","\n","Please see the paper for more details: https://arxiv.org/abs/2106.11959\n","\n","Please see here their code written for torch nn's: https://github.com/Yura52/rtdl\n","\n","References:\n","        * [gorishniy2021revisiting]  Gorishniy, Rubachev, Khrulkov, Babenko \"Revisiting Deep Learning Models for Tabular Data\" 2021\n"]},{"cell_type":"markdown","metadata":{"id":"spa-ASdtmNFF"},"source":["NOTE: I rewrote the original code quite a bit to suit our purpose, one can find my oop code (on the tensorflow framework) in the helper folder provided with this notebook."]},{"cell_type":"markdown","metadata":{"id":"wOAMlX3Vn9De"},"source":["NOTE: Below we use instead of the .fit function a costum training loop (to have a extra bit of freedom)."]},{"cell_type":"markdown","metadata":{"id":"KDkbII4ie6ny"},"source":["NOTE: we run the code 15 times on different seeds (to calc the avg and std of runtime and results)."]},{"cell_type":"markdown","metadata":{"id":"Kxt0W9Aejymu"},"source":["NOTE: It is very important that the learn/test split stays the same for all models, otherwise the results are not comparable!\n","But we can change up the learn-train/learn-val split for each model to get a better estimate of the generalization error."]},{"cell_type":"markdown","metadata":{"id":"qeE3SWg8ilUn"},"source":["NOTE: as described in the data preperation section, we are not creating here every train and validation split dataset at once but instead create those when fitting the model. So that we are not polluting the RAM. Note that notebooks have usually no garbage collector, so we try to be carefull."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":244,"status":"ok","timestamp":1699787289754,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"WUudp72lguxP"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","# --------------------\n","# NOTE: in the 2021 Gorishniy et al paper the batch size is different for the different Datasets\n","# but is not hyperparameter tuned. Bigger datasets they used a batch size of 1024 and\n","# for smaller datasets a batch size of (256/512).\n","batch_size = 1024\n","learn_data = df_to_tensor(df_freq_prep_nn[bool_in_learn], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","test_data = df_to_tensor(df_freq_prep_nn[bool_in_test], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","# NOTE we use at first just a fraction of the data to test the code:\n","learn_train_dummy_data = df_to_tensor(df_freq_prep_nn[bool_in_learn_train_dummy], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size,\n","                                      dummy_data_for_build=True)\n","\n","for run_index in range(15):\n","    # Create the dataframes needed for training:\n","    learn_train_data = df_to_tensor(df_freq_prep_nn[train_val_split[f\"learn_train_{run_index}\"]],\n","                                    feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","    learn_val_data = df_to_tensor(df_freq_prep_nn[train_val_split[f\"learn_val_{run_index}\"]],\n","                                  feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------We are at Model: {str(run_index).zfill(2)}-----------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    # Define FT-Transformer Models:\n","    # ----------------------\n","    # NOTE: we use here tensorflow/keras model subclasses (not the functional or sequential api)\n","    # NOTE: we use here instead of the .fit function a costum training loop\n","\n","    # create the model:\n","    # ----------------------\n","    set_random_seeds(int(random_seeds[run_index]))\n","\n","    FT_transformer = EnhActuar.Feature_Tokenizer_Transformer(\n","            emb_dim = 32, # NOTE: In the default setting for the 2021 Gorishniy paper they used emb_dim = 192 (but the parameter size would here go trough the roof, so we use something smaller)\n","            nr_features = nr_col,\n","            cat_features = cat_col,\n","            cat_vocabulary = cat_vocabulary,\n","            count_transformer_blocks = 3,\n","            attention_n_heads = 8,\n","            attention_dropout = 0.2,\n","            ffn_d_hidden = None, # NOTE: change to None if ReGLU should be used -> None uses default value (4/3*emb_dim), they write that they used 2*emb_dim if not ReGLU.\n","            ffn_activation_ReGLU = True, # NOTE: set True if ReGLU should be used\n","            ffn_dropout = 0.1,\n","            prenormalization = True,\n","            output_dim = 1,\n","            last_activation = 'exponential',\n","            exposure_name = \"Exposure\",\n","            seed_nr = int(random_seeds[run_index])\n","    )\n","\n","    # See here regarding costum training loop: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n","\n","    # Instantiate an optimizer to train the model.\n","    # ----------------------\n","    # create an optimizer AdamW with learning rate 1e-4, weight decay 1e-5:\n","    optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5)\n","\n","    # Instantiate a loss function\n","    # ----------------------\n","    # we use our own loss function here\n","    # because it is not included in tensorflow in the same way (see section loss function for more details):\n","    loss_fn = Poisson_loss_for_tf_Wrapped()\n","\n","    # Prepare the metrics.\n","    # ----------------------\n","    # we use a costume metric here (because it is not included in tensorflow in the same way):\n","    train_acc_metric = Poisson_Metric_for_tf()\n","    val_acc_metric = Poisson_Metric_for_tf()\n","    test_acc_metric = Poisson_Metric_for_tf()\n","\n","    @tf.function\n","    def train_step(x, y):\n","        # Open a GradientTape to record the operations run during the forward pass, which enables auto-differentiation.\n","        with tf.GradientTape() as tape:\n","            # Run the forward pass of the layer. The operations that the layer applies to its inputs are going to be recorded on the GradientTape.\n","            y_pred = FT_transformer(x, training=True)[\"output\"]  # prediction for this minibatch\n","            # Compute the loss value for this minibatch.\n","            loss_value = loss_fn(y, y_pred)\n","        # Use the gradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss.\n","        grads = tape.gradient(loss_value, FT_transformer.trainable_weights)\n","        # Run one step of gradient descent by updating the value of the variables to minimize the loss.\n","        optimizer.apply_gradients(zip(grads, FT_transformer.trainable_weights))\n","        # Update training metric.\n","        train_acc_metric.update_state(y, y_pred)\n","        return loss_value\n","\n","    @tf.function\n","    def val_step(x, y):\n","        # Run the forward pass of the layer.\n","        # (note: training=False is needed because the layers have different behavior during training versus inference (e.g. Dropout))\n","        y_pred = FT_transformer(x, training=False)[\"output\"]\n","        # Update val metrics\n","        val_acc_metric.update_state(y, y_pred)\n","\n","    @tf.function\n","    def test_step(x, y):\n","        # Run the forward pass of the layer.\n","        # (note: training=False is needed because the layers have different behavior during training versus inference (e.g. Dropout))\n","        y_pred = FT_transformer(x, training=False)[\"output\"]\n","        # Update val metrics\n","        test_acc_metric.update_state(y, y_pred)\n","\n","    # model fitting:\n","    # ----------------------\n","    start_time = time.time()\n","    Val_Progress = helper.Easy_ProgressTracker(patience=15)\n","    epochs = 500\n","\n","    for epoch in range(epochs):\n","        # Iterate over the batches of the dataset.\n","        for step, (x_batch_train, y_batch_train) in enumerate(learn_train_data):\n","            loss_value = train_step(x_batch_train, y_batch_train)\n","            helper.costume_progress_bar(f\"Ensemble: {str(run_index).zfill(2)}/{14} / Epoch: {epoch} / Batch: {step} / Train-Loss (Batch): {round(float(loss_value),4)}\",step,len(learn_train_data), 30)\n","\n","        # Display metrics at the end of each epoch.\n","        print_train_loss = train_acc_metric.result()\n","        # Reset training metrics at the end of each epoch\n","        train_acc_metric.reset_states()\n","\n","        # Run a validation at the end of each epoch.\n","        for x_batch_val, y_batch_val in learn_val_data:\n","            val_step(x_batch_val, y_batch_val)\n","        print_val_loss = val_acc_metric.result()\n","        val_acc_metric.reset_states()\n","        for x_batch_test, y_batch_test in test_data:\n","            test_step(x_batch_test, y_batch_test)\n","        print_test_loss = test_acc_metric.result()\n","        test_acc_metric.reset_states()\n","\n","        Val_Progress(current_epoch=epoch, current_score = print_val_loss)\n","\n","        print(f\"\\nEnsemble: {str(run_index).zfill(2)}/{14} / Epoch: {epoch} / Train-Loss: %.4f / Val-Loss: %.4f / Test-Loss: %.4f / Time taken: %s / ---- Currently Best Val-Epoch: %d\" % (\n","            # str(run_index).zfill(2),\n","            float(print_train_loss),\n","            float(print_val_loss),\n","            float(print_test_loss),\n","            datetime.timedelta(seconds=int(time.time() - start_time)),\n","            Val_Progress.best_epoch\n","            ), end = \" \")\n","        if Val_Progress.progress == True:\n","            print(\"<------- Best VAL Epoch so far\")\n","        else:\n","            print(\"\\r\")\n","\n","\n","        # Callback: save best model / early stopping:\n","        # ----------------------\n","        earliest_epoch2save = 10\n","        if Val_Progress.progress and Val_Progress.current_epoch >= earliest_epoch2save:\n","            # FT_transformer.save(storage_path +'/Poisson_FT_transformer')\n","            FT_transformer.save_weights(f'{storage_path}/saved_models/Poisson_FT_transformer_{run_index}.weights.h5')\n","        if Val_Progress.patience_over:\n","            break\n","\n","    # create some metrics after the loop\n","    best_epoch_FT_transformer = Val_Progress.best_epoch\n","    execution_time_FT_transformer = time.time() - start_time\n","\n","    # load the best saved model and epochs_and_time from the pickle file:\n","    # ----------------------\n","    # FT_transformer = keras.models.load_model(save_path +'/Poisson_FT_transformer')\n","    FT_transformer.load_weights(f'{storage_path}/saved_models/Poisson_FT_transformer_{run_index}.weights.h5')\n","\n","    # predict with the model:\n","    # ----------------------\n","    y_pred[\"train\"][f\"FT_transformer\"] = np.array([x for [x] in FT_transformer.predict(learn_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                )[\"output\"]])\n","    y_pred[\"test\"][f\"FT_transformer\"] = np.array([x for [x] in FT_transformer.predict(test_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                )[\"output\"]])\n","\n","    # evaluate the model:\n","    # ----------------------\n","    FT_transformer_results = Results(model=f\"FT_transformer (run: {run_index})\",\n","                                epochs=best_epoch_FT_transformer,\n","                                run_time=execution_time_FT_transformer,\n","                                nr_parameters=[np.sum([np.prod(v.get_shape().as_list()) for v in FT_transformer.trainable_weights])],\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"FT_transformer\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"FT_transformer\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"FT_transformer\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"FT_transformer\"].sum()/exposure[\"test\"].sum())\n","    # store the results in the dataframe:\n","    store_results_in_df(FT_transformer_results)\n","    display(df_results)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCfKIXHQMlLA"},"outputs":[],"source":["# # save the results:\n","# with open(f'{storage_path}/Data/df_results.pickle', 'wb') as handle:\n","#     pickle.dump(df_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# load the results:\n","with open(f'{storage_path}/Data/df_results.pickle', 'rb') as handle:\n","    df_results = pickle.load(handle)"]},{"cell_type":"markdown","metadata":{"id":"rZeHvQt1jymv"},"source":["## 4.2 CANN-FT-Transformer:  "]},{"cell_type":"markdown","metadata":{"id":"KyzKR0l8IAAG"},"source":["Note: we run the code 15 times on different seeds (to calc the avg and std of runtime and results)."]},{"cell_type":"markdown","metadata":{"id":"G_pGuaF_IN5p"},"source":["Note: the Code for the CANN below is basically the same code as for the FT-Transformer. The only changes are\n","* we use a other exposure column (Exposure_x_GLM3_pred instead of Exposure)\n","* we set the initial weights and bias of the last layer to zero"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uThlQxUQITGq"},"outputs":[],"source":["# create the new exposure times GLM3_pred column for CANN models.\n","df_freq_prep_nn[\"Exposure_x_GLM3_pred\"] = list(poisson_glm3.predict(X_glm3)*df_freq_prep_nn[\"Exposure\"])"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":232,"status":"ok","timestamp":1699787302575,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"k8XS-9fQIlWC"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","# --------------------\n","# NOTE: in the 2021 Gorishniy et al paper the batch size is different for the different Datasets\n","# but is not hyperparameter tuned. Bigger datasets they used a batch size of 1024 and\n","# for smaller datasets a batch size of (256/512).\n","batch_size = 1024\n","learn_data = df_to_tensor(df_freq_prep_nn[bool_in_learn], feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size)\n","test_data = df_to_tensor(df_freq_prep_nn[bool_in_test], feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","# NOTE we use at first just a fraction of the data to test the code:\n","learn_train_dummy_data = df_to_tensor(df_freq_prep_nn[bool_in_learn_train_dummy], feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size,\n","                                      dummy_data_for_build=True)\n","\n","for run_index in range(15):\n","    # Create the dataframes needed for training:\n","    learn_train_data = df_to_tensor(df_freq_prep_nn[train_val_split[f\"learn_train_{run_index}\"]],\n","                                    feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size)\n","    learn_val_data = df_to_tensor(df_freq_prep_nn[train_val_split[f\"learn_val_{run_index}\"]],\n","                                  feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------We are at Model: {str(run_index).zfill(2)}-----------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    # Define FT-Transformer Models:\n","    # ----------------------\n","    # NOTE: we use here tensorflow/keras model subclasses (not the functional or sequential api)\n","    # NOTE: we use here instead of the .fit function a costum training loop\n","\n","    # create the model:\n","    # ----------------------\n","    set_random_seeds(int(random_seeds[run_index]))\n","\n","    FT_transformer = EnhActuar.Feature_Tokenizer_Transformer(\n","            emb_dim = 32, # NOTE: In the default setting for the 2021 Gorishniy paper they used emb_dim = 192 (but the parameter size would here go trough the roof, so we use something smaller)\n","            nr_features = nr_col,\n","            cat_features = cat_col,\n","            cat_vocabulary = cat_vocabulary,\n","            count_transformer_blocks = 3,\n","            attention_n_heads = 8,\n","            attention_dropout = 0.2,\n","            ffn_d_hidden = None, # NOTE: change to None if ReGLU should be used -> None uses default value (4/3*emb_dim), they write that they used 2*emb_dim if not ReGLU.\n","            ffn_activation_ReGLU = True, # NOTE: set True if ReGLU should be used\n","            ffn_dropout = 0.1,\n","            prenormalization = True,\n","            output_dim = 1,\n","            last_activation = 'exponential',\n","            exposure_name = \"Exposure_x_GLM3_pred\",\n","            last_layer_initial_weights = \"zeros\",\n","            last_layer_initial_bias = \"zeros\",\n","            seed_nr = int(random_seeds[run_index])\n","    )\n","\n","    # See here regarding costum training loop: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n","\n","    # Instantiate an optimizer to train the model.\n","    # ----------------------\n","    # create an optimizer AdamW with learning rate 1e-4, weight decay 1e-5:\n","    optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5)\n","\n","    # Instantiate a loss function\n","    # ----------------------\n","    # we use our own loss function here\n","    # because it is not included in tensorflow in the same way (see section loss function for more details):\n","    loss_fn = Poisson_loss_for_tf_Wrapped()\n","\n","    # Prepare the metrics.\n","    # ----------------------\n","    # we use a costume metric here (because it is not included in tensorflow in the same way):\n","    train_acc_metric = Poisson_Metric_for_tf()\n","    val_acc_metric = Poisson_Metric_for_tf()\n","    test_acc_metric = Poisson_Metric_for_tf()\n","\n","    @tf.function\n","    def train_step(x, y):\n","        # Open a GradientTape to record the operations run during the forward pass, which enables auto-differentiation.\n","        with tf.GradientTape() as tape:\n","            # Run the forward pass of the layer. The operations that the layer applies to its inputs are going to be recorded on the GradientTape.\n","            y_pred = FT_transformer(x, training=True)[\"output\"]  # prediction for this minibatch\n","            # Compute the loss value for this minibatch.\n","            loss_value = loss_fn(y, y_pred)\n","        # Use the gradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss.\n","        grads = tape.gradient(loss_value, FT_transformer.trainable_weights)\n","        # Run one step of gradient descent by updating the value of the variables to minimize the loss.\n","        optimizer.apply_gradients(zip(grads, FT_transformer.trainable_weights))\n","        # Update training metric.\n","        train_acc_metric.update_state(y, y_pred)\n","        return loss_value\n","\n","    @tf.function\n","    def val_step(x, y):\n","        # Run the forward pass of the layer.\n","        # (note: training=False is needed because the layers have different behavior during training versus inference (e.g. Dropout))\n","        y_pred = FT_transformer(x, training=False)[\"output\"]\n","        # Update val metrics\n","        val_acc_metric.update_state(y, y_pred)\n","\n","    @tf.function\n","    def test_step(x, y):\n","        # Run the forward pass of the layer.\n","        # (note: training=False is needed because the layers have different behavior during training versus inference (e.g. Dropout))\n","        y_pred = FT_transformer(x, training=False)[\"output\"]\n","        # Update val metrics\n","        test_acc_metric.update_state(y, y_pred)\n","\n","    # model fitting:\n","    # ----------------------\n","    start_time = time.time()\n","    Val_Progress = helper.Easy_ProgressTracker(patience=15)\n","    epochs = 500\n","\n","    for epoch in range(epochs):\n","        # Iterate over the batches of the dataset.\n","        for step, (x_batch_train, y_batch_train) in enumerate(learn_train_data):\n","            loss_value = train_step(x_batch_train, y_batch_train)\n","            helper.costume_progress_bar(f\"Ensemble: {str(run_index).zfill(2)}/{14} / Epoch: {epoch} / Batch: {step} / Train-Loss (Batch): {round(float(loss_value),4)}\",step,len(learn_train_data), 30)\n","\n","        # Display metrics at the end of each epoch.\n","        print_train_loss = train_acc_metric.result()\n","        # Reset training metrics at the end of each epoch\n","        train_acc_metric.reset_states()\n","\n","        # Run a validation at the end of each epoch.\n","        for x_batch_val, y_batch_val in learn_val_data:\n","            val_step(x_batch_val, y_batch_val)\n","        print_val_loss = val_acc_metric.result()\n","        val_acc_metric.reset_states()\n","        for x_batch_test, y_batch_test in test_data:\n","            test_step(x_batch_test, y_batch_test)\n","        print_test_loss = test_acc_metric.result()\n","        test_acc_metric.reset_states()\n","\n","        Val_Progress(current_epoch=epoch, current_score = print_val_loss)\n","\n","        print(f\"\\nEnsemble: {str(run_index).zfill(2)}/{14} / Epoch: {epoch} / Train-Loss: %.4f / Val-Loss: %.4f / Test-Loss: %.4f / Time taken: %s / ---- Currently Best Val-Epoch: %d\" % (\n","            # str(run_index).zfill(2),\n","            float(print_train_loss),\n","            float(print_val_loss),\n","            float(print_test_loss),\n","            datetime.timedelta(seconds=int(time.time() - start_time)),\n","            Val_Progress.best_epoch\n","            ), end = \" \")\n","        if Val_Progress.progress == True:\n","            print(\"<------- Best VAL Epoch so far\")\n","        else:\n","            print(\"\\r\")\n","\n","\n","        # Callback: save best model / early stopping:\n","        # ----------------------\n","        earliest_epoch2save = 10\n","        if Val_Progress.progress and Val_Progress.current_epoch >= earliest_epoch2save:\n","            FT_transformer.save_weights(f'{storage_path}/saved_models/Poisson_CAFTT_{run_index}.weights.h5')\n","        if Val_Progress.patience_over:\n","            break\n","\n","    # create some metrics after the loop\n","    best_epoch_FT_transformer = Val_Progress.best_epoch\n","    execution_time_FT_transformer = time.time() - start_time\n","\n","    # load the best saved model and epochs_and_time from the pickle file:\n","    # ----------------------\n","    FT_transformer.load_weights(f'{storage_path}/saved_models/Poisson_CAFTT_{run_index}.weights.h5')\n","\n","    # predict with the model:\n","    # ----------------------\n","    y_pred[\"train\"][f\"CAFTT\"] = np.array([x for [x] in FT_transformer.predict(learn_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                )[\"output\"]])\n","    y_pred[\"test\"][f\"CAFTT\"] = np.array([x for [x] in FT_transformer.predict(test_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                )[\"output\"]])\n","\n","    # evaluate the model:\n","    # ----------------------\n","    CAFTT_results = Results(model=f\"CAFTT (run: {run_index})\",\n","                                epochs=best_epoch_FT_transformer,\n","                                run_time=execution_time_FT_transformer,\n","                                nr_parameters=[np.sum([np.prod(v.get_shape().as_list()) for v in FT_transformer.trainable_weights])],\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"CAFTT\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"CAFTT\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"CAFTT\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"CAFTT\"].sum()/exposure[\"test\"].sum())\n","    # store the results in the dataframe:\n","    store_results_in_df(CAFTT_results)\n","    display(df_results)\n","    # save the results:\n","    with open(f'{storage_path}/Data/df_results.pickle', 'wb') as handle:\n","        pickle.dump(df_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrvIt_sNKw2z"},"outputs":[],"source":["# # save the results:\n","# with open(f'{storage_path}/Data/df_results.pickle', 'wb') as handle:\n","#     pickle.dump(df_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# load the results:\n","with open(f'{storage_path}/Data/df_results.pickle', 'rb') as handle:\n","    df_results = pickle.load(handle)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3IPmjCg5jymw"},"source":["## 4.3 LocalGLM-FT-Transformer:  "]},{"cell_type":"markdown","metadata":{"id":"-zCJQSTNitmf"},"source":["Note: we run the code 15 times on different seeds (to calc the avg and std of runtime and results)."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":232,"status":"ok","timestamp":1699787315092,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"NWgC23RMjPC9"},"outputs":[],"source":["# Create the dataframes for creation of the glm-ohe-start model:\n","# --------------------\n","data_nn_ohe_learn, y_true_learn = create_ffn_ohe_data(bool_in_learn)\n","\n","# create dummy glm for initial weights\n","# ----------------------\n","poisson_glm_dummy = PoissonRegressor(alpha = 0,max_iter=1000) # scikit-learn.org: alpha = 0 is equivalent to unpenalized GLMs\n","poisson_glm_dummy.fit(data_nn_ohe_learn[0],y_true_learn/data_nn_ohe_learn[1],sample_weight=data_nn_ohe_learn[1]) # note: data_nn_ohe_learn = [X_ohe,exposure]\n","# get the betas from the glm:\n","glm_nr_col_betas = poisson_glm_dummy.coef_[:len(nr_col)]\n","current_beta_index = len(nr_col)\n","glm_cat_col_betas = {}\n","for c in cat_vocabulary.keys():\n","    glm_cat_col_betas[c] = poisson_glm_dummy.coef_[current_beta_index:current_beta_index+len(cat_vocabulary[c])]\n","    current_beta_index += len(cat_vocabulary[c])\n","glm_intercept = poisson_glm_dummy.intercept_\n","\n","\n","# Create the dataframes needed for evaluation:\n","# --------------------\n","# NOTE: in the 2021 Gorishniy paper the batch size is different for the different Datasets\n","# but is not hyperparameter tuned. Bigger datasets they used a batch size of 1024 and\n","# for smaller datasets a batch size of (256/512).\n","batch_size = 1024\n","learn_data = df_to_tensor(df_freq_prep_nn[bool_in_learn], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","test_data = df_to_tensor(df_freq_prep_nn[bool_in_test], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","for run_index in range(15):\n","\n","    # Create the dataframes needed for training:\n","    learn_train_data = df_to_tensor(df_freq_prep_nn[train_val_split[f\"learn_train_{run_index}\"]],\n","                                    feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","    learn_val_data = df_to_tensor(df_freq_prep_nn[train_val_split[f\"learn_val_{run_index}\"]],\n","                                  feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------We are at Model: {str(run_index).zfill(2)}-----------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    print(f\"-------------------------------------------------\")\n","    # Define FT-Transformer Models:\n","    # ----------------------\n","    # NOTE: we use here tensorflow/keras model subclasses (not the functional or sequential api)\n","    # NOTE: we use here instead of the .fit function a costum training loop\n","\n","    # create the model:\n","    # ----------------------\n","    set_random_seeds(int(random_seeds[run_index]))\n","    LocalGLMftt = EnhActuar.LocalGLM_FT_Transformer(\n","            emb_dim = 32, # NOTE: In the default setting for the 2021 Gorishniy paper they used emb_dim = 192 (but the parameter size would here go trough the roof, so we use something smaller)\n","            nr_features = nr_col,\n","            cat_features = cat_col,\n","            cat_vocabulary = cat_vocabulary,\n","            count_transformer_blocks = 3,\n","            attention_n_heads = 8,\n","            attention_dropout = 0.2,\n","            ffn_d_hidden = None, # NOTE: change to None if ReGLU should be used -> None uses default value (4/3*emb_dim), they write that they used 2*emb_dim if not ReGLU.\n","            ffn_activation_ReGLU = True, # NOTE: set True if ReGLU should be used\n","            ffn_dropout = 0.1,\n","            prenormalization = True,\n","            output_dim = 1,\n","            last_activation = 'exponential',\n","            exposure_name = \"Exposure\",\n","            last_layer_initial_weights = \"zeros\",\n","            last_layer_initial_bias = \"ones\",\n","            init_glm_cat_col_weights = glm_cat_col_betas,\n","            init_glm_nr_col_weights = glm_nr_col_betas,\n","            init_glm_bias = glm_intercept,\n","            trainable_glm_emb = False,\n","            seed_nr = int(random_seeds[run_index])\n","    )\n","\n","    # See here regarding costum training loop: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n","\n","    # Instantiate an optimizer to train the model.\n","    # ----------------------\n","    # create an optimizer AdamW with learning rate 1e-4, weight decay 1e-5:\n","    optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5)\n","\n","    # Instantiate a loss function\n","    # ----------------------\n","    # we use our own loss function here\n","    # because it is not included in tensorflow in the same way (see section loss function for more details):\n","    loss_fn = Poisson_loss_for_tf_Wrapped()\n","\n","    # Prepare the metrics.\n","    # ----------------------\n","    # we use a costume metric here (because it is not included in tensorflow in the same way):\n","    train_acc_metric = Poisson_Metric_for_tf()\n","    val_acc_metric = Poisson_Metric_for_tf()\n","    test_acc_metric = Poisson_Metric_for_tf()\n","\n","    @tf.function\n","    def train_step(x, y):\n","        # Open a GradientTape to record the operations run during the forward pass, which enables auto-differentiation.\n","        with tf.GradientTape() as tape:\n","            # Run the forward pass of the layer. The operations that the layer applies to its inputs are going to be recorded on the GradientTape.\n","            y_pred = LocalGLMftt(x, training=True)[\"output\"]  # prediction for this minibatch\n","            # Compute the loss value for this minibatch.\n","            loss_value = loss_fn(y, y_pred)\n","        # Use the gradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss.\n","        grads = tape.gradient(loss_value, LocalGLMftt.trainable_weights)\n","        # Run one step of gradient descent by updating the value of the variables to minimize the loss.\n","        optimizer.apply_gradients(zip(grads, LocalGLMftt.trainable_weights))\n","        # Update training metric.\n","        train_acc_metric.update_state(y, y_pred)\n","        return loss_value\n","\n","    @tf.function\n","    def val_step(x, y):\n","        # Run the forward pass of the layer.\n","        # (note: training=False is needed because the layers have different behavior during training versus inference (e.g. Dropout))\n","        y_pred = LocalGLMftt(x, training=False)[\"output\"]\n","        # Update val metrics\n","        val_acc_metric.update_state(y, y_pred)\n","\n","    @tf.function\n","    def test_step(x, y):\n","        # Run the forward pass of the layer.\n","        # (note: training=False is needed because the layers have different behavior during training versus inference (e.g. Dropout))\n","        y_pred = LocalGLMftt(x, training=False)[\"output\"]\n","        # Update val metrics\n","        test_acc_metric.update_state(y, y_pred)\n","\n","    # model fitting:\n","    # ----------------------\n","    start_time = time.time()\n","    Val_Progress = helper.Easy_ProgressTracker(patience=15)\n","    epochs = 500\n","\n","    for epoch in range(epochs):\n","        # Iterate over the batches of the dataset.\n","        for step, (x_batch_train, y_batch_train) in enumerate(learn_train_data):\n","            loss_value = train_step(x_batch_train, y_batch_train)\n","            helper.costume_progress_bar(f\"Ensemble: {str(run_index).zfill(2)}/{14} / Epoch: {epoch} / Batch: {step} / Train-Loss (Batch): {round(float(loss_value),4)}\",step,len(learn_train_data), 30)\n","\n","        # Display metrics at the end of each epoch.\n","        print_train_loss = train_acc_metric.result()\n","        # Reset training metrics at the end of each epoch\n","        train_acc_metric.reset_states()\n","\n","        # Run a validation at the end of each epoch.\n","        for x_batch_val, y_batch_val in learn_val_data:\n","            val_step(x_batch_val, y_batch_val)\n","        print_val_loss = val_acc_metric.result()\n","        val_acc_metric.reset_states()\n","        for x_batch_test, y_batch_test in test_data:\n","            test_step(x_batch_test, y_batch_test)\n","        print_test_loss = test_acc_metric.result()\n","        test_acc_metric.reset_states()\n","\n","        Val_Progress(current_epoch=epoch, current_score = print_val_loss)\n","\n","        print(f\"\\nEnsemble: {str(run_index).zfill(2)}/{14} / Epoch: {epoch} / Train-Loss: %.4f / Val-Loss: %.4f / Test-Loss: %.4f / Time taken: %s / ---- Currently Best Val-Epoch: %d\" % (\n","            # str(run_index).zfill(2),\n","            float(print_train_loss),\n","            float(print_val_loss),\n","            float(print_test_loss),\n","            datetime.timedelta(seconds=int(time.time() - start_time)),\n","            Val_Progress.best_epoch\n","            ), end = \" \")\n","        if Val_Progress.progress == True:\n","            print(\"<------- Best VAL Epoch so far\")\n","        else:\n","            print(\"\\r\")\n","\n","\n","        # Callback: save best model / early stopping:\n","        # ----------------------\n","        earliest_epoch2save = 10\n","        if Val_Progress.progress and Val_Progress.current_epoch >= earliest_epoch2save:\n","            LocalGLMftt.save_weights(f'{storage_path}/saved_models/Poisson_LocalGLMftt_{run_index}.weights.h5')\n","        if Val_Progress.patience_over:\n","            break\n","\n","    # create some metrics after the loop\n","    best_epoch_LocalGLMftt = Val_Progress.best_epoch\n","    execution_time_LocalGLMftt = time.time() - start_time\n","\n","    # load the best saved model and epochs_and_time from the pickle file:\n","    # ----------------------\n","    LocalGLMftt.load_weights(f'{storage_path}/saved_models/Poisson_LocalGLMftt_{run_index}.weights.h5')\n","\n","    # predict with the model:\n","    # ----------------------\n","    y_pred[\"train\"][f\"LocalGLMftt\"] = np.array([x for [x] in LocalGLMftt.predict(learn_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                )[\"output\"]])\n","    y_pred[\"test\"][f\"LocalGLMftt\"] = np.array([x for [x] in LocalGLMftt.predict(test_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                )[\"output\"]])\n","\n","    # evaluate the model:\n","    # ----------------------\n","    LocalGLMftt_results = Results(model=f\"LocalGLMftt (run: {run_index})\",\n","                                epochs=best_epoch_LocalGLMftt,\n","                                run_time=execution_time_LocalGLMftt,\n","                                nr_parameters=[np.sum([np.prod(v.get_shape().as_list()) for v in LocalGLMftt.trainable_weights])],\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"LocalGLMftt\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"LocalGLMftt\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"LocalGLMftt\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"LocalGLMftt\"].sum()/exposure[\"test\"].sum())\n","    # store the results in the dataframe:\n","    store_results_in_df(LocalGLMftt_results)\n","    display(df_results)\n","    # save the results:\n","    with open(f'{storage_path}/Data/df_results.pickle', 'wb') as handle:\n","        pickle.dump(df_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dX9XtiNNcoVu"},"outputs":[],"source":["# save the results:\n","# with open(f'{storage_path}/Data/df_results.pickle', 'wb') as handle:\n","#     pickle.dump(df_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# load the results:\n","with open(f'{storage_path}/Data/df_results.pickle', 'rb') as handle:\n","    df_results = pickle.load(handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":808},"executionInfo":{"elapsed":421,"status":"ok","timestamp":1699573296821,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"QdAozwhbdCy3","outputId":"5f8fb7e0-9e57-4a96-8b9f-91cf9177f0bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results Average:\n"]},{"data":{"text/html":["\n","  <div id=\"df-6c05297b-4dc7-483c-a4c5-98ba3f2d9451\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>homogeneous model</td>\n","      <td>0.000000</td>\n","      <td>0.054847</td>\n","      <td>1.0</td>\n","      <td>0.252132</td>\n","      <td>0.254454</td>\n","      <td>0.073631</td>\n","      <td>0.073631</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GLM1</td>\n","      <td>0.000000</td>\n","      <td>2.220158</td>\n","      <td>49.0</td>\n","      <td>0.241015</td>\n","      <td>0.241463</td>\n","      <td>0.073631</td>\n","      <td>0.073900</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GLM2</td>\n","      <td>0.000000</td>\n","      <td>2.752693</td>\n","      <td>48.0</td>\n","      <td>0.240911</td>\n","      <td>0.241125</td>\n","      <td>0.073631</td>\n","      <td>0.073981</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GLM3</td>\n","      <td>0.000000</td>\n","      <td>1.900497</td>\n","      <td>50.0</td>\n","      <td>0.240844</td>\n","      <td>0.241022</td>\n","      <td>0.073631</td>\n","      <td>0.074048</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>FFN_OHE</td>\n","      <td>42.200000</td>\n","      <td>37.805560</td>\n","      <td>1306.0</td>\n","      <td>0.237535</td>\n","      <td>0.238652</td>\n","      <td>0.073906</td>\n","      <td>0.074310</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>FNN_CAT_EMB</td>\n","      <td>72.933333</td>\n","      <td>58.728892</td>\n","      <td>792.0</td>\n","      <td>0.237682</td>\n","      <td>0.238267</td>\n","      <td>0.073774</td>\n","      <td>0.074238</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>CANN</td>\n","      <td>90.333333</td>\n","      <td>68.559120</td>\n","      <td>792.0</td>\n","      <td>0.237420</td>\n","      <td>0.238102</td>\n","      <td>0.074019</td>\n","      <td>0.074438</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LocalGLMnet</td>\n","      <td>25.333333</td>\n","      <td>29.720892</td>\n","      <td>1737.0</td>\n","      <td>0.237095</td>\n","      <td>0.239211</td>\n","      <td>0.073825</td>\n","      <td>0.074267</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>FT_transformer</td>\n","      <td>78.866667</td>\n","      <td>1569.860410</td>\n","      <td>27133.0</td>\n","      <td>0.237803</td>\n","      <td>0.239389</td>\n","      <td>0.061140</td>\n","      <td>0.061290</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>CAFTT</td>\n","      <td>57.333333</td>\n","      <td>1170.160723</td>\n","      <td>27133.0</td>\n","      <td>0.237146</td>\n","      <td>0.238072</td>\n","      <td>0.065975</td>\n","      <td>0.066235</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LocalGLMftt</td>\n","      <td>53.200000</td>\n","      <td>1187.006637</td>\n","      <td>27430.0</td>\n","      <td>0.237214</td>\n","      <td>0.238801</td>\n","      <td>0.067904</td>\n","      <td>0.068316</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c05297b-4dc7-483c-a4c5-98ba3f2d9451')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6c05297b-4dc7-483c-a4c5-98ba3f2d9451 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6c05297b-4dc7-483c-a4c5-98ba3f2d9451');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2171fb13-db62-4c16-bb94-f67737e58a38\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2171fb13-db62-4c16-bb94-f67737e58a38')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2171fb13-db62-4c16-bb94-f67737e58a38 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                model     epochs     run_time  nr_parameters  loss_train  \\\n","0   homogeneous model   0.000000     0.054847            1.0    0.252132   \n","1                GLM1   0.000000     2.220158           49.0    0.241015   \n","2                GLM2   0.000000     2.752693           48.0    0.240911   \n","3                GLM3   0.000000     1.900497           50.0    0.240844   \n","4             FFN_OHE  42.200000    37.805560         1306.0    0.237535   \n","5         FNN_CAT_EMB  72.933333    58.728892          792.0    0.237682   \n","6                CANN  90.333333    68.559120          792.0    0.237420   \n","7         LocalGLMnet  25.333333    29.720892         1737.0    0.237095   \n","8      FT_transformer  78.866667  1569.860410        27133.0    0.237803   \n","9               CAFTT  57.333333  1170.160723        27133.0    0.237146   \n","10        LocalGLMftt  53.200000  1187.006637        27430.0    0.237214   \n","\n","    loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0    0.254454             0.073631            0.073631  \n","1    0.241463             0.073631            0.073900  \n","2    0.241125             0.073631            0.073981  \n","3    0.241022             0.073631            0.074048  \n","4    0.238652             0.073906            0.074310  \n","5    0.238267             0.073774            0.074238  \n","6    0.238102             0.074019            0.074438  \n","7    0.239211             0.073825            0.074267  \n","8    0.239389             0.061140            0.061290  \n","9    0.238072             0.065975            0.066235  \n","10   0.238801             0.067904            0.068316  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Results Standard-Deviation:\n"]},{"data":{"text/html":["\n","  <div id=\"df-50a26011-4308-4f2c-8fbe-b117e52ac8f8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>homogeneous model</td>\n","      <td>0.000000</td>\n","      <td>0.002512</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>5.745950e-17</td>\n","      <td>2.872975e-17</td>\n","      <td>2.872975e-17</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GLM1</td>\n","      <td>0.000000</td>\n","      <td>0.473819</td>\n","      <td>0.0</td>\n","      <td>5.745950e-17</td>\n","      <td>5.745950e-17</td>\n","      <td>1.436488e-17</td>\n","      <td>0.000000e+00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GLM2</td>\n","      <td>0.000000</td>\n","      <td>0.970156</td>\n","      <td>0.0</td>\n","      <td>5.745950e-17</td>\n","      <td>2.872975e-17</td>\n","      <td>0.000000e+00</td>\n","      <td>1.436488e-17</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GLM3</td>\n","      <td>0.000000</td>\n","      <td>0.408252</td>\n","      <td>0.0</td>\n","      <td>2.872975e-17</td>\n","      <td>2.872975e-17</td>\n","      <td>0.000000e+00</td>\n","      <td>1.436488e-17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>FFN_OHE</td>\n","      <td>14.663853</td>\n","      <td>8.624492</td>\n","      <td>0.0</td>\n","      <td>3.255191e-04</td>\n","      <td>1.570462e-04</td>\n","      <td>1.223993e-03</td>\n","      <td>1.209107e-03</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>FNN_CAT_EMB</td>\n","      <td>21.661245</td>\n","      <td>13.907265</td>\n","      <td>0.0</td>\n","      <td>1.590947e-04</td>\n","      <td>1.514444e-04</td>\n","      <td>1.071399e-03</td>\n","      <td>1.088943e-03</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>CANN</td>\n","      <td>53.898935</td>\n","      <td>33.162059</td>\n","      <td>0.0</td>\n","      <td>6.076588e-04</td>\n","      <td>3.253586e-04</td>\n","      <td>1.111365e-03</td>\n","      <td>1.103431e-03</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LocalGLMnet</td>\n","      <td>7.622023</td>\n","      <td>5.097405</td>\n","      <td>0.0</td>\n","      <td>3.340630e-04</td>\n","      <td>2.176521e-04</td>\n","      <td>8.787938e-04</td>\n","      <td>9.078453e-04</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>FT_transformer</td>\n","      <td>16.638452</td>\n","      <td>302.316624</td>\n","      <td>0.0</td>\n","      <td>8.982910e-04</td>\n","      <td>5.281384e-04</td>\n","      <td>1.459836e-03</td>\n","      <td>1.458034e-03</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>CAFTT</td>\n","      <td>14.185841</td>\n","      <td>220.449926</td>\n","      <td>0.0</td>\n","      <td>4.739992e-04</td>\n","      <td>1.743234e-04</td>\n","      <td>4.993066e-04</td>\n","      <td>4.707813e-04</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LocalGLMftt</td>\n","      <td>16.384662</td>\n","      <td>310.648783</td>\n","      <td>0.0</td>\n","      <td>5.933743e-04</td>\n","      <td>1.596498e-04</td>\n","      <td>9.643511e-04</td>\n","      <td>9.918441e-04</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50a26011-4308-4f2c-8fbe-b117e52ac8f8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-50a26011-4308-4f2c-8fbe-b117e52ac8f8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-50a26011-4308-4f2c-8fbe-b117e52ac8f8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d9e9cd55-5577-46b6-9ab7-916d45c2253d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d9e9cd55-5577-46b6-9ab7-916d45c2253d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d9e9cd55-5577-46b6-9ab7-916d45c2253d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                model     epochs    run_time  nr_parameters    loss_train  \\\n","0   homogeneous model   0.000000    0.002512            0.0  0.000000e+00   \n","1                GLM1   0.000000    0.473819            0.0  5.745950e-17   \n","2                GLM2   0.000000    0.970156            0.0  5.745950e-17   \n","3                GLM3   0.000000    0.408252            0.0  2.872975e-17   \n","4             FFN_OHE  14.663853    8.624492            0.0  3.255191e-04   \n","5         FNN_CAT_EMB  21.661245   13.907265            0.0  1.590947e-04   \n","6                CANN  53.898935   33.162059            0.0  6.076588e-04   \n","7         LocalGLMnet   7.622023    5.097405            0.0  3.340630e-04   \n","8      FT_transformer  16.638452  302.316624            0.0  8.982910e-04   \n","9               CAFTT  14.185841  220.449926            0.0  4.739992e-04   \n","10        LocalGLMftt  16.384662  310.648783            0.0  5.933743e-04   \n","\n","       loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0   5.745950e-17         2.872975e-17        2.872975e-17  \n","1   5.745950e-17         1.436488e-17        0.000000e+00  \n","2   2.872975e-17         0.000000e+00        1.436488e-17  \n","3   2.872975e-17         0.000000e+00        1.436488e-17  \n","4   1.570462e-04         1.223993e-03        1.209107e-03  \n","5   1.514444e-04         1.071399e-03        1.088943e-03  \n","6   3.253586e-04         1.111365e-03        1.103431e-03  \n","7   2.176521e-04         8.787938e-04        9.078453e-04  \n","8   5.281384e-04         1.459836e-03        1.458034e-03  \n","9   1.743234e-04         4.993066e-04        4.707813e-04  \n","10  1.596498e-04         9.643511e-04        9.918441e-04  "]},"metadata":{},"output_type":"display_data"}],"source":["print(\"Results Average:\")\n","display(calc_avg_df([\"homogeneous model\",\"GLM1\",\"GLM2\",\"GLM3\",\"FFN_OHE\",\"FNN_CAT_EMB\",\"CANN\",\"LocalGLMnet\",\"FT_transformer\",\"CAFTT\",\"LocalGLMftt\"]))\n","print(\"Results Standard-Deviation:\")\n","display(calc_std_df([\"homogeneous model\",\"GLM1\",\"GLM2\",\"GLM3\",\"FFN_OHE\",\"FNN_CAT_EMB\",\"CANN\",\"LocalGLMnet\",\"FT_transformer\",\"CAFTT\",\"LocalGLMftt\"]))"]},{"cell_type":"markdown","metadata":{"id":"4p-8uxNSo5_G"},"source":["# 5. Ensemble Models"]},{"cell_type":"markdown","metadata":{"id":"SNfQncwWET2i"},"source":["## 5.1 Ensembles FFN OHE:"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1699787351487,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"6vf9FbjVEaGh"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_ohe_learn, y_true_learn = create_ffn_ohe_data(bool_in_learn)\n","data_nn_ohe_test, y_true_test = create_ffn_ohe_data(bool_in_test)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FNN Model:\n","        # ----------------------\n","        # note we use here the function api instead of the model subclassing\n","        # to make the code more readable and easier to understand:\n","        # (for the transformer based models we will use model subclasses)\n","        def Create_Poisson_FFN_OHE(input_dim=42,mean_model_results=1):\n","            # set random seeds\n","            set_random_seeds(int(random_seeds[run_index]))\n","            # Build the network\n","            Input_Matrix_OHE = tf.keras.layers.Input(shape=(input_dim,), dtype='float32', name='Input_Matrix')\n","            Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","            hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(Input_Matrix_OHE)\n","            hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","            hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","            Result_FFN1 = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_FFN1',\n","                            weights=[np.zeros((10, 1)), np.array([np.log(mean_model_results)])],\n","                            trainable=True)(hidden3)\n","            Response = tf.keras.layers.Multiply(name='Result')([Result_FFN1, Input_Exposure])\n","            # Define and Return the model\n","            return tf.keras.models.Model(inputs=[Input_Matrix_OHE, Input_Exposure], outputs=[Response], name='Poisson_FFN_OHE')\n","\n","        # create the model:\n","        # ----------------------\n","        FFN_OHE = Create_Poisson_FFN_OHE(input_dim=40,mean_model_results=constant_model)\n","\n","        # load the saved model weights:\n","        # ----------------------\n","        FFN_OHE.load_weights(f'{storage_path}/saved_models/Poisson_FFN_OHE_{run_index}.weights.h5')\n","\n","\n","        # predict with the models:\n","        # ----------------------\n","        y_pred[\"train\"][f\"FFN_OHE_{run_index}\"] = np.array([x for [x] in FFN_OHE.predict(data_nn_ohe_learn, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","        y_pred[\"test\"][f\"FFN_OHE_{run_index}\"] = np.array([x for [x] in FFN_OHE.predict(data_nn_ohe_test, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","\n","\n","    # evaluate the models:\n","    # ----------------------\n","    # store the results in the results class:\n","    y_pred[\"train\"][f\"Ensemble_FFN_OHE_{index}\"] = np.mean([y_pred[\"train\"][f\"FFN_OHE_{i}\"] for i in ensemble_range], axis=0)\n","    y_pred[\"test\"][f\"Ensemble_FFN_OHE_{index}\"] = np.mean([y_pred[\"test\"][f\"FFN_OHE_{i}\"] for i in ensemble_range], axis=0)\n","\n","    Ensemble_FFN_OHE_results = Results(model=f\"Ensemble_FFN_OHE (run: {index})\",\n","                                epochs=0,\n","                                run_time=0,\n","                                nr_parameters=0,\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"Ensemble_FFN_OHE_{index}\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"Ensemble_FFN_OHE_{index}\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"Ensemble_FFN_OHE_{index}\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"Ensemble_FFN_OHE_{index}\"].sum()/exposure[\"test\"].sum())\n","\n","    # # store the results in the result-dataframe:\n","    store_results_in_df(Ensemble_FFN_OHE_results)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_ohe_learn, data_nn_ohe_test"]},{"cell_type":"markdown","metadata":{"id":"pzrEtDM4ETq4"},"source":["## 5.2 Ensembles FFN CAT EMB:"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1699787359343,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"q2GFYkLgEeoG"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_emb_learn, y_true_learn = create_ffn_cat_emb_data(bool_in_learn)\n","data_nn_emb_test, y_true_test = create_ffn_cat_emb_data(bool_in_test)\n","\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        # Define FNN with Cat. Embedding Model:\n","        # ----------------------\n","        # note we use here the function api instead of the model subclassing\n","        # to make the code more readable and easier to understand:\n","        # (for the transformer based models we will use model subclasses)\n","        print(f\"Model: {run_index}\")\n","        def Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=1,mean_model_results=1):\n","            # set random seeds\n","            set_random_seeds(int(random_seeds[run_index]))\n","\n","            Input_Matrix_Num = tf.keras.layers.Input(shape=(input_nr_dim,), dtype='float32', name='Input_Matrix_Num')\n","            Input_VehBrand = tf.keras.layers.Input(shape=(1,), name='Input_VehBrand')\n","            Input_Region = tf.keras.layers.Input(shape=(1,), name='Input_Region')\n","            Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","\n","            All_Inputs = [Input_Matrix_Num,Input_VehBrand,Input_Region,Input_Exposure]\n","\n","            Emb_VehBrand = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"VehBrand\"].keys()),output_dim=emb_dim,\n","                                    embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05 ),\n","                                    name=\"Embedding_VehBrand\")(Input_VehBrand)\n","            Emb_Region = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"Region\"].keys()),output_dim=emb_dim,\n","                                    embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05),\n","                                name=\"Embedding_Region\")(Input_Region)\n","\n","            Reshaped_Emb_VehBrand = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_VehBrand\")(Emb_VehBrand)\n","            Reshaped_Emb_Region = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_Region\")(Emb_Region)\n","\n","            concatenation_layer = tf.keras.layers.Concatenate(name=\"concatenation_layer\")([Input_Matrix_Num,Reshaped_Emb_VehBrand,Reshaped_Emb_Region])\n","\n","            # Build the network\n","            hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(concatenation_layer)\n","            hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","            hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","            Result_FFN1 = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_FFN1',\n","                            weights=[np.zeros((10, 1)), np.array([np.log(mean_model_results)])],\n","                            trainable=True)(hidden3)\n","\n","            Response = tf.keras.layers.Multiply(name='Result')([Result_FFN1, Input_Exposure])\n","\n","            # Define the model\n","            return tf.keras.models.Model(inputs=All_Inputs, outputs=[Response], name='Poisson_CAT_EMB')\n","\n","        # create the model:\n","        # ----------------------\n","        emb_dim=2\n","        FNN_CAT_EMB = Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=emb_dim,mean_model_results=constant_model)\n","\n","        # load the saved model weights:\n","        # ----------------------\n","        FNN_CAT_EMB.load_weights(f'{storage_path}/saved_models/Poisson_FNN_CAT_EMB_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"FNN_CAT_EMB_{run_index}\"] = np.array([x for [x] in FNN_CAT_EMB.predict(data_nn_emb_learn, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","        y_pred[\"test\"][f\"FNN_CAT_EMB_{run_index}\"] = np.array([x for [x] in FNN_CAT_EMB.predict(data_nn_emb_test, verbose=0,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","\n","\n","    # evaluate the models:\n","    # ----------------------\n","    # store the results in the results class:\n","    y_pred[\"train\"][f\"Ensemble_FNN_CAT_EMB_{index}\"] = np.mean([y_pred[\"train\"][f\"FNN_CAT_EMB_{i}\"] for i in ensemble_range], axis=0)\n","    y_pred[\"test\"][f\"Ensemble_FNN_CAT_EMB_{index}\"] = np.mean([y_pred[\"test\"][f\"FNN_CAT_EMB_{i}\"] for i in ensemble_range], axis=0)\n","\n","    Ensemble_FNN_CAT_EMB_results = Results(model=f\"Ensemble_FNN_CAT_EMB (run: {index})\",\n","                                epochs=0,\n","                                run_time=0,\n","                                nr_parameters=0,\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"Ensemble_FNN_CAT_EMB_{index}\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"Ensemble_FNN_CAT_EMB_{index}\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"Ensemble_FNN_CAT_EMB_{index}\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"Ensemble_FNN_CAT_EMB_{index}\"].sum()/exposure[\"test\"].sum())\n","\n","    # # store the results in the result-dataframe:\n","    store_results_in_df(Ensemble_FNN_CAT_EMB_results)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_emb_learn, data_nn_emb_test"]},{"cell_type":"markdown","metadata":{"id":"hjb29bn-ETiP"},"source":["## 5.3 Ensembles CANN:"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":224,"status":"ok","timestamp":1699787366284,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"cCtAUB-6Ej9M"},"outputs":[],"source":["# create the new exposure times GLM3_pred column for CANN models.\n","df_freq_prep_nn[\"Exposure_x_GLM3_pred\"] = list(poisson_glm3.predict(X_glm3)*df_freq_prep_nn[\"Exposure\"])\n","\n","# Create the dataframes needed for evaluation:\n","data_nn_emb_learn, y_true_learn = create_ffn_cat_emb_data(bool_in_learn, exposure_name = \"Exposure_x_GLM3_pred\")\n","data_nn_emb_test, y_true_test = create_ffn_cat_emb_data(bool_in_test, exposure_name = \"Exposure_x_GLM3_pred\")\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        # Define FNN with Cat. Embedding Model:\n","        # ----------------------\n","        # note we use here the function api instead of the model subclassing\n","        # to make the code more readable and easier to understand:\n","        # (for the transformer based models we will use model subclasses)\n","        print(f\"Model: {run_index}\")\n","        def Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=1,mean_model_results=1):\n","            # set random seeds\n","            set_random_seeds(int(random_seeds[run_index]))\n","\n","            Input_Matrix_Num = tf.keras.layers.Input(shape=(input_nr_dim,), dtype='float32', name='Input_Matrix_Num')\n","            Input_VehBrand = tf.keras.layers.Input(shape=(1,), name='Input_VehBrand')\n","            Input_Region = tf.keras.layers.Input(shape=(1,), name='Input_Region')\n","            Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","\n","            All_Inputs = [Input_Matrix_Num,Input_VehBrand,Input_Region,Input_Exposure]\n","\n","            Emb_VehBrand = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"VehBrand\"].keys()),output_dim=emb_dim,\n","                                    embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05 ),\n","                                    name=\"Embedding_VehBrand\")(Input_VehBrand)\n","            Emb_Region = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"Region\"].keys()),output_dim=emb_dim,\n","                                    embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05),\n","                                name=\"Embedding_Region\")(Input_Region)\n","\n","            Reshaped_Emb_VehBrand = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_VehBrand\")(Emb_VehBrand)\n","            Reshaped_Emb_Region = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_Region\")(Emb_Region)\n","\n","            concatenation_layer = tf.keras.layers.Concatenate(name=\"concatenation_layer\")([Input_Matrix_Num,Reshaped_Emb_VehBrand,Reshaped_Emb_Region])\n","\n","            # Build the network\n","            hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(concatenation_layer)\n","            hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","            hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","            Result_FFN1 = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_FFN1',\n","                            weights=[np.zeros((10, 1)), np.array([0])],\n","                            trainable=True)(hidden3)\n","\n","            Response = tf.keras.layers.Multiply(name='Result')([Result_FFN1, Input_Exposure])\n","\n","            # Define the model\n","            return tf.keras.models.Model(inputs=All_Inputs, outputs=[Response], name='Poisson_CAT_EMB')\n","\n","        # create the model:\n","        # ----------------------\n","        emb_dim=2\n","        CANN = Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=emb_dim,mean_model_results=constant_model)\n","\n","        # load the saved model weights:\n","        # ----------------------\n","        CANN.load_weights(f'{storage_path}/saved_models/Poisson_CANN_{run_index}.weights.h5')\n","\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"CANN_{run_index}\"] = np.array([x for [x] in CANN.predict(data_nn_emb_learn, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","        y_pred[\"test\"][f\"CANN_{run_index}\"] = np.array([x for [x] in CANN.predict(data_nn_emb_test, verbose=0,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","\n","\n","    # evaluate the models:\n","    # ----------------------\n","    # store the results in the results class:\n","    y_pred[\"train\"][f\"Ensemble_CANN_{index}\"] = np.mean([y_pred[\"train\"][f\"CANN_{i}\"] for i in ensemble_range], axis=0)\n","    y_pred[\"test\"][f\"Ensemble_CANN_{index}\"] = np.mean([y_pred[\"test\"][f\"CANN_{i}\"] for i in ensemble_range], axis=0)\n","\n","    Ensemble_CANN_results = Results(model=f\"Ensemble_CANN (run: {index})\",\n","                                epochs=0,\n","                                run_time=0,\n","                                nr_parameters=0,\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"Ensemble_CANN_{index}\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"Ensemble_CANN_{index}\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"Ensemble_CANN_{index}\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"Ensemble_CANN_{index}\"].sum()/exposure[\"test\"].sum())\n","\n","    # # store the results in the result-dataframe:\n","    store_results_in_df(Ensemble_CANN_results)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_emb_learn, data_nn_emb_test\n"]},{"cell_type":"markdown","metadata":{"id":"q-dZzIY9ETVN"},"source":["## 5.4 Ensembles LocalGLMnet:"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1699787374223,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"zFXmRmpjEn4o"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_ohe_learn, y_true_learn = create_ffn_ohe_data(bool_in_learn)\n","data_nn_ohe_test, y_true_test = create_ffn_ohe_data(bool_in_test)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FNN with Cat. Embedding Model:\n","        # ----------------------\n","        # note we use here the function api instead of the model subclassing\n","        # to make the code more readable and easier to understand:\n","        # (for the transformer based models we will use model subclasses)\n","\n","        # create dummy glm for initial weights\n","        # ----------------------\n","        poisson_glm_dummy = PoissonRegressor(alpha = 0,max_iter=1000) # scikit-learn.org: alpha = 0 is equivalent to unpenalized GLMs\n","        poisson_glm_dummy.fit(data_nn_ohe_learn[0],y_true_learn/data_nn_ohe_learn[1],sample_weight=data_nn_ohe_learn[1]) # note: data_nn_ohe_learn = [X_ohe,exposure]\n","\n","        # Define LocalGLMnet:\n","        # ----------------------\n","        def Create_Poisson_LocalGLMnet(input_dim=40,initial_glm_bias=1, initial_glm_betas=None):\n","            # set random seeds\n","            set_random_seeds(int(random_seeds[run_index]))\n","            Input_Matrix_OHE = tf.keras.layers.Input(shape=(input_dim,), dtype='float32', name='Input_Matrix')\n","            Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","            # Build the network\n","            hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(Input_Matrix_OHE)\n","            hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","            hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","            Attention = tf.keras.layers.Dense(units=input_dim, activation='linear', name='attention',\n","                            weights=[np.zeros((10, input_dim)), initial_glm_betas])(hidden3)\n","            # note that the weights are set to 0 and the bias is set to the initial glm betas\n","            # create a layer that calculates the dot product between the attention weights (Attention) and the input matrix Input_Matrix_OHE:\n","            # (Attention has the same dimension as the input matrix Input_Matrix_OHE):\n","            weighted_input = tf.keras.layers.Multiply(name='feature_contributions')([Attention, Input_Matrix_OHE])\n","            scalar_product = tf.keras.layers.Dense(units=1, activation='linear', name='scalar_product',\n","                                weights=[np.ones((input_dim, 1)), np.array([0])],\n","                                trainable=False)(weighted_input)\n","            # Note that we actually don't want to make the following weights trainable,\n","            # but to get the bias to be trainable we need to do so. see comment in Book Wüthrich & Merz (2023) page 500\n","            Result_LocalGLMnet_without_Exposure = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_LocalGLMnet_without_Exposure',\n","                            weights=[np.ones((1, 1)), np.array([initial_glm_bias])],\n","                            trainable=True)(scalar_product)\n","            Response = tf.keras.layers.Multiply(name='Result')([Result_LocalGLMnet_without_Exposure, Input_Exposure])\n","            return tf.keras.models.Model(inputs=[Input_Matrix_OHE, Input_Exposure], outputs=[Response], name='Poisson_LocalGLMnet')\n","\n","        # create the model:\n","        # ----------------------\n","        LocalGLMnet = Create_Poisson_LocalGLMnet(input_dim=40,initial_glm_bias=poisson_glm_dummy.intercept_,initial_glm_betas=poisson_glm_dummy.coef_)\n","\n","        # load the saved model weights:\n","        # ----------------------\n","        LocalGLMnet.load_weights(f'{storage_path}/saved_models/Poisson_LocalGLMnet_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"LocalGLMnet_{run_index}\"] = np.array([x for [x] in LocalGLMnet.predict(data_nn_ohe_learn, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","        y_pred[\"test\"][f\"LocalGLMnet_{run_index}\"] = np.array([x for [x] in LocalGLMnet.predict(data_nn_ohe_test, verbose=0,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","\n","\n","    # evaluate the models:\n","    # ----------------------\n","    # store the results in the results class:\n","    y_pred[\"train\"][f\"Ensemble_LocalGLMnet_{index}\"] = np.mean([y_pred[\"train\"][f\"LocalGLMnet_{i}\"] for i in ensemble_range], axis=0)\n","    y_pred[\"test\"][f\"Ensemble_LocalGLMnet_{index}\"] = np.mean([y_pred[\"test\"][f\"LocalGLMnet_{i}\"] for i in ensemble_range], axis=0)\n","\n","    Ensemble_LocalGLMnet_results = Results(model=f\"Ensemble_LocalGLMnet (run: {index})\",\n","                                epochs=0,\n","                                run_time=0,\n","                                nr_parameters=0,\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"Ensemble_LocalGLMnet_{index}\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"Ensemble_LocalGLMnet_{index}\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"Ensemble_LocalGLMnet_{index}\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"Ensemble_LocalGLMnet_{index}\"].sum()/exposure[\"test\"].sum())\n","\n","    # # store the results in the result-dataframe:\n","    store_results_in_df(Ensemble_LocalGLMnet_results)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_ohe_learn, data_nn_ohe_test\n"]},{"cell_type":"markdown","metadata":{"id":"0JaUP3UbETNo"},"source":["## 5.5 Ensembles FT-Transformer:"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1699787381700,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"1_u5kCjzExox"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","# --------------------\n","# NOTE: in the 2021 Gorishniy paper the batch size is different for the different Datasets\n","# but is not hyperparameter tuned. Bigger datasets they used a batch size of 1024 and\n","# for smaller datasets a batch size of (256/512).\n","batch_size = 1024\n","learn_data = df_to_tensor(df_freq_prep_nn[bool_in_learn], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","test_data = df_to_tensor(df_freq_prep_nn[bool_in_test], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","# NOTE we use at first just a fraction of the data to test the code:\n","learn_train_dummy_data = df_to_tensor(df_freq_prep_nn[bool_in_learn_train_dummy], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size,\n","                                      dummy_data_for_build=True)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FT-Transformer Models:\n","        # ----------------------\n","        # NOTE: we use here tensorflow/keras model subclasses (not the functional or sequential api)\n","        # NOTE: we use here instead of the .fit function a costum training loop\n","\n","        # create the model:\n","        # ----------------------\n","        set_random_seeds(int(random_seeds[run_index]))\n","\n","        FT_transformer = EnhActuar.Feature_Tokenizer_Transformer(\n","                emb_dim = 32, # NOTE: In the default setting for the 2021 Gorishniy paper they used emb_dim = 192 (but the parameter size would here go trough the roof, so we use something smaller)\n","                nr_features = nr_col,\n","                cat_features = cat_col,\n","                cat_vocabulary = cat_vocabulary,\n","                count_transformer_blocks = 3,\n","                attention_n_heads = 8,\n","                attention_dropout = 0.2,\n","                ffn_d_hidden = None, # NOTE: change to None if ReGLU should be used -> None uses default value (4/3*emb_dim), they write that they used 2*emb_dim if not ReGLU.\n","                ffn_activation_ReGLU = True, # NOTE: set True if ReGLU should be used\n","                ffn_dropout = 0.1,\n","                prenormalization = True,\n","                output_dim = 1,\n","                last_activation = 'exponential',\n","                exposure_name = \"Exposure\",\n","                seed_nr = int(random_seeds[run_index])\n","        )\n","\n","        FT_transformer.predict(learn_train_dummy_data,verbose=0,batch_size=100000)\n","\n","        # load the best saved model and epochs_and_time from the pickle file:\n","        # ----------------------\n","        # FT_transformer = keras.models.load_model(save_path +'/Poisson_FT_transformer')\n","        FT_transformer.load_weights(f'{storage_path}/saved_models/Poisson_FT_transformer_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"FT_transformer_{run_index}\"] = np.array([x for [x] in FT_transformer.predict(learn_data,verbose=0,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","        y_pred[\"test\"][f\"FT_transformer_{run_index}\"] = np.array([x for [x] in FT_transformer.predict(test_data,verbose=0,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","\n","\n","    # evaluate the models:\n","    # ----------------------\n","    # store the results in the results class:\n","    y_pred[\"train\"][f\"Ensemble_FT_transformer_{index}\"] = np.mean([y_pred[\"train\"][f\"FT_transformer_{i}\"] for i in ensemble_range], axis=0)\n","    y_pred[\"test\"][f\"Ensemble_FT_transformer_{index}\"] = np.mean([y_pred[\"test\"][f\"FT_transformer_{i}\"] for i in ensemble_range], axis=0)\n","\n","    Ensemble_FT_transformer_results = Results(model=f\"Ensemble_FT_transformer (run: {index})\",\n","                                epochs=0,\n","                                run_time=0,\n","                                nr_parameters=0,\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"Ensemble_FT_transformer_{index}\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"Ensemble_FT_transformer_{index}\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"Ensemble_FT_transformer_{index}\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"Ensemble_FT_transformer_{index}\"].sum()/exposure[\"test\"].sum())\n","\n","    # # store the results in the result-dataframe:\n","    store_results_in_df(Ensemble_FT_transformer_results)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del learn_data, test_data"]},{"cell_type":"markdown","metadata":{"id":"5RLeTLEKETH_"},"source":["## 5.6 Ensembles CAFTT:"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":319,"status":"ok","timestamp":1699787388581,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"ZEJ5_BePE-b2"},"outputs":[],"source":["# create the new exposure times GLM3_pred column for CANN models.\n","df_freq_prep_nn[\"Exposure_x_GLM3_pred\"] = list(poisson_glm3.predict(X_glm3)*df_freq_prep_nn[\"Exposure\"])\n","\n","# Create the dataframes needed for evaluation:\n","# --------------------\n","# NOTE: in the 2021 Gorishniy et al paper the batch size is different for the different Datasets\n","# but is not hyperparameter tuned. Bigger datasets they used a batch size of 1024 and\n","# for smaller datasets a batch size of (256/512).\n","batch_size = 1024\n","learn_data = df_to_tensor(df_freq_prep_nn[bool_in_learn], feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size)\n","test_data = df_to_tensor(df_freq_prep_nn[bool_in_test], feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","# NOTE we use at first just a fraction of the data to test the code:\n","learn_train_dummy_data = df_to_tensor(df_freq_prep_nn[bool_in_learn_train_dummy], feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size,\n","                                      dummy_data_for_build=True)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FT-Transformer Models:\n","        # ----------------------\n","        # NOTE: we use here tensorflow/keras model subclasses (not the functional or sequential api)\n","        # NOTE: we use here instead of the .fit function a costum training loop\n","\n","        # create the model:\n","        # ----------------------\n","        set_random_seeds(int(random_seeds[run_index]))\n","\n","        FT_transformer = EnhActuar.Feature_Tokenizer_Transformer(\n","                emb_dim = 32, # NOTE: In the default setting for the 2021 Gorishniy paper they used emb_dim = 192 (but the parameter size would here go trough the roof, so we use something smaller)\n","                nr_features = nr_col,\n","                cat_features = cat_col,\n","                cat_vocabulary = cat_vocabulary,\n","                count_transformer_blocks = 3,\n","                attention_n_heads = 8,\n","                attention_dropout = 0.2,\n","                ffn_d_hidden = None, # NOTE: change to None if ReGLU should be used -> None uses default value (4/3*emb_dim), they write that they used 2*emb_dim if not ReGLU.\n","                ffn_activation_ReGLU = True, # NOTE: set True if ReGLU should be used\n","                ffn_dropout = 0.1,\n","                prenormalization = True,\n","                output_dim = 1,\n","                last_activation = 'exponential',\n","                exposure_name = \"Exposure_x_GLM3_pred\",\n","                last_layer_initial_weights = \"zeros\",\n","                last_layer_initial_bias = \"zeros\",\n","                seed_nr = int(random_seeds[run_index])\n","        )\n","\n","        FT_transformer.predict(learn_train_dummy_data,verbose=0,batch_size=100000)\n","\n","        # load the best saved model and epochs_and_time from the pickle file:\n","        # ----------------------\n","        FT_transformer.load_weights(f'{storage_path}/saved_models/Poisson_CAFTT_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"CAFTT_{run_index}\"] = np.array([x for [x] in FT_transformer.predict(learn_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","        y_pred[\"test\"][f\"CAFTT_{run_index}\"] = np.array([x for [x] in FT_transformer.predict(test_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","\n","    # evaluate the models:\n","    # ----------------------\n","    # store the results in the results class:\n","    y_pred[\"train\"][f\"Ensemble_CAFTT_{index}\"] = np.mean([y_pred[\"train\"][f\"CAFTT_{i}\"] for i in ensemble_range], axis=0)\n","    y_pred[\"test\"][f\"Ensemble_CAFTT_{index}\"] = np.mean([y_pred[\"test\"][f\"CAFTT_{i}\"] for i in ensemble_range], axis=0)\n","\n","    Ensemble_CAFTT_results = Results(model=f\"Ensemble_CAFTT (run: {index})\",\n","                                epochs=0,\n","                                run_time=0,\n","                                nr_parameters=0,\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"Ensemble_CAFTT_{index}\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"Ensemble_CAFTT_{index}\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"Ensemble_CAFTT_{index}\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"Ensemble_CAFTT_{index}\"].sum()/exposure[\"test\"].sum())\n","\n","    # # store the results in the result-dataframe:\n","    store_results_in_df(Ensemble_CAFTT_results)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del learn_data, test_data"]},{"cell_type":"markdown","metadata":{"id":"cLZvckKwEvXB"},"source":["## 5.7 Ensembles LocalGLMftt:"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1699787402246,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"EsCywe7LoxRC"},"outputs":[],"source":["# Create the dataframes for creation of the glm-ohe-start model:\n","# --------------------\n","data_nn_ohe_learn, y_true_learn = create_ffn_ohe_data(bool_in_learn)\n","\n","# create dummy glm for initial weights\n","# ----------------------\n","poisson_glm_dummy = PoissonRegressor(alpha = 0,max_iter=1000) # scikit-learn.org: alpha = 0 is equivalent to unpenalized GLMs\n","poisson_glm_dummy.fit(data_nn_ohe_learn[0],y_true_learn/data_nn_ohe_learn[1],sample_weight=data_nn_ohe_learn[1]) # note: data_nn_ohe_learn = [X_ohe,exposure]\n","# get the betas from the glm:\n","glm_nr_col_betas = poisson_glm_dummy.coef_[:len(nr_col)]\n","current_beta_index = len(nr_col)\n","glm_cat_col_betas = {}\n","for c in cat_vocabulary.keys():\n","    glm_cat_col_betas[c] = poisson_glm_dummy.coef_[current_beta_index:current_beta_index+len(cat_vocabulary[c])]\n","    current_beta_index += len(cat_vocabulary[c])\n","glm_intercept = poisson_glm_dummy.intercept_\n","\n","\n","# Create the dataframes needed for evaluation:\n","# --------------------\n","# NOTE: in the 2021 Gorishniy et al paper the batch size is different for the different Datasets\n","# but is not hyperparameter tuned. Bigger datasets they used a batch size of 1024 and\n","# for smaller datasets a batch size of (256/512).\n","batch_size = 1024\n","learn_data = df_to_tensor(df_freq_prep_nn[bool_in_learn], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","test_data = df_to_tensor(df_freq_prep_nn[bool_in_test], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","learn_train_dummy_data = df_to_tensor(df_freq_prep_nn[bool_in_learn_train_dummy], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size,\n","                                      dummy_data_for_build=True)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FT-Transformer Models:\n","        # ----------------------\n","        # NOTE: we use here tensorflow/keras model subclasses (not the functional or sequential api)\n","        # NOTE: we use here instead of the .fit function a costum training loop\n","\n","        # create the model:\n","        # ----------------------\n","        set_random_seeds(int(random_seeds[run_index]))\n","        LocalGLMftt = EnhActuar.LocalGLM_FT_Transformer(\n","                emb_dim = 32, # NOTE: In the default setting for the 2021 Gorishniy paper they used emb_dim = 192 (but the parameter size would here go trough the roof, so we use something smaller)\n","                nr_features = nr_col,\n","                cat_features = cat_col,\n","                cat_vocabulary = cat_vocabulary,\n","                count_transformer_blocks = 3,\n","                attention_n_heads = 8,\n","                attention_dropout = 0.2,\n","                ffn_d_hidden = None, # NOTE: change to None if ReGLU should be used -> None uses default value (4/3*emb_dim), they write that they used 2*emb_dim if not ReGLU.\n","                ffn_activation_ReGLU = True, # NOTE: set True if ReGLU should be used\n","                ffn_dropout = 0.1,\n","                prenormalization = True,\n","                output_dim = 1,\n","                last_activation = 'exponential',\n","                exposure_name = \"Exposure\",\n","                last_layer_initial_weights = \"zeros\",\n","                last_layer_initial_bias = \"ones\",\n","                init_glm_cat_col_weights = glm_cat_col_betas,\n","                init_glm_nr_col_weights = glm_nr_col_betas,\n","                init_glm_bias = glm_intercept,\n","                trainable_glm_emb = False,\n","                seed_nr = int(random_seeds[run_index])\n","        )\n","\n","        LocalGLMftt.predict(learn_train_dummy_data,verbose=0,batch_size=100000)\n","\n","        # load the best saved model and epochs_and_time from the pickle file:\n","        # ----------------------\n","        LocalGLMftt.load_weights(f'{storage_path}/saved_models/Poisson_LocalGLMftt_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"LocalGLMftt_{run_index}\"] = np.array([x for [x] in LocalGLMftt.predict(learn_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","        y_pred[\"test\"][f\"LocalGLMftt_{run_index}\"] = np.array([x for [x] in LocalGLMftt.predict(test_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","\n","\n","    # evaluate the models:\n","    # ----------------------\n","    # store the results in the results class:\n","    y_pred[\"train\"][f\"Ensemble_LocalGLMftt_{index}\"] = np.mean([y_pred[\"train\"][f\"LocalGLMftt_{i}\"] for i in ensemble_range], axis=0)\n","    y_pred[\"test\"][f\"Ensemble_LocalGLMftt_{index}\"] = np.mean([y_pred[\"test\"][f\"LocalGLMftt_{i}\"] for i in ensemble_range], axis=0)\n","\n","    Ensemble_LocalGLMftt_results = Results(model=f\"Ensemble_LocalGLMftt (run: {index})\",\n","                                epochs=0,\n","                                run_time=0,\n","                                nr_parameters=0,\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"Ensemble_LocalGLMftt_{index}\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"Ensemble_LocalGLMftt_{index}\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"Ensemble_LocalGLMftt_{index}\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"Ensemble_LocalGLMftt_{index}\"].sum()/exposure[\"test\"].sum())\n","\n","    # # store the results in the result-dataframe:\n","    store_results_in_df(Ensemble_LocalGLMftt_results)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del learn_data, test_data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WA78lFmDh5sA"},"outputs":[],"source":["# # save the results:\n","# with open(f'{storage_path}/Data/df_results.pickle', 'wb') as handle:\n","#     pickle.dump(df_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# load the results:\n","with open(f'{storage_path}/Data/df_results.pickle', 'rb') as handle:\n","    df_results = pickle.load(handle)\n"]},{"cell_type":"markdown","metadata":{"id":"RU2gxzJ4Xmg5"},"source":[]},{"cell_type":"markdown","metadata":{"id":"7-4s8jShX48M"},"source":["# 6. Rebase Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Ni9IbOoaxJc"},"outputs":[],"source":["def create_single_rebase_results(model_name,run_index):\n","    rebase_factor = np.mean(y_true[\"train\"])/np.mean(y_pred[\"train\"][f\"{model_name}_{run_index}\"])\n","\n","    y_pred[\"train\"][f\"Rebase_{model_name}_{run_index}\"] = y_pred[\"train\"][f\"{model_name}_{run_index}\"] * rebase_factor\n","    y_pred[\"test\"][f\"Rebase_{model_name}_{run_index}\"] = y_pred[\"test\"][f\"{model_name}_{run_index}\"] * rebase_factor\n","\n","    Rebase_results = Results(model=f\"Rebase_{model_name} (run: {run_index})\",\n","                                epochs=0,\n","                                run_time=0,\n","                                nr_parameters=0,\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"Rebase_{model_name}_{run_index}\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"Rebase_{model_name}_{run_index}\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"Rebase_{model_name}_{run_index}\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"Rebase_{model_name}_{run_index}\"].sum()/exposure[\"test\"].sum())\n","\n","    #  store the results in the result-dataframe:\n","    store_results_in_df(Rebase_results)\n","\n","def create_ensemble_results(model_name,index):\n","    # evaluate the models:\n","    # ----------------------\n","    # store the results in the results class:\n","    y_pred[\"train\"][f\"Ensemble_{model_name}_{index}\"] = np.mean([y_pred[\"train\"][f\"{model_name}_{i}\"] for i in ensemble_range], axis=0)\n","    y_pred[\"test\"][f\"Ensemble_{model_name}_{index}\"] = np.mean([y_pred[\"test\"][f\"{model_name}_{i}\"] for i in ensemble_range], axis=0)\n","\n","    Ensemble_results = Results(model=f\"Ensemble_{model_name} (run: {index})\",\n","                                epochs=0,\n","                                run_time=0,\n","                                nr_parameters=0,\n","                                poisson_deviance_loss_train=poisson_deviance_loss(y_true[\"train\"], y_pred[\"train\"][f\"Ensemble_{model_name}_{index}\"]),\n","                                poisson_deviance_loss_test=poisson_deviance_loss(y_true[\"test\"], y_pred[\"test\"][f\"Ensemble_{model_name}_{index}\"]),\n","                                pred_avg_freq_train=y_pred[\"train\"][f\"Ensemble_{model_name}_{index}\"].sum()/exposure[\"train\"].sum(),\n","                                pred_avg_freq_test=y_pred[\"test\"][f\"Ensemble_{model_name}_{index}\"].sum()/exposure[\"test\"].sum())\n","\n","    # # store the results in the result-dataframe:\n","    store_results_in_df(Ensemble_results)"]},{"cell_type":"markdown","metadata":{"id":"YPjrW250X48N"},"source":["## 6.1 Rebase FFN OHE:"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":220,"status":"ok","timestamp":1699787411533,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"03h2dyMeX48N"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_ohe_learn, y_true_learn = create_ffn_ohe_data(bool_in_learn)\n","data_nn_ohe_test, y_true_test = create_ffn_ohe_data(bool_in_test)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FNN Model:\n","        # ----------------------\n","        # note we use here the function api instead of the model subclassing\n","        # to make the code more readable and easier to understand:\n","        # (for the transformer based models we will use model subclasses)\n","        def Create_Poisson_FFN_OHE(input_dim=42,mean_model_results=1):\n","            # set random seeds\n","            set_random_seeds(int(random_seeds[run_index]))\n","            # Build the network\n","            Input_Matrix_OHE = tf.keras.layers.Input(shape=(input_dim,), dtype='float32', name='Input_Matrix')\n","            Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","            hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(Input_Matrix_OHE)\n","            hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","            hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","            Result_FFN1 = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_FFN1',\n","                            weights=[np.zeros((10, 1)), np.array([np.log(mean_model_results)])],\n","                            trainable=True)(hidden3)\n","            Response = tf.keras.layers.Multiply(name='Result')([Result_FFN1, Input_Exposure])\n","            # Define and Return the model\n","            return tf.keras.models.Model(inputs=[Input_Matrix_OHE, Input_Exposure], outputs=[Response], name='Poisson_FFN_OHE')\n","\n","        # create the model:\n","        # ----------------------\n","        FFN_OHE = Create_Poisson_FFN_OHE(input_dim=40,mean_model_results=constant_model)\n","\n","        # load the saved model weights:\n","        # ----------------------\n","        FFN_OHE.load_weights(f'{storage_path}/saved_models/Poisson_FFN_OHE_{run_index}.weights.h5')\n","\n","\n","        # predict with the models:\n","        # ----------------------\n","        y_pred[\"train\"][f\"FFN_OHE_{run_index}\"] = np.array([x for [x] in FFN_OHE.predict(data_nn_ohe_learn, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","        y_pred[\"test\"][f\"FFN_OHE_{run_index}\"] = np.array([x for [x] in FFN_OHE.predict(data_nn_ohe_test, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","\n","        create_single_rebase_results(\"FFN_OHE\",run_index)\n","    create_ensemble_results(\"Rebase_FFN_OHE\",index)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_ohe_learn, data_nn_ohe_test"]},{"cell_type":"markdown","metadata":{"id":"BT2qkptSX48N"},"source":["## 6.2 Rebase FFN CAT EMB:"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1699787423696,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"UEwuFTg7X48N"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_emb_learn, y_true_learn = create_ffn_cat_emb_data(bool_in_learn)\n","data_nn_emb_test, y_true_test = create_ffn_cat_emb_data(bool_in_test)\n","\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        # Define FNN with Cat. Embedding Model:\n","        # ----------------------\n","        # note we use here the function api instead of the model subclassing\n","        # to make the code more readable and easier to understand:\n","        # (for the transformer based models we will use model subclasses)\n","        print(f\"Model: {run_index}\")\n","        def Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=1,mean_model_results=1):\n","            # set random seeds\n","            set_random_seeds(int(random_seeds[run_index]))\n","\n","            Input_Matrix_Num = tf.keras.layers.Input(shape=(input_nr_dim,), dtype='float32', name='Input_Matrix_Num')\n","            Input_VehBrand = tf.keras.layers.Input(shape=(1,), name='Input_VehBrand')\n","            Input_Region = tf.keras.layers.Input(shape=(1,), name='Input_Region')\n","            Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","\n","            All_Inputs = [Input_Matrix_Num,Input_VehBrand,Input_Region,Input_Exposure]\n","\n","            Emb_VehBrand = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"VehBrand\"].keys()),output_dim=emb_dim,\n","                                    embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05 ),\n","                                    name=\"Embedding_VehBrand\")(Input_VehBrand)\n","            Emb_Region = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"Region\"].keys()),output_dim=emb_dim,\n","                                    embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05),\n","                                name=\"Embedding_Region\")(Input_Region)\n","\n","            Reshaped_Emb_VehBrand = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_VehBrand\")(Emb_VehBrand)\n","            Reshaped_Emb_Region = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_Region\")(Emb_Region)\n","\n","            concatenation_layer = tf.keras.layers.Concatenate(name=\"concatenation_layer\")([Input_Matrix_Num,Reshaped_Emb_VehBrand,Reshaped_Emb_Region])\n","\n","            # Build the network\n","            hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(concatenation_layer)\n","            hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","            hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","            Result_FFN1 = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_FFN1',\n","                            weights=[np.zeros((10, 1)), np.array([np.log(mean_model_results)])],\n","                            trainable=True)(hidden3)\n","\n","            Response = tf.keras.layers.Multiply(name='Result')([Result_FFN1, Input_Exposure])\n","\n","            # Define the model\n","            return tf.keras.models.Model(inputs=All_Inputs, outputs=[Response], name='Poisson_CAT_EMB')\n","\n","        # create the model:\n","        # ----------------------\n","        emb_dim=2\n","        FNN_CAT_EMB = Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=emb_dim,mean_model_results=constant_model)\n","\n","        # load the saved model weights:\n","        # ----------------------\n","        FNN_CAT_EMB.load_weights(f'{storage_path}/saved_models/Poisson_FNN_CAT_EMB_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"FNN_CAT_EMB_{run_index}\"] = np.array([x for [x] in FNN_CAT_EMB.predict(data_nn_emb_learn, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","        y_pred[\"test\"][f\"FNN_CAT_EMB_{run_index}\"] = np.array([x for [x] in FNN_CAT_EMB.predict(data_nn_emb_test, verbose=0,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","\n","\n","        create_single_rebase_results(\"FNN_CAT_EMB\",run_index)\n","    create_ensemble_results(\"Rebase_FNN_CAT_EMB\",index)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_emb_learn, data_nn_emb_test"]},{"cell_type":"markdown","metadata":{"id":"bhzdZssuX48O"},"source":["## 6.3 Rebase CANN:"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1699787438398,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"-TpDmi7sX48O"},"outputs":[],"source":["# create the new exposure times GLM3_pred column for CANN models.\n","df_freq_prep_nn[\"Exposure_x_GLM3_pred\"] = list(poisson_glm3.predict(X_glm3)*df_freq_prep_nn[\"Exposure\"])\n","\n","# Create the dataframes needed for evaluation:\n","data_nn_emb_learn, y_true_learn = create_ffn_cat_emb_data(bool_in_learn, exposure_name = \"Exposure_x_GLM3_pred\")\n","data_nn_emb_test, y_true_test = create_ffn_cat_emb_data(bool_in_test, exposure_name = \"Exposure_x_GLM3_pred\")\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        # Define FNN with Cat. Embedding Model:\n","        # ----------------------\n","        # note we use here the function api instead of the model subclassing\n","        # to make the code more readable and easier to understand:\n","        # (for the transformer based models we will use model subclasses)\n","        print(f\"Model: {run_index}\")\n","        def Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=1,mean_model_results=1):\n","            # set random seeds\n","            set_random_seeds(int(random_seeds[run_index]))\n","\n","            Input_Matrix_Num = tf.keras.layers.Input(shape=(input_nr_dim,), dtype='float32', name='Input_Matrix_Num')\n","            Input_VehBrand = tf.keras.layers.Input(shape=(1,), name='Input_VehBrand')\n","            Input_Region = tf.keras.layers.Input(shape=(1,), name='Input_Region')\n","            Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","\n","            All_Inputs = [Input_Matrix_Num,Input_VehBrand,Input_Region,Input_Exposure]\n","\n","            Emb_VehBrand = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"VehBrand\"].keys()),output_dim=emb_dim,\n","                                    embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05 ),\n","                                    name=\"Embedding_VehBrand\")(Input_VehBrand)\n","            Emb_Region = tf.keras.layers.Embedding(input_dim=len(cat_encoder_all[\"Region\"].keys()),output_dim=emb_dim,\n","                                    embeddings_initializer=keras.initializers.RandomNormal(mean=1.0, stddev=0.05),\n","                                name=\"Embedding_Region\")(Input_Region)\n","\n","            Reshaped_Emb_VehBrand = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_VehBrand\")(Emb_VehBrand)\n","            Reshaped_Emb_Region = tf.keras.layers.Reshape(target_shape=(emb_dim,),name=\"Reshaped_Embedding_Region\")(Emb_Region)\n","\n","            concatenation_layer = tf.keras.layers.Concatenate(name=\"concatenation_layer\")([Input_Matrix_Num,Reshaped_Emb_VehBrand,Reshaped_Emb_Region])\n","\n","            # Build the network\n","            hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(concatenation_layer)\n","            hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","            hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","            Result_FFN1 = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_FFN1',\n","                            weights=[np.zeros((10, 1)), np.array([0])],\n","                            trainable=True)(hidden3)\n","\n","            Response = tf.keras.layers.Multiply(name='Result')([Result_FFN1, Input_Exposure])\n","\n","            # Define the model\n","            return tf.keras.models.Model(inputs=All_Inputs, outputs=[Response], name='Poisson_CAT_EMB')\n","\n","        # create the model:\n","        # ----------------------\n","        emb_dim=2\n","        CANN = Create_Poisson_FNN_CAT_EMB(input_nr_dim=7,emb_dim=emb_dim,mean_model_results=constant_model)\n","\n","        # load the saved model weights:\n","        # ----------------------\n","        CANN.load_weights(f'{storage_path}/saved_models/Poisson_CANN_{run_index}.weights.h5')\n","\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"CANN_{run_index}\"] = np.array([x for [x] in CANN.predict(data_nn_emb_learn, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","        y_pred[\"test\"][f\"CANN_{run_index}\"] = np.array([x for [x] in CANN.predict(data_nn_emb_test, verbose=0,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","\n","        create_single_rebase_results(\"CANN\",run_index)\n","    create_ensemble_results(\"Rebase_CANN\",index)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_emb_learn, data_nn_emb_test\n"]},{"cell_type":"markdown","metadata":{"id":"73OIhwfgX48O"},"source":["## 6.4 Rebase LocalGLMnet:"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":223,"status":"ok","timestamp":1699787455758,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"2gdZ8-iNX48O"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","data_nn_ohe_learn, y_true_learn = create_ffn_ohe_data(bool_in_learn)\n","data_nn_ohe_test, y_true_test = create_ffn_ohe_data(bool_in_test)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FNN with Cat. Embedding Model:\n","        # ----------------------\n","        # note we use here the function api instead of the model subclassing\n","        # to make the code more readable and easier to understand:\n","        # (for the transformer based models we will use model subclasses)\n","\n","        # create dummy glm for initial weights\n","        # ----------------------\n","        poisson_glm_dummy = PoissonRegressor(alpha = 0,max_iter=1000) # scikit-learn.org: alpha = 0 is equivalent to unpenalized GLMs\n","        poisson_glm_dummy.fit(data_nn_ohe_learn[0],y_true_learn/data_nn_ohe_learn[1],sample_weight=data_nn_ohe_learn[1]) # note: data_nn_ohe_learn = [X_ohe,exposure]\n","\n","        # Define LocalGLMnet:\n","        # ----------------------\n","        def Create_Poisson_LocalGLMnet(input_dim=40,initial_glm_bias=1, initial_glm_betas=None):\n","            # set random seeds\n","            set_random_seeds(int(random_seeds[run_index]))\n","            Input_Matrix_OHE = tf.keras.layers.Input(shape=(input_dim,), dtype='float32', name='Input_Matrix')\n","            Input_Exposure = tf.keras.layers.Input(shape=(1,), dtype='float32', name='Input_Exposure')\n","            # Build the network\n","            hidden1 = tf.keras.layers.Dense(units=20, activation=tanh, name='hidden1')(Input_Matrix_OHE)\n","            hidden2 = tf.keras.layers.Dense(units=15, activation=tanh, name='hidden2')(hidden1)\n","            hidden3 = tf.keras.layers.Dense(units=10, activation=tanh, name='hidden3')(hidden2)\n","            Attention = tf.keras.layers.Dense(units=input_dim, activation='linear', name='attention',\n","                            weights=[np.zeros((10, input_dim)), initial_glm_betas])(hidden3)\n","            # note that the weights are set to 0 and the bias is set to the initial glm betas\n","            # create a layer that calculates the dot product between the attention weights (Attention) and the input matrix Input_Matrix_OHE:\n","            # (Attention has the same dimension as the input matrix Input_Matrix_OHE):\n","            weighted_input = tf.keras.layers.Multiply(name='feature_contributions')([Attention, Input_Matrix_OHE])\n","            scalar_product = tf.keras.layers.Dense(units=1, activation='linear', name='scalar_product',\n","                                weights=[np.ones((input_dim, 1)), np.array([0])],\n","                                trainable=False)(weighted_input)\n","            # Note that we actually don't want to make the following weights trainable,\n","            # but to get the bias to be trainable we need to do so. see comment in Book Wüthrich & Merz (2023) page 500\n","            Result_LocalGLMnet_without_Exposure = tf.keras.layers.Dense(units=1, activation='exponential', name='Result_LocalGLMnet_without_Exposure',\n","                            weights=[np.ones((1, 1)), np.array([initial_glm_bias])],\n","                            trainable=True)(scalar_product)\n","            Response = tf.keras.layers.Multiply(name='Result')([Result_LocalGLMnet_without_Exposure, Input_Exposure])\n","            return tf.keras.models.Model(inputs=[Input_Matrix_OHE, Input_Exposure], outputs=[Response], name='Poisson_LocalGLMnet')\n","\n","        # create the model:\n","        # ----------------------\n","        LocalGLMnet = Create_Poisson_LocalGLMnet(input_dim=40,initial_glm_bias=poisson_glm_dummy.intercept_,initial_glm_betas=poisson_glm_dummy.coef_)\n","\n","        # load the saved model weights:\n","        # ----------------------\n","        LocalGLMnet.load_weights(f'{storage_path}/saved_models/Poisson_LocalGLMnet_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"LocalGLMnet_{run_index}\"] = np.array([x for [x] in LocalGLMnet.predict(data_nn_ohe_learn, verbose=0,\n","                                                                            batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","        y_pred[\"test\"][f\"LocalGLMnet_{run_index}\"] = np.array([x for [x] in LocalGLMnet.predict(data_nn_ohe_test, verbose=0,\n","                                                                        batch_size=100000,use_multiprocessing=True, workers=os.cpu_count())])\n","\n","        create_single_rebase_results(\"LocalGLMnet\",run_index)\n","    create_ensemble_results(\"Rebase_LocalGLMnet\",index)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del data_nn_ohe_learn, data_nn_ohe_test\n"]},{"cell_type":"markdown","metadata":{"id":"jadej3rLX48P"},"source":["## 6.5 Rebase FT-Transformer:"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":227,"status":"ok","timestamp":1699787469411,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"ZWxOhy-ZX48P"},"outputs":[],"source":["# Create the dataframes needed for evaluation:\n","# --------------------\n","# NOTE: in the 2021 Gorishniy paper the batch size is different for the different Datasets\n","# but is not hyperparameter tuned. Bigger datasets they used a batch size of 1024 and\n","# for smaller datasets a batch size of (256/512).\n","batch_size = 1024\n","learn_data = df_to_tensor(df_freq_prep_nn[bool_in_learn], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","test_data = df_to_tensor(df_freq_prep_nn[bool_in_test], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","# NOTE we use at first just a fraction of the data to test the code:\n","learn_train_dummy_data = df_to_tensor(df_freq_prep_nn[bool_in_learn_train_dummy], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size,\n","                                      dummy_data_for_build=True)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FT-Transformer Models:\n","        # ----------------------\n","        # NOTE: we use here tensorflow/keras model subclasses (not the functional or sequential api)\n","        # NOTE: we use here instead of the .fit function a costum training loop\n","\n","        # create the model:\n","        # ----------------------\n","        set_random_seeds(int(random_seeds[run_index]))\n","\n","        FT_transformer = EnhActuar.Feature_Tokenizer_Transformer(\n","                emb_dim = 32, # NOTE: In the default setting for the 2021 Gorishniy paper they used emb_dim = 192 (but the parameter size would here go trough the roof, so we use something smaller)\n","                nr_features = nr_col,\n","                cat_features = cat_col,\n","                cat_vocabulary = cat_vocabulary,\n","                count_transformer_blocks = 3,\n","                attention_n_heads = 8,\n","                attention_dropout = 0.2,\n","                ffn_d_hidden = None, # NOTE: change to None if ReGLU should be used -> None uses default value (4/3*emb_dim), they write that they used 2*emb_dim if not ReGLU.\n","                ffn_activation_ReGLU = True, # NOTE: set True if ReGLU should be used\n","                ffn_dropout = 0.1,\n","                prenormalization = True,\n","                output_dim = 1,\n","                last_activation = 'exponential',\n","                exposure_name = \"Exposure\",\n","                seed_nr = int(random_seeds[run_index])\n","        )\n","\n","        FT_transformer.predict(learn_train_dummy_data,verbose=0,batch_size=100000)\n","\n","        # load the best saved model and epochs_and_time from the pickle file:\n","        # ----------------------\n","        # FT_transformer = keras.models.load_model(save_path +'/Poisson_FT_transformer')\n","        FT_transformer.load_weights(f'{storage_path}/saved_models/Poisson_FT_transformer_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"FT_transformer_{run_index}\"] = np.array([x for [x] in FT_transformer.predict(learn_data,verbose=0,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","        y_pred[\"test\"][f\"FT_transformer_{run_index}\"] = np.array([x for [x] in FT_transformer.predict(test_data,verbose=0,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","\n","        create_single_rebase_results(\"FT_transformer\",run_index)\n","    create_ensemble_results(\"Rebase_FT_transformer\",index)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del learn_data, test_data"]},{"cell_type":"markdown","metadata":{"id":"F_IRqRwAX48P"},"source":["## 6.6 Rebase CAFTT:"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":230,"status":"ok","timestamp":1699787478048,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"abD-L7fOX48P"},"outputs":[],"source":["# create the new exposure times GLM3_pred column for CANN models.\n","df_freq_prep_nn[\"Exposure_x_GLM3_pred\"] = list(poisson_glm3.predict(X_glm3)*df_freq_prep_nn[\"Exposure\"])\n","\n","# Create the dataframes needed for evaluation:\n","# --------------------\n","# NOTE: in the 2021 Gorishniy et al paper the batch size is different for the different Datasets\n","# but is not hyperparameter tuned. Bigger datasets they used a batch size of 1024 and\n","# for smaller datasets a batch size of (256/512).\n","batch_size = 1024\n","learn_data = df_to_tensor(df_freq_prep_nn[bool_in_learn], feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size)\n","test_data = df_to_tensor(df_freq_prep_nn[bool_in_test], feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","# NOTE we use at first just a fraction of the data to test the code:\n","learn_train_dummy_data = df_to_tensor(df_freq_prep_nn[bool_in_learn_train_dummy], feature_cols=nr_col+cat_col, exposure=\"Exposure_x_GLM3_pred\", target=\"ClaimNb\", batch_size=batch_size,\n","                                      dummy_data_for_build=True)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FT-Transformer Models:\n","        # ----------------------\n","        # NOTE: we use here tensorflow/keras model subclasses (not the functional or sequential api)\n","        # NOTE: we use here instead of the .fit function a costum training loop\n","\n","        # create the model:\n","        # ----------------------\n","        set_random_seeds(int(random_seeds[run_index]))\n","\n","        FT_transformer = EnhActuar.Feature_Tokenizer_Transformer(\n","                emb_dim = 32, # NOTE: In the default setting for the 2021 Gorishniy paper they used emb_dim = 192 (but the parameter size would here go trough the roof, so we use something smaller)\n","                nr_features = nr_col,\n","                cat_features = cat_col,\n","                cat_vocabulary = cat_vocabulary,\n","                count_transformer_blocks = 3,\n","                attention_n_heads = 8,\n","                attention_dropout = 0.2,\n","                ffn_d_hidden = None, # NOTE: change to None if ReGLU should be used -> None uses default value (4/3*emb_dim), they write that they used 2*emb_dim if not ReGLU.\n","                ffn_activation_ReGLU = True, # NOTE: set True if ReGLU should be used\n","                ffn_dropout = 0.1,\n","                prenormalization = True,\n","                output_dim = 1,\n","                last_activation = 'exponential',\n","                exposure_name = \"Exposure_x_GLM3_pred\",\n","                last_layer_initial_weights = \"zeros\",\n","                last_layer_initial_bias = \"zeros\",\n","                seed_nr = int(random_seeds[run_index])\n","        )\n","\n","        FT_transformer.predict(learn_train_dummy_data,verbose=0,batch_size=100000)\n","\n","        # load the best saved model and epochs_and_time from the pickle file:\n","        # ----------------------\n","        FT_transformer.load_weights(f'{storage_path}/saved_models/Poisson_CAFTT_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"CAFTT_{run_index}\"] = np.array([x for [x] in FT_transformer.predict(learn_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","        y_pred[\"test\"][f\"CAFTT_{run_index}\"] = np.array([x for [x] in FT_transformer.predict(test_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","\n","        create_single_rebase_results(\"CAFTT\",run_index)\n","    create_ensemble_results(\"Rebase_CAFTT\",index)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del learn_data, test_data"]},{"cell_type":"markdown","metadata":{"id":"TiNQM4GKX48P"},"source":["## 6.7 Rebase LocalGLMftt:"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":233,"status":"ok","timestamp":1699787497700,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"Bm5YxNQeX48Q"},"outputs":[],"source":["# Create the dataframes for creation of the glm-ohe-start model:\n","# --------------------\n","data_nn_ohe_learn, y_true_learn = create_ffn_ohe_data(bool_in_learn)\n","\n","# create dummy glm for initial weights\n","# ----------------------\n","poisson_glm_dummy = PoissonRegressor(alpha = 0,max_iter=1000) # scikit-learn.org: alpha = 0 is equivalent to unpenalized GLMs\n","poisson_glm_dummy.fit(data_nn_ohe_learn[0],y_true_learn/data_nn_ohe_learn[1],sample_weight=data_nn_ohe_learn[1]) # note: data_nn_ohe_learn = [X_ohe,exposure]\n","# get the betas from the glm:\n","glm_nr_col_betas = poisson_glm_dummy.coef_[:len(nr_col)]\n","current_beta_index = len(nr_col)\n","glm_cat_col_betas = {}\n","for c in cat_vocabulary.keys():\n","    glm_cat_col_betas[c] = poisson_glm_dummy.coef_[current_beta_index:current_beta_index+len(cat_vocabulary[c])]\n","    current_beta_index += len(cat_vocabulary[c])\n","glm_intercept = poisson_glm_dummy.intercept_\n","\n","\n","# Create the dataframes needed for evaluation:\n","# --------------------\n","# NOTE: in the 2021 Gorishniy paper the batch size is different for the different Datasets\n","# but is not hyperparameter tuned. Bigger datasets they used a batch size of 1024 and\n","# for smaller datasets a batch size of (256/512).\n","batch_size = 1024\n","learn_data = df_to_tensor(df_freq_prep_nn[bool_in_learn], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","test_data = df_to_tensor(df_freq_prep_nn[bool_in_test], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size)\n","\n","learn_train_dummy_data = df_to_tensor(df_freq_prep_nn[bool_in_learn_train_dummy], feature_cols=nr_col+cat_col, exposure=\"Exposure\", target=\"ClaimNb\", batch_size=batch_size,\n","                                      dummy_data_for_build=True)\n","\n","for index, ensemble_range in enumerate([range(0,5),range(5,10),range(10,15)]):\n","    print(ensemble_range)\n","    for run_index in ensemble_range:\n","\n","        print(f\"Model: {run_index}\")\n","        # Define FT-Transformer Models:\n","        # ----------------------\n","        # NOTE: we use here tensorflow/keras model subclasses (not the functional or sequential api)\n","        # NOTE: we use here instead of the .fit function a costum training loop\n","\n","        # create the model:\n","        # ----------------------\n","        set_random_seeds(int(random_seeds[run_index]))\n","        LocalGLMftt = EnhActuar.LocalGLM_FT_Transformer(\n","                emb_dim = 32, # NOTE: In the default setting for the 2021 Gorishniy paper they used emb_dim = 192 (but the parameter size would here go trough the roof, so we use something smaller)\n","                nr_features = nr_col,\n","                cat_features = cat_col,\n","                cat_vocabulary = cat_vocabulary,\n","                count_transformer_blocks = 3,\n","                attention_n_heads = 8,\n","                attention_dropout = 0.2,\n","                ffn_d_hidden = None, # NOTE: change to None if ReGLU should be used -> None uses default value (4/3*emb_dim), they write that they used 2*emb_dim if not ReGLU.\n","                ffn_activation_ReGLU = True, # NOTE: set True if ReGLU should be used\n","                ffn_dropout = 0.1,\n","                prenormalization = True,\n","                output_dim = 1,\n","                last_activation = 'exponential',\n","                exposure_name = \"Exposure\",\n","                last_layer_initial_weights = \"zeros\",\n","                last_layer_initial_bias = \"ones\",\n","                init_glm_cat_col_weights = glm_cat_col_betas,\n","                init_glm_nr_col_weights = glm_nr_col_betas,\n","                init_glm_bias = glm_intercept,\n","                trainable_glm_emb = False,\n","                seed_nr = int(random_seeds[run_index])\n","        )\n","\n","        LocalGLMftt.predict(learn_train_dummy_data,verbose=0,batch_size=100000)\n","\n","        # load the best saved model and epochs_and_time from the pickle file:\n","        # ----------------------\n","        LocalGLMftt.load_weights(f'{storage_path}/saved_models/Poisson_LocalGLMftt_{run_index}.weights.h5')\n","\n","        # predict with the model:\n","        # ----------------------\n","        y_pred[\"train\"][f\"LocalGLMftt_{run_index}\"] = np.array([x for [x] in LocalGLMftt.predict(learn_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","        y_pred[\"test\"][f\"LocalGLMftt_{run_index}\"] = np.array([x for [x] in LocalGLMftt.predict(test_data,batch_size=100000,use_multiprocessing=True, workers=os.cpu_count()\n","                                                                                    )[\"output\"]])\n","\n","        create_single_rebase_results(\"LocalGLMftt\",run_index)\n","    create_ensemble_results(\"Rebase_LocalGLMftt\",index)\n","\n","# because notebooks have no garbage collector, we delete here the unneeded data:\n","del learn_data, test_data\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CvFw3ZdTX48Q"},"outputs":[],"source":["# # save the results:\n","# with open(f'{storage_path}/Data/df_results.pickle', 'wb') as handle:\n","#     pickle.dump(df_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","# load the results:\n","with open(f'{storage_path}/Data/df_results.pickle', 'rb') as handle:\n","    df_results = pickle.load(handle)\n"]},{"cell_type":"markdown","metadata":{"id":"-BK3RArXjymx"},"source":["# Result:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CA8ZY117du_X"},"outputs":[],"source":["# display(df_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":808},"executionInfo":{"elapsed":429,"status":"ok","timestamp":1699573695955,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"F4N4N_b9duk9","outputId":"cc0b0425-1367-4752-9246-5117d2bde0af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results Average:\n"]},{"data":{"text/html":["\n","  <div id=\"df-cb2d2973-0f48-43fc-84c8-ad6fed502804\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>homogeneous model</td>\n","      <td>0.000000</td>\n","      <td>0.054847</td>\n","      <td>1.0</td>\n","      <td>0.252132</td>\n","      <td>0.254454</td>\n","      <td>0.073631</td>\n","      <td>0.073631</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GLM1</td>\n","      <td>0.000000</td>\n","      <td>2.220158</td>\n","      <td>49.0</td>\n","      <td>0.241015</td>\n","      <td>0.241463</td>\n","      <td>0.073631</td>\n","      <td>0.073900</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GLM2</td>\n","      <td>0.000000</td>\n","      <td>2.752693</td>\n","      <td>48.0</td>\n","      <td>0.240911</td>\n","      <td>0.241125</td>\n","      <td>0.073631</td>\n","      <td>0.073981</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GLM3</td>\n","      <td>0.000000</td>\n","      <td>1.900497</td>\n","      <td>50.0</td>\n","      <td>0.240844</td>\n","      <td>0.241022</td>\n","      <td>0.073631</td>\n","      <td>0.074048</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>FFN_OHE</td>\n","      <td>42.200000</td>\n","      <td>37.805560</td>\n","      <td>1306.0</td>\n","      <td>0.237535</td>\n","      <td>0.238652</td>\n","      <td>0.073906</td>\n","      <td>0.074310</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>FNN_CAT_EMB</td>\n","      <td>72.933333</td>\n","      <td>58.728892</td>\n","      <td>792.0</td>\n","      <td>0.237682</td>\n","      <td>0.238267</td>\n","      <td>0.073774</td>\n","      <td>0.074238</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>CANN</td>\n","      <td>90.333333</td>\n","      <td>68.559120</td>\n","      <td>792.0</td>\n","      <td>0.237420</td>\n","      <td>0.238102</td>\n","      <td>0.074019</td>\n","      <td>0.074438</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LocalGLMnet</td>\n","      <td>25.333333</td>\n","      <td>29.720892</td>\n","      <td>1737.0</td>\n","      <td>0.237095</td>\n","      <td>0.239211</td>\n","      <td>0.073825</td>\n","      <td>0.074267</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>FT_transformer</td>\n","      <td>78.866667</td>\n","      <td>1569.860410</td>\n","      <td>27133.0</td>\n","      <td>0.237803</td>\n","      <td>0.239389</td>\n","      <td>0.061140</td>\n","      <td>0.061290</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>CAFTT</td>\n","      <td>57.333333</td>\n","      <td>1170.160723</td>\n","      <td>27133.0</td>\n","      <td>0.237146</td>\n","      <td>0.238072</td>\n","      <td>0.065975</td>\n","      <td>0.066235</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LocalGLMftt</td>\n","      <td>53.200000</td>\n","      <td>1187.006637</td>\n","      <td>27430.0</td>\n","      <td>0.237214</td>\n","      <td>0.238801</td>\n","      <td>0.067904</td>\n","      <td>0.068316</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb2d2973-0f48-43fc-84c8-ad6fed502804')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cb2d2973-0f48-43fc-84c8-ad6fed502804 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cb2d2973-0f48-43fc-84c8-ad6fed502804');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-04343633-6081-458e-8f11-593568fc5a1b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-04343633-6081-458e-8f11-593568fc5a1b')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-04343633-6081-458e-8f11-593568fc5a1b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                model     epochs     run_time  nr_parameters  loss_train  \\\n","0   homogeneous model   0.000000     0.054847            1.0    0.252132   \n","1                GLM1   0.000000     2.220158           49.0    0.241015   \n","2                GLM2   0.000000     2.752693           48.0    0.240911   \n","3                GLM3   0.000000     1.900497           50.0    0.240844   \n","4             FFN_OHE  42.200000    37.805560         1306.0    0.237535   \n","5         FNN_CAT_EMB  72.933333    58.728892          792.0    0.237682   \n","6                CANN  90.333333    68.559120          792.0    0.237420   \n","7         LocalGLMnet  25.333333    29.720892         1737.0    0.237095   \n","8      FT_transformer  78.866667  1569.860410        27133.0    0.237803   \n","9               CAFTT  57.333333  1170.160723        27133.0    0.237146   \n","10        LocalGLMftt  53.200000  1187.006637        27430.0    0.237214   \n","\n","    loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0    0.254454             0.073631            0.073631  \n","1    0.241463             0.073631            0.073900  \n","2    0.241125             0.073631            0.073981  \n","3    0.241022             0.073631            0.074048  \n","4    0.238652             0.073906            0.074310  \n","5    0.238267             0.073774            0.074238  \n","6    0.238102             0.074019            0.074438  \n","7    0.239211             0.073825            0.074267  \n","8    0.239389             0.061140            0.061290  \n","9    0.238072             0.065975            0.066235  \n","10   0.238801             0.067904            0.068316  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Results Standard-Deviation:\n"]},{"data":{"text/html":["\n","  <div id=\"df-7821e096-a950-40b2-a312-d0f77f405adc\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>homogeneous model</td>\n","      <td>0.000000</td>\n","      <td>0.002512</td>\n","      <td>0.0</td>\n","      <td>0.000000e+00</td>\n","      <td>5.745950e-17</td>\n","      <td>2.872975e-17</td>\n","      <td>2.872975e-17</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GLM1</td>\n","      <td>0.000000</td>\n","      <td>0.473819</td>\n","      <td>0.0</td>\n","      <td>5.745950e-17</td>\n","      <td>5.745950e-17</td>\n","      <td>1.436488e-17</td>\n","      <td>0.000000e+00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GLM2</td>\n","      <td>0.000000</td>\n","      <td>0.970156</td>\n","      <td>0.0</td>\n","      <td>5.745950e-17</td>\n","      <td>2.872975e-17</td>\n","      <td>0.000000e+00</td>\n","      <td>1.436488e-17</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GLM3</td>\n","      <td>0.000000</td>\n","      <td>0.408252</td>\n","      <td>0.0</td>\n","      <td>2.872975e-17</td>\n","      <td>2.872975e-17</td>\n","      <td>0.000000e+00</td>\n","      <td>1.436488e-17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>FFN_OHE</td>\n","      <td>14.663853</td>\n","      <td>8.624492</td>\n","      <td>0.0</td>\n","      <td>3.255191e-04</td>\n","      <td>1.570462e-04</td>\n","      <td>1.223993e-03</td>\n","      <td>1.209107e-03</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>FNN_CAT_EMB</td>\n","      <td>21.661245</td>\n","      <td>13.907265</td>\n","      <td>0.0</td>\n","      <td>1.590947e-04</td>\n","      <td>1.514444e-04</td>\n","      <td>1.071399e-03</td>\n","      <td>1.088943e-03</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>CANN</td>\n","      <td>53.898935</td>\n","      <td>33.162059</td>\n","      <td>0.0</td>\n","      <td>6.076588e-04</td>\n","      <td>3.253586e-04</td>\n","      <td>1.111365e-03</td>\n","      <td>1.103431e-03</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LocalGLMnet</td>\n","      <td>7.622023</td>\n","      <td>5.097405</td>\n","      <td>0.0</td>\n","      <td>3.340630e-04</td>\n","      <td>2.176521e-04</td>\n","      <td>8.787938e-04</td>\n","      <td>9.078453e-04</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>FT_transformer</td>\n","      <td>16.638452</td>\n","      <td>302.316624</td>\n","      <td>0.0</td>\n","      <td>8.982910e-04</td>\n","      <td>5.281384e-04</td>\n","      <td>1.459836e-03</td>\n","      <td>1.458034e-03</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>CAFTT</td>\n","      <td>14.185841</td>\n","      <td>220.449926</td>\n","      <td>0.0</td>\n","      <td>4.739992e-04</td>\n","      <td>1.743234e-04</td>\n","      <td>4.993066e-04</td>\n","      <td>4.707813e-04</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LocalGLMftt</td>\n","      <td>16.384662</td>\n","      <td>310.648783</td>\n","      <td>0.0</td>\n","      <td>5.933743e-04</td>\n","      <td>1.596498e-04</td>\n","      <td>9.643511e-04</td>\n","      <td>9.918441e-04</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7821e096-a950-40b2-a312-d0f77f405adc')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7821e096-a950-40b2-a312-d0f77f405adc button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7821e096-a950-40b2-a312-d0f77f405adc');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-441c5dc7-b742-4aef-81d9-b53dbf6650c1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-441c5dc7-b742-4aef-81d9-b53dbf6650c1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-441c5dc7-b742-4aef-81d9-b53dbf6650c1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                model     epochs    run_time  nr_parameters    loss_train  \\\n","0   homogeneous model   0.000000    0.002512            0.0  0.000000e+00   \n","1                GLM1   0.000000    0.473819            0.0  5.745950e-17   \n","2                GLM2   0.000000    0.970156            0.0  5.745950e-17   \n","3                GLM3   0.000000    0.408252            0.0  2.872975e-17   \n","4             FFN_OHE  14.663853    8.624492            0.0  3.255191e-04   \n","5         FNN_CAT_EMB  21.661245   13.907265            0.0  1.590947e-04   \n","6                CANN  53.898935   33.162059            0.0  6.076588e-04   \n","7         LocalGLMnet   7.622023    5.097405            0.0  3.340630e-04   \n","8      FT_transformer  16.638452  302.316624            0.0  8.982910e-04   \n","9               CAFTT  14.185841  220.449926            0.0  4.739992e-04   \n","10        LocalGLMftt  16.384662  310.648783            0.0  5.933743e-04   \n","\n","       loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0   5.745950e-17         2.872975e-17        2.872975e-17  \n","1   5.745950e-17         1.436488e-17        0.000000e+00  \n","2   2.872975e-17         0.000000e+00        1.436488e-17  \n","3   2.872975e-17         0.000000e+00        1.436488e-17  \n","4   1.570462e-04         1.223993e-03        1.209107e-03  \n","5   1.514444e-04         1.071399e-03        1.088943e-03  \n","6   3.253586e-04         1.111365e-03        1.103431e-03  \n","7   2.176521e-04         8.787938e-04        9.078453e-04  \n","8   5.281384e-04         1.459836e-03        1.458034e-03  \n","9   1.743234e-04         4.993066e-04        4.707813e-04  \n","10  1.596498e-04         9.643511e-04        9.918441e-04  "]},"metadata":{},"output_type":"display_data"}],"source":["print(\"Results Average:\")\n","display(calc_avg_df([\"homogeneous model\",\"GLM1\",\"GLM2\",\"GLM3\",\"FFN_OHE\",\"FNN_CAT_EMB\",\"CANN\",\"LocalGLMnet\",\"FT_transformer\",\"CAFTT\",\"LocalGLMftt\"]))\n","print(\"Results Standard-Deviation:\")\n","display(calc_std_df([\"homogeneous model\",\"GLM1\",\"GLM2\",\"GLM3\",\"FFN_OHE\",\"FNN_CAT_EMB\",\"CANN\",\"LocalGLMnet\",\"FT_transformer\",\"CAFTT\",\"LocalGLMftt\"]))"]},{"cell_type":"markdown","metadata":{"id":"W-nrJqqdFIFb"},"source":["## Compare Single Model Results to Ensemble (not rebalanced) Results:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":996},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1699573703038,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"T6uVrSFNduMP","outputId":"470c6d6c-f264-41e6-8b4f-0d6928b85e9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results Average:\n"]},{"data":{"text/html":["\n","  <div id=\"df-075c53c9-5fb0-4c5b-84f5-271a6fb84237\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FFN_OHE</td>\n","      <td>42.200000</td>\n","      <td>37.805560</td>\n","      <td>1306.0</td>\n","      <td>0.237535</td>\n","      <td>0.238652</td>\n","      <td>0.073906</td>\n","      <td>0.074310</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Ensemble_FFN_OHE</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.237152</td>\n","      <td>0.238260</td>\n","      <td>0.073906</td>\n","      <td>0.074310</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>FNN_CAT_EMB</td>\n","      <td>72.933333</td>\n","      <td>58.728892</td>\n","      <td>792.0</td>\n","      <td>0.237682</td>\n","      <td>0.238267</td>\n","      <td>0.073774</td>\n","      <td>0.074238</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Ensemble_FNN_CAT_EMB</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.237432</td>\n","      <td>0.238009</td>\n","      <td>0.073773</td>\n","      <td>0.074238</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CANN</td>\n","      <td>90.333333</td>\n","      <td>68.559120</td>\n","      <td>792.0</td>\n","      <td>0.237420</td>\n","      <td>0.238102</td>\n","      <td>0.074019</td>\n","      <td>0.074438</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Ensemble_CANN</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.237013</td>\n","      <td>0.237699</td>\n","      <td>0.074019</td>\n","      <td>0.074438</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LocalGLMnet</td>\n","      <td>25.333333</td>\n","      <td>29.720892</td>\n","      <td>1737.0</td>\n","      <td>0.237095</td>\n","      <td>0.239211</td>\n","      <td>0.073825</td>\n","      <td>0.074267</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Ensemble_LocalGLMnet</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.236635</td>\n","      <td>0.238734</td>\n","      <td>0.073825</td>\n","      <td>0.074267</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>FT_transformer</td>\n","      <td>78.866667</td>\n","      <td>1569.860410</td>\n","      <td>27133.0</td>\n","      <td>0.237803</td>\n","      <td>0.239389</td>\n","      <td>0.061140</td>\n","      <td>0.061290</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Ensemble_FT_transformer</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.237176</td>\n","      <td>0.238803</td>\n","      <td>0.061140</td>\n","      <td>0.061290</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>CAFTT</td>\n","      <td>57.333333</td>\n","      <td>1170.160723</td>\n","      <td>27133.0</td>\n","      <td>0.237146</td>\n","      <td>0.238072</td>\n","      <td>0.065975</td>\n","      <td>0.066235</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Ensemble_CAFTT</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.236748</td>\n","      <td>0.237675</td>\n","      <td>0.065975</td>\n","      <td>0.066235</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>LocalGLMftt</td>\n","      <td>53.200000</td>\n","      <td>1187.006637</td>\n","      <td>27430.0</td>\n","      <td>0.237214</td>\n","      <td>0.238801</td>\n","      <td>0.067904</td>\n","      <td>0.068316</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Ensemble_LocalGLMftt</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.236705</td>\n","      <td>0.238319</td>\n","      <td>0.067904</td>\n","      <td>0.068316</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-075c53c9-5fb0-4c5b-84f5-271a6fb84237')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-075c53c9-5fb0-4c5b-84f5-271a6fb84237 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-075c53c9-5fb0-4c5b-84f5-271a6fb84237');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-d72ba005-fc7c-4c35-b755-57573159496e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d72ba005-fc7c-4c35-b755-57573159496e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-d72ba005-fc7c-4c35-b755-57573159496e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                      model     epochs     run_time  nr_parameters  \\\n","0                   FFN_OHE  42.200000    37.805560         1306.0   \n","1          Ensemble_FFN_OHE   0.000000     0.000000            0.0   \n","2               FNN_CAT_EMB  72.933333    58.728892          792.0   \n","3      Ensemble_FNN_CAT_EMB   0.000000     0.000000            0.0   \n","4                      CANN  90.333333    68.559120          792.0   \n","5             Ensemble_CANN   0.000000     0.000000            0.0   \n","6               LocalGLMnet  25.333333    29.720892         1737.0   \n","7      Ensemble_LocalGLMnet   0.000000     0.000000            0.0   \n","8            FT_transformer  78.866667  1569.860410        27133.0   \n","9   Ensemble_FT_transformer   0.000000     0.000000            0.0   \n","10                    CAFTT  57.333333  1170.160723        27133.0   \n","11           Ensemble_CAFTT   0.000000     0.000000            0.0   \n","12              LocalGLMftt  53.200000  1187.006637        27430.0   \n","13     Ensemble_LocalGLMftt   0.000000     0.000000            0.0   \n","\n","    loss_train  loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0     0.237535   0.238652             0.073906            0.074310  \n","1     0.237152   0.238260             0.073906            0.074310  \n","2     0.237682   0.238267             0.073774            0.074238  \n","3     0.237432   0.238009             0.073773            0.074238  \n","4     0.237420   0.238102             0.074019            0.074438  \n","5     0.237013   0.237699             0.074019            0.074438  \n","6     0.237095   0.239211             0.073825            0.074267  \n","7     0.236635   0.238734             0.073825            0.074267  \n","8     0.237803   0.239389             0.061140            0.061290  \n","9     0.237176   0.238803             0.061140            0.061290  \n","10    0.237146   0.238072             0.065975            0.066235  \n","11    0.236748   0.237675             0.065975            0.066235  \n","12    0.237214   0.238801             0.067904            0.068316  \n","13    0.236705   0.238319             0.067904            0.068316  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Results Standard-Deviation:\n"]},{"data":{"text/html":["\n","  <div id=\"df-91ae0d7d-b441-41a6-bce6-51607c8058ca\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FFN_OHE</td>\n","      <td>14.663853</td>\n","      <td>8.624492</td>\n","      <td>0.0</td>\n","      <td>0.000326</td>\n","      <td>0.000157</td>\n","      <td>0.001224</td>\n","      <td>0.001209</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Ensemble_FFN_OHE</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000224</td>\n","      <td>0.000098</td>\n","      <td>0.000648</td>\n","      <td>0.000667</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>FNN_CAT_EMB</td>\n","      <td>21.661245</td>\n","      <td>13.907265</td>\n","      <td>0.0</td>\n","      <td>0.000159</td>\n","      <td>0.000151</td>\n","      <td>0.001071</td>\n","      <td>0.001089</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Ensemble_FNN_CAT_EMB</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000034</td>\n","      <td>0.000109</td>\n","      <td>0.000603</td>\n","      <td>0.000630</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CANN</td>\n","      <td>53.898935</td>\n","      <td>33.162059</td>\n","      <td>0.0</td>\n","      <td>0.000608</td>\n","      <td>0.000325</td>\n","      <td>0.001111</td>\n","      <td>0.001103</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Ensemble_CANN</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000434</td>\n","      <td>0.000299</td>\n","      <td>0.000732</td>\n","      <td>0.000739</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LocalGLMnet</td>\n","      <td>7.622023</td>\n","      <td>5.097405</td>\n","      <td>0.0</td>\n","      <td>0.000334</td>\n","      <td>0.000218</td>\n","      <td>0.000879</td>\n","      <td>0.000908</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Ensemble_LocalGLMnet</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000133</td>\n","      <td>0.000017</td>\n","      <td>0.000105</td>\n","      <td>0.000106</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>FT_transformer</td>\n","      <td>16.638452</td>\n","      <td>302.316624</td>\n","      <td>0.0</td>\n","      <td>0.000898</td>\n","      <td>0.000528</td>\n","      <td>0.001460</td>\n","      <td>0.001458</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Ensemble_FT_transformer</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000066</td>\n","      <td>0.000143</td>\n","      <td>0.000132</td>\n","      <td>0.000109</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>CAFTT</td>\n","      <td>14.185841</td>\n","      <td>220.449926</td>\n","      <td>0.0</td>\n","      <td>0.000474</td>\n","      <td>0.000174</td>\n","      <td>0.000499</td>\n","      <td>0.000471</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Ensemble_CAFTT</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000201</td>\n","      <td>0.000073</td>\n","      <td>0.000131</td>\n","      <td>0.000120</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>LocalGLMftt</td>\n","      <td>16.384662</td>\n","      <td>310.648783</td>\n","      <td>0.0</td>\n","      <td>0.000593</td>\n","      <td>0.000160</td>\n","      <td>0.000964</td>\n","      <td>0.000992</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Ensemble_LocalGLMftt</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000348</td>\n","      <td>0.000114</td>\n","      <td>0.000343</td>\n","      <td>0.000319</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91ae0d7d-b441-41a6-bce6-51607c8058ca')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-91ae0d7d-b441-41a6-bce6-51607c8058ca button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-91ae0d7d-b441-41a6-bce6-51607c8058ca');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-19777b90-5c9c-4e58-8e37-697a7973ddc9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-19777b90-5c9c-4e58-8e37-697a7973ddc9')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-19777b90-5c9c-4e58-8e37-697a7973ddc9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                      model     epochs    run_time  nr_parameters  loss_train  \\\n","0                   FFN_OHE  14.663853    8.624492            0.0    0.000326   \n","1          Ensemble_FFN_OHE   0.000000    0.000000            0.0    0.000224   \n","2               FNN_CAT_EMB  21.661245   13.907265            0.0    0.000159   \n","3      Ensemble_FNN_CAT_EMB   0.000000    0.000000            0.0    0.000034   \n","4                      CANN  53.898935   33.162059            0.0    0.000608   \n","5             Ensemble_CANN   0.000000    0.000000            0.0    0.000434   \n","6               LocalGLMnet   7.622023    5.097405            0.0    0.000334   \n","7      Ensemble_LocalGLMnet   0.000000    0.000000            0.0    0.000133   \n","8            FT_transformer  16.638452  302.316624            0.0    0.000898   \n","9   Ensemble_FT_transformer   0.000000    0.000000            0.0    0.000066   \n","10                    CAFTT  14.185841  220.449926            0.0    0.000474   \n","11           Ensemble_CAFTT   0.000000    0.000000            0.0    0.000201   \n","12              LocalGLMftt  16.384662  310.648783            0.0    0.000593   \n","13     Ensemble_LocalGLMftt   0.000000    0.000000            0.0    0.000348   \n","\n","    loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0    0.000157             0.001224            0.001209  \n","1    0.000098             0.000648            0.000667  \n","2    0.000151             0.001071            0.001089  \n","3    0.000109             0.000603            0.000630  \n","4    0.000325             0.001111            0.001103  \n","5    0.000299             0.000732            0.000739  \n","6    0.000218             0.000879            0.000908  \n","7    0.000017             0.000105            0.000106  \n","8    0.000528             0.001460            0.001458  \n","9    0.000143             0.000132            0.000109  \n","10   0.000174             0.000499            0.000471  \n","11   0.000073             0.000131            0.000120  \n","12   0.000160             0.000964            0.000992  \n","13   0.000114             0.000343            0.000319  "]},"metadata":{},"output_type":"display_data"}],"source":["print(\"Results Average:\")\n","display(calc_avg_df([\"FFN_OHE\",\"Ensemble_FFN_OHE\",\n","                     \"FNN_CAT_EMB\",\"Ensemble_FNN_CAT_EMB\",\n","                     \"CANN\",\"Ensemble_CANN\",\n","                     \"LocalGLMnet\",\"Ensemble_LocalGLMnet\",\n","                     \"FT_transformer\",\"Ensemble_FT_transformer\",\n","                     \"CAFTT\", \"Ensemble_CAFTT\",\n","                     \"LocalGLMftt\", \"Ensemble_LocalGLMftt\"]))\n","print(\"Results Standard-Deviation:\")\n","display(calc_std_df([\"FFN_OHE\",\"Ensemble_FFN_OHE\",\n","                     \"FNN_CAT_EMB\",\"Ensemble_FNN_CAT_EMB\",\n","                     \"CANN\",\"Ensemble_CANN\",\n","                     \"LocalGLMnet\",\"Ensemble_LocalGLMnet\",\n","                     \"FT_transformer\",\"Ensemble_FT_transformer\",\n","                     \"CAFTT\", \"Ensemble_CAFTT\",\n","                     \"LocalGLMftt\", \"Ensemble_LocalGLMftt\"]))"]},{"cell_type":"markdown","metadata":{"id":"6fSgosapeyOu"},"source":["## Rebased Results (FT_transformer):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":683},"executionInfo":{"elapsed":539,"status":"ok","timestamp":1699573884686,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"byeZy_4ge0Kd","outputId":"2fc104d3-6340-4917-a381-b878a7e044be"},"outputs":[{"name":"stdout","output_type":"stream","text":["Results Average:\n"]},{"data":{"text/html":["\n","  <div id=\"df-42e5842a-1ee9-4e9b-8642-e404079d6320\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FT_transformer</td>\n","      <td>78.866667</td>\n","      <td>1569.860410</td>\n","      <td>27133.0</td>\n","      <td>0.237803</td>\n","      <td>0.239389</td>\n","      <td>0.061140</td>\n","      <td>0.061290</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Rebase_FT_transformer</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.236517</td>\n","      <td>0.238149</td>\n","      <td>0.073631</td>\n","      <td>0.073812</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ensemble_Rebase_FT_transformer</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.235922</td>\n","      <td>0.237587</td>\n","      <td>0.073631</td>\n","      <td>0.073812</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CAFTT</td>\n","      <td>57.333333</td>\n","      <td>1170.160723</td>\n","      <td>27133.0</td>\n","      <td>0.237146</td>\n","      <td>0.238072</td>\n","      <td>0.065975</td>\n","      <td>0.066235</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Rebase_CAFTT</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.236692</td>\n","      <td>0.237659</td>\n","      <td>0.073631</td>\n","      <td>0.073921</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Ensemble_Rebase_CAFTT</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.236296</td>\n","      <td>0.237263</td>\n","      <td>0.073631</td>\n","      <td>0.073921</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LocalGLMftt</td>\n","      <td>53.200000</td>\n","      <td>1187.006637</td>\n","      <td>27430.0</td>\n","      <td>0.237214</td>\n","      <td>0.238801</td>\n","      <td>0.067904</td>\n","      <td>0.068316</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Rebase_LocalGLMftt</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.236958</td>\n","      <td>0.238589</td>\n","      <td>0.073631</td>\n","      <td>0.074077</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Ensemble_Rebase_LocalGLMftt</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.236448</td>\n","      <td>0.238111</td>\n","      <td>0.073631</td>\n","      <td>0.074077</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42e5842a-1ee9-4e9b-8642-e404079d6320')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-42e5842a-1ee9-4e9b-8642-e404079d6320 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-42e5842a-1ee9-4e9b-8642-e404079d6320');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-9d5d5b86-361c-45f5-938a-1c83f8abd1e1\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d5d5b86-361c-45f5-938a-1c83f8abd1e1')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-9d5d5b86-361c-45f5-938a-1c83f8abd1e1 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                            model     epochs     run_time  nr_parameters  \\\n","0                  FT_transformer  78.866667  1569.860410        27133.0   \n","1           Rebase_FT_transformer   0.000000     0.000000            0.0   \n","2  Ensemble_Rebase_FT_transformer   0.000000     0.000000            0.0   \n","3                           CAFTT  57.333333  1170.160723        27133.0   \n","4                    Rebase_CAFTT   0.000000     0.000000            0.0   \n","5           Ensemble_Rebase_CAFTT   0.000000     0.000000            0.0   \n","6                     LocalGLMftt  53.200000  1187.006637        27430.0   \n","7              Rebase_LocalGLMftt   0.000000     0.000000            0.0   \n","8     Ensemble_Rebase_LocalGLMftt   0.000000     0.000000            0.0   \n","\n","   loss_train  loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0    0.237803   0.239389             0.061140            0.061290  \n","1    0.236517   0.238149             0.073631            0.073812  \n","2    0.235922   0.237587             0.073631            0.073812  \n","3    0.237146   0.238072             0.065975            0.066235  \n","4    0.236692   0.237659             0.073631            0.073921  \n","5    0.236296   0.237263             0.073631            0.073921  \n","6    0.237214   0.238801             0.067904            0.068316  \n","7    0.236958   0.238589             0.073631            0.074077  \n","8    0.236448   0.238111             0.073631            0.074077  "]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Results Standard-Deviation:\n"]},{"data":{"text/html":["\n","  <div id=\"df-8ee969cc-1df7-46bb-b972-9aa7d1a3f49c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model</th>\n","      <th>epochs</th>\n","      <th>run_time</th>\n","      <th>nr_parameters</th>\n","      <th>loss_train</th>\n","      <th>loss_test</th>\n","      <th>pred_avg_freq_train</th>\n","      <th>pred_avg_freq_test</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FT_transformer</td>\n","      <td>16.638452</td>\n","      <td>302.316624</td>\n","      <td>0.0</td>\n","      <td>0.000898</td>\n","      <td>0.000528</td>\n","      <td>1.459836e-03</td>\n","      <td>0.001458</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Rebase_FT_transformer</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000636</td>\n","      <td>0.000360</td>\n","      <td>1.299345e-08</td>\n","      <td>0.000106</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Ensemble_Rebase_FT_transformer</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000035</td>\n","      <td>0.000144</td>\n","      <td>1.947447e-08</td>\n","      <td>0.000027</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CAFTT</td>\n","      <td>14.185841</td>\n","      <td>220.449926</td>\n","      <td>0.0</td>\n","      <td>0.000474</td>\n","      <td>0.000174</td>\n","      <td>4.993066e-04</td>\n","      <td>0.000471</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Rebase_CAFTT</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000458</td>\n","      <td>0.000171</td>\n","      <td>1.597395e-08</td>\n","      <td>0.000055</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Ensemble_Rebase_CAFTT</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000185</td>\n","      <td>0.000060</td>\n","      <td>3.497719e-09</td>\n","      <td>0.000014</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LocalGLMftt</td>\n","      <td>16.384662</td>\n","      <td>310.648783</td>\n","      <td>0.0</td>\n","      <td>0.000593</td>\n","      <td>0.000160</td>\n","      <td>9.643511e-04</td>\n","      <td>0.000992</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Rebase_LocalGLMftt</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000659</td>\n","      <td>0.000170</td>\n","      <td>1.735819e-08</td>\n","      <td>0.000067</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Ensemble_Rebase_LocalGLMftt</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.000338</td>\n","      <td>0.000099</td>\n","      <td>2.731805e-08</td>\n","      <td>0.000028</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ee969cc-1df7-46bb-b972-9aa7d1a3f49c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8ee969cc-1df7-46bb-b972-9aa7d1a3f49c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8ee969cc-1df7-46bb-b972-9aa7d1a3f49c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-a965419b-6c3c-4046-90a7-e2ac82f10aa7\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a965419b-6c3c-4046-90a7-e2ac82f10aa7')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-a965419b-6c3c-4046-90a7-e2ac82f10aa7 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["                            model     epochs    run_time  nr_parameters  \\\n","0                  FT_transformer  16.638452  302.316624            0.0   \n","1           Rebase_FT_transformer   0.000000    0.000000            0.0   \n","2  Ensemble_Rebase_FT_transformer   0.000000    0.000000            0.0   \n","3                           CAFTT  14.185841  220.449926            0.0   \n","4                    Rebase_CAFTT   0.000000    0.000000            0.0   \n","5           Ensemble_Rebase_CAFTT   0.000000    0.000000            0.0   \n","6                     LocalGLMftt  16.384662  310.648783            0.0   \n","7              Rebase_LocalGLMftt   0.000000    0.000000            0.0   \n","8     Ensemble_Rebase_LocalGLMftt   0.000000    0.000000            0.0   \n","\n","   loss_train  loss_test  pred_avg_freq_train  pred_avg_freq_test  \n","0    0.000898   0.000528         1.459836e-03            0.001458  \n","1    0.000636   0.000360         1.299345e-08            0.000106  \n","2    0.000035   0.000144         1.947447e-08            0.000027  \n","3    0.000474   0.000174         4.993066e-04            0.000471  \n","4    0.000458   0.000171         1.597395e-08            0.000055  \n","5    0.000185   0.000060         3.497719e-09            0.000014  \n","6    0.000593   0.000160         9.643511e-04            0.000992  \n","7    0.000659   0.000170         1.735819e-08            0.000067  \n","8    0.000338   0.000099         2.731805e-08            0.000028  "]},"metadata":{},"output_type":"display_data"}],"source":["print(\"Results Average:\")\n","display(calc_avg_df([\"FT_transformer\",\"Rebase_FT_transformer\",\"Ensemble_Rebase_FT_transformer\",\"CAFTT\",\"Rebase_CAFTT\",\"Ensemble_Rebase_CAFTT\",\"LocalGLMftt\",\"Rebase_LocalGLMftt\",\"Ensemble_Rebase_LocalGLMftt\"]))\n","print(\"Results Standard-Deviation:\")\n","display(calc_std_df([\"FT_transformer\",\"Rebase_FT_transformer\",\"Ensemble_Rebase_FT_transformer\",\"CAFTT\",\"Rebase_CAFTT\",\"Ensemble_Rebase_CAFTT\",\"LocalGLMftt\",\"Rebase_LocalGLMftt\",\"Ensemble_Rebase_LocalGLMftt\"]))"]},{"cell_type":"markdown","metadata":{"id":"nL44-Wckjym0"},"source":["# Information about the Packages and Environment:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2244,"status":"ok","timestamp":1699574850162,"user":{"displayName":"Alexej","userId":"05436248405167721905"},"user_tz":-60},"id":"fT0zm60Ljym0","outputId":"d3ef86f7-fce4-4182-df1b-c059b8cb113f"},"outputs":[{"name":"stdout","output_type":"stream","text":["the python version is:\n","3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n","sys.version_info(major=3, minor=10, micro=12, releaselevel='final', serial=0)\n","\n","print pip list:\n","Package                          Version\n","-------------------------------- ---------------------\n","absl-py                          1.4.0\n","aiohttp                          3.8.6\n","aiosignal                        1.3.1\n","alabaster                        0.7.13\n","albumentations                   1.3.1\n","altair                           4.2.2\n","anyio                            3.7.1\n","appdirs                          1.4.4\n","argon2-cffi                      23.1.0\n","argon2-cffi-bindings             21.2.0\n","array-record                     0.5.0\n","arviz                            0.15.1\n","astropy                          5.3.4\n","astunparse                       1.6.3\n","async-timeout                    4.0.3\n","atpublic                         4.0\n","attrs                            23.1.0\n","audioread                        3.0.1\n","autograd                         1.6.2\n","Babel                            2.13.1\n","backcall                         0.2.0\n","beautifulsoup4                   4.11.2\n","bidict                           0.22.1\n","bigframes                        0.12.0\n","bleach                           6.1.0\n","blinker                          1.4\n","blis                             0.7.11\n","blosc2                           2.0.0\n","bokeh                            3.3.0\n","bqplot                           0.12.42\n","branca                           0.7.0\n","build                            1.0.3\n","CacheControl                     0.13.1\n","cachetools                       5.3.2\n","catalogue                        2.0.10\n","certifi                          2023.7.22\n","cffi                             1.16.0\n","chardet                          5.2.0\n","charset-normalizer               3.3.2\n","chex                             0.1.7\n","click                            8.1.7\n","click-plugins                    1.1.1\n","cligj                            0.7.2\n","cloudpickle                      2.2.1\n","cmake                            3.27.7\n","cmdstanpy                        1.2.0\n","colorcet                         3.0.1\n","colorlover                       0.3.0\n","colour                           0.1.5\n","community                        1.0.0b1\n","confection                       0.1.3\n","cons                             0.4.6\n","contextlib2                      21.6.0\n","contourpy                        1.2.0\n","cryptography                     41.0.5\n","cufflinks                        0.17.3\n","cupy-cuda11x                     11.0.0\n","cvxopt                           1.3.2\n","cvxpy                            1.3.2\n","cycler                           0.12.1\n","cymem                            2.0.8\n","Cython                           3.0.5\n","dask                             2023.8.1\n","datascience                      0.17.6\n","db-dtypes                        1.1.1\n","dbus-python                      1.2.18\n","debugpy                          1.6.6\n","decorator                        4.4.2\n","defusedxml                       0.7.1\n","diskcache                        5.6.3\n","distributed                      2023.8.1\n","distro                           1.7.0\n","dlib                             19.24.2\n","dm-tree                          0.1.8\n","docutils                         0.18.1\n","dopamine-rl                      4.0.6\n","duckdb                           0.9.1\n","earthengine-api                  0.1.377\n","easydict                         1.11\n","ecos                             2.0.12\n","editdistance                     0.6.2\n","eerepr                           0.0.4\n","en-core-web-sm                   3.6.0\n","entrypoints                      0.4\n","et-xmlfile                       1.1.0\n","etils                            1.5.2\n","etuples                          0.3.9\n","exceptiongroup                   1.1.3\n","fastai                           2.7.13\n","fastcore                         1.5.29\n","fastdownload                     0.0.7\n","fastjsonschema                   2.18.1\n","fastprogress                     1.0.3\n","fastrlock                        0.8.2\n","filelock                         3.13.1\n","fiona                            1.9.5\n","firebase-admin                   5.3.0\n","Flask                            2.2.5\n","flatbuffers                      23.5.26\n","flax                             0.7.5\n","folium                           0.14.0\n","fonttools                        4.44.0\n","frozendict                       2.3.8\n","frozenlist                       1.4.0\n","fsspec                           2023.6.0\n","future                           0.18.3\n","gast                             0.5.4\n","gcsfs                            2023.6.0\n","GDAL                             3.4.3\n","gdown                            4.6.6\n","geemap                           0.28.2\n","gensim                           4.3.2\n","geocoder                         1.38.1\n","geographiclib                    2.0\n","geopandas                        0.13.2\n","geopy                            2.3.0\n","gin-config                       0.5.0\n","glob2                            0.7\n","google                           2.0.3\n","google-api-core                  2.11.1\n","google-api-python-client         2.84.0\n","google-auth                      2.17.3\n","google-auth-httplib2             0.1.1\n","google-auth-oauthlib             1.0.0\n","google-cloud-bigquery            3.12.0\n","google-cloud-bigquery-connection 1.12.1\n","google-cloud-bigquery-storage    2.22.0\n","google-cloud-core                2.3.3\n","google-cloud-datastore           2.15.2\n","google-cloud-firestore           2.11.1\n","google-cloud-functions           1.13.3\n","google-cloud-iam                 2.12.2\n","google-cloud-language            2.9.1\n","google-cloud-resource-manager    1.10.4\n","google-cloud-storage             2.8.0\n","google-cloud-translate           3.11.3\n","google-colab                     1.0.0\n","google-crc32c                    1.5.0\n","google-pasta                     0.2.0\n","google-resumable-media           2.6.0\n","googleapis-common-protos         1.61.0\n","googledrivedownloader            0.4\n","graphviz                         0.20.1\n","greenlet                         3.0.1\n","grpc-google-iam-v1               0.12.6\n","grpcio                           1.59.2\n","grpcio-status                    1.48.2\n","gspread                          3.4.2\n","gspread-dataframe                3.3.1\n","gym                              0.25.2\n","gym-notices                      0.0.8\n","h5netcdf                         1.3.0\n","h5py                             3.9.0\n","holidays                         0.36\n","holoviews                        1.17.1\n","html5lib                         1.1\n","httpimport                       1.3.1\n","httplib2                         0.22.0\n","humanize                         4.7.0\n","hyperopt                         0.2.7\n","ibis-framework                   6.2.0\n","idna                             3.4\n","imageio                          2.31.6\n","imageio-ffmpeg                   0.4.9\n","imagesize                        1.4.1\n","imbalanced-learn                 0.10.1\n","imgaug                           0.4.0\n","importlib-metadata               6.8.0\n","importlib-resources              6.1.1\n","imutils                          0.5.4\n","inflect                          7.0.0\n","iniconfig                        2.0.0\n","install                          1.3.5\n","intel-openmp                     2023.2.0\n","ipyevents                        2.0.2\n","ipyfilechooser                   0.6.0\n","ipykernel                        5.5.6\n","ipyleaflet                       0.17.4\n","ipython                          7.34.0\n","ipython-genutils                 0.2.0\n","ipython-sql                      0.5.0\n","ipytree                          0.2.2\n","ipywidgets                       7.7.1\n","itsdangerous                     2.1.2\n","jax                              0.4.20\n","jaxlib                           0.4.20+cuda11.cudnn86\n","jeepney                          0.7.1\n","jieba                            0.42.1\n","Jinja2                           3.1.2\n","joblib                           1.3.2\n","jsonpickle                       3.0.2\n","jsonschema                       4.19.2\n","jsonschema-specifications        2023.7.1\n","jupyter-client                   6.1.12\n","jupyter-console                  6.1.0\n","jupyter_core                     5.5.0\n","jupyter-server                   1.24.0\n","jupyterlab-pygments              0.2.2\n","jupyterlab-widgets               3.0.9\n","kaggle                           1.5.16\n","keras                            2.14.0\n","keyring                          23.5.0\n","kiwisolver                       1.4.5\n","langcodes                        3.3.0\n","launchpadlib                     1.10.16\n","lazr.restfulclient               0.14.4\n","lazr.uri                         1.0.6\n","lazy_loader                      0.3\n","libclang                         16.0.6\n","librosa                          0.10.1\n","lida                             0.0.10\n","lightgbm                         4.1.0\n","linkify-it-py                    2.0.2\n","llmx                             0.0.15a0\n","llvmlite                         0.41.1\n","locket                           1.0.0\n","logical-unification              0.4.6\n","lxml                             4.9.3\n","malloy                           2023.1064\n","Markdown                         3.5.1\n","markdown-it-py                   3.0.0\n","MarkupSafe                       2.1.3\n","matplotlib                       3.7.1\n","matplotlib-inline                0.1.6\n","matplotlib-venn                  0.11.9\n","mdit-py-plugins                  0.4.0\n","mdurl                            0.1.2\n","miniKanren                       1.0.3\n","missingno                        0.5.2\n","mistune                          0.8.4\n","mizani                           0.9.3\n","mkl                              2023.2.0\n","ml-dtypes                        0.2.0\n","mlxtend                          0.22.0\n","more-itertools                   10.1.0\n","moviepy                          1.0.3\n","mpmath                           1.3.0\n","msgpack                          1.0.7\n","multidict                        6.0.4\n","multipledispatch                 1.0.0\n","multitasking                     0.0.11\n","murmurhash                       1.0.10\n","music21                          9.1.0\n","natsort                          8.4.0\n","nbclassic                        1.0.0\n","nbclient                         0.9.0\n","nbconvert                        6.5.4\n","nbformat                         5.9.2\n","nest-asyncio                     1.5.8\n","networkx                         3.2.1\n","nibabel                          4.0.2\n","nltk                             3.8.1\n","notebook                         6.5.5\n","notebook_shim                    0.2.3\n","numba                            0.58.1\n","numexpr                          2.8.7\n","numpy                            1.23.5\n","oauth2client                     4.1.3\n","oauthlib                         3.2.2\n","opencv-contrib-python            4.8.0.76\n","opencv-python                    4.8.0.76\n","opencv-python-headless           4.8.1.78\n","openpyxl                         3.1.2\n","opt-einsum                       3.3.0\n","optax                            0.1.7\n","orbax-checkpoint                 0.4.2\n","osqp                             0.6.2.post8\n","packaging                        23.2\n","pandas                           1.5.3\n","pandas-datareader                0.10.0\n","pandas-gbq                       0.17.9\n","pandas-stubs                     1.5.3.230304\n","pandocfilters                    1.5.0\n","panel                            1.3.1\n","param                            2.0.0\n","parso                            0.8.3\n","parsy                            2.1\n","partd                            1.4.1\n","pathlib                          1.0.1\n","pathy                            0.10.3\n","patsy                            0.5.3\n","peewee                           3.17.0\n","pexpect                          4.8.0\n","pickleshare                      0.7.5\n","Pillow                           9.4.0\n","pip                              23.1.2\n","pip-tools                        6.13.0\n","platformdirs                     3.11.0\n","plotly                           5.15.0\n","plotnine                         0.12.4\n","pluggy                           1.3.0\n","polars                           0.17.3\n","pooch                            1.8.0\n","portpicker                       1.5.2\n","prefetch-generator               1.0.3\n","preshed                          3.0.9\n","prettytable                      3.9.0\n","proglog                          0.1.10\n","progressbar2                     4.2.0\n","prometheus-client                0.18.0\n","promise                          2.3\n","prompt-toolkit                   3.0.39\n","prophet                          1.1.5\n","proto-plus                       1.22.3\n","protobuf                         3.20.3\n","psutil                           5.9.5\n","psycopg2                         2.9.9\n","ptyprocess                       0.7.0\n","py-cpuinfo                       9.0.0\n","py4j                             0.10.9.7\n","pyarrow                          9.0.0\n","pyasn1                           0.5.0\n","pyasn1-modules                   0.3.0\n","pycocotools                      2.0.7\n","pycparser                        2.21\n","pyct                             0.5.0\n","pydantic                         1.10.13\n","pydata-google-auth               1.8.2\n","pydot                            1.4.2\n","pydot-ng                         2.0.0\n","pydotplus                        2.0.2\n","PyDrive                          1.3.1\n","PyDrive2                         1.6.3\n","pyerfa                           2.0.1.1\n","pygame                           2.5.2\n","Pygments                         2.16.1\n","PyGObject                        3.42.1\n","PyJWT                            2.3.0\n","pymc                             5.7.2\n","pymystem3                        0.2.0\n","PyOpenGL                         3.1.7\n","pyOpenSSL                        23.3.0\n","pyparsing                        3.1.1\n","pyperclip                        1.8.2\n","pyproj                           3.6.1\n","pyproject_hooks                  1.0.0\n","pyshp                            2.3.1\n","PySocks                          1.7.1\n","pytensor                         2.14.2\n","pytest                           7.4.3\n","python-apt                       0.0.0\n","python-box                       7.1.1\n","python-dateutil                  2.8.2\n","python-louvain                   0.16\n","python-slugify                   8.0.1\n","python-utils                     3.8.1\n","pytz                             2023.3.post1\n","pyviz_comms                      3.0.0\n","PyWavelets                       1.4.1\n","PyYAML                           6.0.1\n","pyzmq                            23.2.1\n","qdldl                            0.1.7.post0\n","qudida                           0.0.4\n","ratelim                          0.1.6\n","referencing                      0.30.2\n","regex                            2023.6.3\n","requests                         2.31.0\n","requests-oauthlib                1.3.1\n","requirements-parser              0.5.0\n","rich                             13.6.0\n","rpds-py                          0.12.0\n","rpy2                             3.4.2\n","rsa                              4.9\n","scikit-image                     0.19.3\n","scikit-learn                     1.2.2\n","scipy                            1.11.3\n","scooby                           0.9.2\n","scs                              3.2.3\n","seaborn                          0.12.2\n","SecretStorage                    3.3.1\n","Send2Trash                       1.8.2\n","setuptools                       67.7.2\n","shapely                          2.0.2\n","six                              1.16.0\n","sklearn-pandas                   2.2.0\n","smart-open                       6.4.0\n","sniffio                          1.3.0\n","snowballstemmer                  2.2.0\n","sortedcontainers                 2.4.0\n","soundfile                        0.12.1\n","soupsieve                        2.5\n","soxr                             0.3.7\n","spacy                            3.6.1\n","spacy-legacy                     3.0.12\n","spacy-loggers                    1.0.5\n","Sphinx                           5.0.2\n","sphinxcontrib-applehelp          1.0.7\n","sphinxcontrib-devhelp            1.0.5\n","sphinxcontrib-htmlhelp           2.0.4\n","sphinxcontrib-jsmath             1.0.1\n","sphinxcontrib-qthelp             1.0.6\n","sphinxcontrib-serializinghtml    1.1.9\n","SQLAlchemy                       2.0.23\n","sqlglot                          17.16.2\n","sqlparse                         0.4.4\n","srsly                            2.4.8\n","stanio                           0.3.0\n","statsmodels                      0.14.0\n","sympy                            1.12\n","tables                           3.8.0\n","tabulate                         0.9.0\n","tbb                              2021.10.0\n","tblib                            3.0.0\n","tenacity                         8.2.3\n","tensorboard                      2.14.1\n","tensorboard-data-server          0.7.2\n","tensorflow                       2.14.0\n","tensorflow-datasets              4.9.3\n","tensorflow-estimator             2.14.0\n","tensorflow-gcs-config            2.14.0\n","tensorflow-hub                   0.15.0\n","tensorflow-io-gcs-filesystem     0.34.0\n","tensorflow-metadata              1.14.0\n","tensorflow-probability           0.22.0\n","tensorstore                      0.1.45\n","termcolor                        2.3.0\n","terminado                        0.17.1\n","text-unidecode                   1.3\n","textblob                         0.17.1\n","tf-slim                          1.1.0\n","thinc                            8.1.12\n","threadpoolctl                    3.2.0\n","tifffile                         2023.9.26\n","tinycss2                         1.2.1\n","toml                             0.10.2\n","tomli                            2.0.1\n","toolz                            0.12.0\n","torch                            2.1.0+cu118\n","torchaudio                       2.1.0+cu118\n","torchdata                        0.7.0\n","torchsummary                     1.5.1\n","torchtext                        0.16.0\n","torchvision                      0.16.0+cu118\n","tornado                          6.3.2\n","tqdm                             4.66.1\n","traitlets                        5.7.1\n","traittypes                       0.2.1\n","triton                           2.1.0\n","tweepy                           4.14.0\n","typer                            0.9.0\n","types-pytz                       2023.3.1.1\n","types-setuptools                 68.2.0.0\n","typing_extensions                4.5.0\n","tzlocal                          5.2\n","uc-micro-py                      1.0.2\n","uritemplate                      4.1.1\n","urllib3                          2.0.7\n","vega-datasets                    0.9.0\n","wadllib                          1.3.6\n","wasabi                           1.1.2\n","wcwidth                          0.2.9\n","webcolors                        1.13\n","webencodings                     0.5.1\n","websocket-client                 1.6.4\n","Werkzeug                         3.0.1\n","wheel                            0.41.3\n","widgetsnbextension               3.6.6\n","wordcloud                        1.9.2\n","wrapt                            1.14.1\n","xarray                           2023.7.0\n","xarray-einstats                  0.6.0\n","xgboost                          2.0.1\n","xlrd                             2.0.1\n","xxhash                           3.4.1\n","xyzservices                      2023.10.1\n","yarl                             1.9.2\n","yellowbrick                      1.5\n","yfinance                         0.2.31\n","zict                             3.0.0\n","zipp                             3.17.0\n","\n","print conda list:\n","/bin/bash: line 1: conda: command not found\n"]}],"source":["#print python version\n","print(\"the python version is:\")\n","print(sys.version)\n","print(sys.version_info)\n","print()\n","print(\"print pip list:\")\n","!pip list\n","print()\n","print(\"print conda list:\")\n","!conda list"]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"vscode":{"interpreter":{"hash":"1c610201dd013e8a553c0c81ea3f6aba71316b0cca1afc83fa96b8db366642e7"}}},"nbformat":4,"nbformat_minor":0}